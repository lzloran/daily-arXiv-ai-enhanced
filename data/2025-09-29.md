<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 109]
- [cs.AI](#cs.AI) [Total: 51]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Novel Differential Feature Learning for Effective Hallucination Detection and Classification](https://arxiv.org/abs/2509.21357)
*Wenkai Wang,Vincent Lee,Yizhen Zheng*

Main category: cs.CL

TL;DR: 该论文提出了一个双模型架构，通过投影融合块和差分特征学习机制来检测大语言模型的幻觉现象，发现幻觉信号集中在高度稀疏的特征子集中，仅使用1%的特征维度即可维持检测性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型幻觉是一个关键挑战，现有研究虽然发现特定隐藏层在幻觉内容和事实内容之间存在差异，但幻觉信号在层内的精确定位仍不明确，限制了高效检测方法的发展。

Method: 提出双模型架构，包含投影融合块用于自适应层间特征加权，以及差分特征学习机制，通过计算并行编码器在相同输入下学习互补表征的差异来识别判别性特征。

Result: 在HaluEval的问答、对话和摘要数据集上的系统实验表明，幻觉信号集中在高度稀疏的特征子集中，在问答和对话任务上实现了显著的准确率提升。分析揭示了分层的"漏斗模式"，浅层特征多样性高，深层特征使用集中。

Conclusion: 幻觉信号比先前假设的更加集中，这为开发计算效率高的检测系统提供了途径，可以在保持准确性的同时降低推理成本。

Abstract: Large language model hallucination represents a critical challenge where
outputs deviate from factual accuracy due to distributional biases in training
data. While recent investigations establish that specific hidden layers exhibit
differences between hallucinatory and factual content, the precise localization
of hallucination signals within layers remains unclear, limiting the
development of efficient detection methods. We propose a dual-model
architecture integrating a Projected Fusion (PF) block for adaptive inter-layer
feature weighting and a Differential Feature Learning (DFL) mechanism that
identifies discriminative features by computing differences between parallel
encoders learning complementary representations from identical inputs. Through
systematic experiments across HaluEval's question answering, dialogue, and
summarization datasets, we demonstrate that hallucination signals concentrate
in highly sparse feature subsets, achieving significant accuracy improvements
on question answering and dialogue tasks. Notably, our analysis reveals a
hierarchical "funnel pattern" where shallow layers exhibit high feature
diversity while deep layers demonstrate concentrated usage, enabling detection
performance to be maintained with minimal degradation using only 1\% of feature
dimensions. These findings suggest that hallucination signals are more
concentrated than previously assumed, offering a pathway toward computationally
efficient detection systems that could reduce inference costs while maintaining
accuracy.

</details>


### [2] [Influence Guided Context Selection for Effective Retrieval-Augmented Generation](https://arxiv.org/abs/2509.21359)
*Jiale Deng,Yanyan Shen,Ziyuan Pei,Youmin Chen,Linpeng Huang*

Main category: cs.CL

TL;DR: 提出了一种基于上下文影响力值(CI值)的RAG上下文选择方法，通过量化移除每个上下文对生成性能的影响来评估上下文质量，无需复杂超参数调优，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法因检索到的上下文质量差(包含无关或噪声信息)而效果受限，传统基于预定义质量评估指标的方法未能充分利用查询、上下文列表和生成器的综合信息。

Method: 将上下文质量评估重新定义为推理时数据估值问题，引入CI值度量上下文质量，开发参数化代理模型预测CI值，采用分层架构捕获局部查询-上下文相关性和全局上下文间交互。

Result: 在8个NLP任务和多个LLM上的广泛实验表明，该方法显著优于最先进的基线方法，能有效过滤低质量上下文同时保留关键信息。

Conclusion: CI值提供了一种全面评估上下文质量的新方法，通过整合查询感知相关性、列表感知独特性和生成器感知对齐，显著提升了RAG系统的性能。

Abstract: Retrieval-Augmented Generation (RAG) addresses large language model (LLM)
hallucinations by grounding responses in external knowledge, but its
effectiveness is compromised by poor-quality retrieved contexts containing
irrelevant or noisy information. While existing approaches attempt to improve
performance through context selection based on predefined context quality
assessment metrics, they show limited gains over standard RAG. We attribute
this limitation to their failure in holistically utilizing available
information (query, context list, and generator) for comprehensive quality
assessment. Inspired by recent advances in data selection, we reconceptualize
context quality assessment as an inference-time data valuation problem and
introduce the Contextual Influence Value (CI value). This novel metric
quantifies context quality by measuring the performance degradation when
removing each context from the list, effectively integrating query-aware
relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI
value eliminates complex selection hyperparameter tuning by simply retaining
contexts with positive CI values. To address practical challenges of label
dependency and computational overhead, we develop a parameterized surrogate
model for CI value prediction during inference. The model employs a
hierarchical architecture that captures both local query-context relevance and
global inter-context interactions, trained through oracle CI value supervision
and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and
multiple LLMs demonstrate that our context selection method significantly
outperforms state-of-the-art baselines, effectively filtering poor-quality
contexts while preserving critical information. Code is available at
https://github.com/SJTU-DMTai/RAG-CSM.

</details>


### [3] [Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs](https://arxiv.org/abs/2509.21361)
*Norman Paulsen*

Main category: cs.CL

TL;DR: 研究发现LLM的实际有效上下文窗口远小于宣称的最大上下文窗口，大多数模型在1000个token内就出现严重性能下降，实际有效窗口可能只有宣称大小的1%。


<details>
  <summary>Details</summary>
Motivation: 测试LLM提供商宣称的最大上下文窗口在实际使用中的有效性，揭示宣称值与实际性能之间的差距。

Method: 定义了最大有效上下文窗口概念，制定了测试方法评估不同大小和问题类型下的上下文窗口有效性，创建了标准化比较方式。

Result: 收集了数十万个数据点，发现最大有效上下文窗口与宣称的最大上下文窗口存在显著差异，且随问题类型变化。顶级模型在100个token内就可能失败，大多数模型在1000个token内准确率严重下降。

Conclusion: LLM的实际有效上下文窗口远小于宣称值，且随问题类型变化，这为提高模型准确性和减少幻觉提供了明确可行的改进方向。

Abstract: Large language model (LLM) providers boast big numbers for maximum context
window sizes. To test the real world use of context windows, we 1) define a
concept of maximum effective context window, 2) formulate a testing method of a
context window's effectiveness over various sizes and problem types, and 3)
create a standardized way to compare model efficacy for increasingly larger
context window sizes to find the point of failure. We collected hundreds of
thousands of data points across several models and found significant
differences between reported Maximum Context Window (MCW) size and Maximum
Effective Context Window (MECW) size. Our findings show that the MECW is, not
only, drastically different from the MCW but also shifts based on the problem
type. A few top of the line models in our test group failed with as little as
100 tokens in context; most had severe degradation in accuracy by 1000 tokens
in context. All models fell far short of their Maximum Context Window by as
much as 99 percent. Our data reveals the Maximum Effective Context Window
shifts based on the type of problem provided, offering clear and actionable
insights into how to improve model accuracy and decrease model hallucination
rates.

</details>


### [4] [How Large Language Models Need Symbolism](https://arxiv.org/abs/2509.21404)
*Xiaotie Deng,Hanyu Li*

Main category: cs.CL

TL;DR: AI未来发展需要超越单纯扩展规模，需要人类符号作为指南针来引导LLM的直觉


<details>
  <summary>Details</summary>
Motivation: 当前AI发展过度依赖规模扩展，缺乏真正的发现能力，需要人类符号来引导其强大的直觉

Method: 提出使用人类创造的符号作为指南针，引导大型语言模型的直觉能力

Result: 通过人类符号的引导，可以解锁AI的真正发现能力

Conclusion: AI的未来发展需要人类符号作为指南针，而不仅仅是规模扩展

Abstract: We argue that AI's future requires more than scaling. To unlock genuine
discovery, large language models need a compass: human-crafted symbols to guide
their powerful but blind intuition.

</details>


### [5] [One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning](https://arxiv.org/abs/2509.21443)
*Sualeha Farid,Jayden Lin,Zean Chen,Shivani Kumar,David Jurgens*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在跨语言道德判断中存在显著不一致性，反映了文化错位问题，需要更具文化意识的AI系统。


<details>
  <summary>Details</summary>
Motivation: LLMs主要基于英语数据训练，在多元文化和多语言环境中进行道德推理时存在泛化能力担忧，需要系统研究语言如何调节LLMs的道德决策。

Method: 将两个成熟的道德推理基准翻译为五种文化和类型学上不同的语言，进行多语言零样本评估，通过精心构建的研究问题分析差异驱动因素。

Result: 发现LLMs在不同语言中的道德判断存在显著不一致，往往反映文化错位，并识别出从分歧到推理策略等多种差异驱动因素。

Conclusion: 研究揭示了LLMs道德推理中的结构化错误类型学，呼吁开发更具文化意识的AI系统，并通过案例研究连接了预训练数据在塑造LLMs道德指南针中的作用。

Abstract: Large Language Models (LLMs) are increasingly deployed in multilingual and
multicultural environments where moral reasoning is essential for generating
ethically appropriate responses. Yet, the dominant pretraining of LLMs on
English-language data raises critical concerns about their ability to
generalize judgments across diverse linguistic and cultural contexts. In this
work, we systematically investigate how language mediates moral decision-making
in LLMs. We translate two established moral reasoning benchmarks into five
culturally and typologically diverse languages, enabling multilingual zero-shot
evaluation. Our analysis reveals significant inconsistencies in LLMs' moral
judgments across languages, often reflecting cultural misalignment. Through a
combination of carefully constructed research questions, we uncover the
underlying drivers of these disparities, ranging from disagreements to
reasoning strategies employed by LLMs. Finally, through a case study, we link
the role of pretraining data in shaping an LLM's moral compass. Through this
work, we distill our insights into a structured typology of moral reasoning
errors that calls for more culturally-aware AI.

</details>


### [6] [LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges with GPT-5](https://arxiv.org/abs/2509.21450)
*Gaurav Kumar Gupta,Nirajan Acharya,Pranal Pande*

Main category: cs.CL

TL;DR: 评估GPT-5在糖尿病诊断和管理中的表现，使用符合ADA标准的合成病例，在症状识别、实验室解读、妊娠糖尿病筛查等五个场景中测试，结果显示与ADA标准高度一致。


<details>
  <summary>Details</summary>
Motivation: 糖尿病是全球重大健康挑战，早期识别困难，LLMs的发展为提供结构化、可解释的决策支持提供了机会。

Method: 使用完全基于合成病例的模拟框架，病例符合ADA 2025护理标准，测试GPT-5在五个代表性场景中的表现：症状识别、实验室解读、妊娠糖尿病筛查、远程监测和多模态并发症检测。

Result: GPT-5在病例分类、临床推理生成、患者解释和结构化JSON摘要输出方面表现出与ADA定义标准的高度一致性。

Conclusion: GPT-5可能作为临床医生和患者的双重用途工具，同时强调了可重复评估框架在负责任评估医疗领域LLMs的重要性。

Abstract: Diabetes mellitus is a major global health challenge, affecting over half a
billion adults worldwide with prevalence projected to rise. Although the
American Diabetes Association (ADA) provides clear diagnostic thresholds, early
recognition remains difficult due to vague symptoms, borderline laboratory
values, gestational complexity, and the demands of long-term monitoring.
Advances in large language models (LLMs) offer opportunities to enhance
decision support through structured, interpretable, and patient-friendly
outputs. This study evaluates GPT-5, the latest generative pre-trained
transformer, using a simulation framework built entirely on synthetic cases
aligned with ADA Standards of Care 2025 and inspired by public datasets
including NHANES, Pima Indians, EyePACS, and MIMIC-IV. Five representative
scenarios were tested: symptom recognition, laboratory interpretation,
gestational diabetes screening, remote monitoring, and multimodal complication
detection. For each, GPT-5 classified cases, generated clinical rationales,
produced patient explanations, and output structured JSON summaries. Results
showed strong alignment with ADA-defined criteria, suggesting GPT-5 may
function as a dual-purpose tool for clinicians and patients, while underscoring
the importance of reproducible evaluation frameworks for responsibly assessing
LLMs in healthcare.

</details>


### [7] [Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes](https://arxiv.org/abs/2509.21456)
*Guangliang Liu,Bocheng Chen,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 本文分析了在减轻语言模型性别刻板印象时，公平性目标与下游任务性能之间的权衡机制，发现当前方法存在局限性。


<details>
  <summary>Details</summary>
Motivation: 研究道德对齐过程中性能权衡的底层机制，特别是在减轻性别刻板印象时，公平性目标如何影响下游任务性能。

Method: 通过遗忘和公平性目标的视角，分析减轻性别刻板印象时的性能权衡机制，考察选择性遗忘对整体遗忘水平的影响。

Result: 发现下游任务性能主要由整体遗忘水平驱动，选择性遗忘刻板印象会增加整体遗忘，且现有缓解遗忘的方法效果有限。

Conclusion: 当前公平性目标在实现性能权衡方面存在局限性，需要更有效的方法来平衡道德对齐与任务性能。

Abstract: Moral alignment has emerged as a widely adopted approach for regulating the
behavior of pretrained language models (PLMs), typically through fine-tuning or
model editing on curated datasets. However, this process often comes at the
cost of degraded downstream task performance. Prior studies commonly aim to
achieve a performance trade-off by encouraging PLMs to selectively forget
stereotypical knowledge through carefully designed fairness objectives, while
preserving their helpfulness. In this short paper, we investigate the
underlying mechanisms of the performance trade-off in the context of mitigating
gender stereotypes, through the lens of forgetting and the fairness objective.
Our analysis reveals the limitations of current fairness objective in achieving
trade-off by demonstrating that: (1) downstream task performance is primarily
driven by the overall forgetting level; (2) selective forgetting of stereotypes
tends to increase overall forgetting; and (3) general solutions for mitigating
forgetting are ineffective at reducing overall forgetting and fail to improve
downstream task performance.

</details>


### [8] [A State-of-the-Art SQL Reasoning Model using RLVR](https://arxiv.org/abs/2509.21459)
*Alnur Ali,Ashutosh Baheti,Jonathan Chang,Ta-Chung Chi,Brandon Cui,Andrew Drozdov,Jonathan Frankle,Abhay Gupta,Pallavi Koppol,Sean Kulinski,Jonathan Li,Dipendra Misra,Krista Opsahl-Ong,Jose Javier Gonzalez Ortiz,Matei Zaharia,Yue Zhang*

Main category: cs.CL

TL;DR: 提出了一种基于强化学习的自定义推理模型训练方法，在BIRD数据科学基准测试中实现了最先进的准确率，无需额外训练数据或专有模型。


<details>
  <summary>Details</summary>
Motivation: 开发能够整合组织特定知识的自定义推理模型，解决企业客户面临的问题，特别是在奖励函数可验证的强化学习场景中。

Method: 采用简单通用的训练流程：精心设计的提示和模型选择，使用离线强化学习方法TAO进行预热，然后进行严格的在线RLVR训练。

Result: 在BIRD基准测试中，首次提交就达到了私有测试集上的最先进准确率：不使用自一致性时为73.56%，使用自一致性时为75.68%，且所需生成次数少于第二名方法。

Conclusion: 虽然BIRD只是一个代理任务，但该框架的简单性使其广泛适用于企业领域，如商业智能、数据科学和编程。

Abstract: Developing custom reasoning models via Reinforcement Learning (RL) that can
incorporate organization-specific knowledge has great potential to address
problems faced by enterprise customers. In many of these problems, the reward
function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We
apply RLVR to a popular data science benchmark called BIRD that measures the
ability of an AI agent to convert a natural language query for a database to
SQL executions. We apply a simple and general-purpose training recipe involving
careful prompt and model selection, a warm-up stage using our offline RL
approach called TAO, followed by rigorous online RLVR training. With no
additional training data beyond the BIRD training set and no use of proprietary
models, our very first submission to the BIRD leaderboard reached
state-of-the-art accuracy on the private test set: 73.56% without
self-consistency and 75.68% with self-consistency. In the latter case, our
model also required fewer generations than the second-best approach. While BIRD
is only a proxy task, the simplicity of our framework makes it broadly
applicable to enterprise domains such as business intelligence, data science,
and coding.

</details>


### [9] [Learning to Reason with Mixture of Tokens](https://arxiv.org/abs/2509.21482)
*Adit Jain,Brendan Rappazzo*

Main category: cs.CL

TL;DR: 本文提出在强化学习可验证奖励（RLVR）中使用混合token生成（MoT-G）方法，通过保留模型概率分布信息来改进语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法在推理步骤中采样离散token，丢弃了模型概率分布的丰富信息。为了充分利用这些分布信息并扩大推理搜索空间，研究者探索了MoT-G在RLVR中的应用。

Method: 提出了一个统一框架，将现有MoT-G方法（包括构建混合嵌入作为token嵌入加权和的训练无关方法）推广，并扩展RLVR直接在连续混合空间中生成思维链。

Result: 在Reasoning-Gym推理密集型任务套件上评估，MoT-G方法相比标准解码在7/10任务上获得5-35%提升，且仅用一半轨迹数达到相当精度，表明训练效率提高。

Conclusion: MoT-G的好处可能源于其在推理过程中保持更高隐藏状态熵和促进token空间探索的能力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a leading
approach for improving large language model (LLM) reasoning capabilities. Most
current methods follow variants of Group Relative Policy Optimization, which
samples multiple reasoning completions, scores them relative to each other, and
adjusts the policy accordingly. However, these approaches invariably sample
discrete tokens at each reasoning step, discarding the rich distributional
information in the model's probability distribution over candidate tokens.
While preserving and utilizing this distributional information has proven
beneficial in non-RL settings, current RLVR methods seem to be unnecessarily
constraining the reasoning search space by not using this information. To
address this limitation, we investigate mixture-of-token generation (MoT-G) in
RLVR. We present a unified framework that generalizes existing MoT-G
approaches, including existing training-free methods that construct mixture
embeddings as weighted sums over token embeddings, and extend RLVR to operate
directly in this continuous mixture space for generating chain-of-thought.
Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive
language tasks, we find that MoT--G methods achieve substantial improvements
(5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the
Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of
trajectories, suggesting improved training efficiency. Through comprehensive
hidden-state and token-level analyses, we provide evidence that MoT--G's
benefits may stem from its ability to maintain higher hidden-state entropy
throughout the reasoning process and promote exploration in token space.

</details>


### [10] [Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning](https://arxiv.org/abs/2509.21487)
*Jillian Xu,Dylan Zhou,Vinay Shukla,Yang Yang,Junrui Ruan,Shuhuai Lin,Wenfei Zou,Yinxiao Liu,Karthik Lakshmanan*

Main category: cs.CL

TL;DR: DHRD方法通过添加分类头和推理头，在训练时利用教师推理监督，测试时只使用分类头，实现了准确率提升同时保持高推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决CoT提示在提升分类准确率的同时引入推理吞吐量显著下降的问题，寻求准确率与效率的平衡。

Method: DHRD训练方法：为解码器语言模型添加池化分类头（训练和推理使用）和推理头（仅训练使用），使用标签交叉熵和token级语言模型损失的加权和作为损失函数。

Result: 在七个SuperGLUE任务上相对池化基线提升0.65-5.47%，蕴含/因果任务提升更大；推理吞吐量匹配池化分类器，比CoT解码快96-142倍。

Conclusion: DHRD方法成功解决了CoT的准确率与吞吐量权衡问题，实现了准确率提升同时保持高效推理。

Abstract: Chain-of-Thought (CoT) prompting often improves classification accuracy, but
it introduces a significant throughput penalty with rationale generation (Wei
et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we
introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for
decoder-only language models (LMs) that adds (i) a pooled classification head
used during training and inference and (ii) a reasoning head supervised by
teacher rationales used only in training. We train with a loss function that is
a weighted sum of label cross-entropy and token-level LM loss over
input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative
gains of 0.65-5.47% over pooled baselines, with notably larger gains on
entailment/causal tasks. Since we disable the reasoning head at test time,
inference throughput matches pooled classifiers and exceeds CoT decoding on the
same backbones by 96-142 times in QPS.

</details>


### [11] [On Code-Induced Reasoning in LLMs](https://arxiv.org/abs/2509.21499)
*Abdul Waheed,Zhen Wu,Carolyn Rosé,Daphne Ippolito*

Main category: cs.CL

TL;DR: 通过系统实验发现，代码的结构特性比语义特性对LLM推理能力影响更大，适当的抽象表示（如伪代码）可以替代真实代码，而保留表面规律性的损坏代码仍能保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 研究代码的哪些方面（结构或语义）对提升大型语言模型的推理能力最为关键，为设计训练数据提供指导。

Method: 构建十种编程语言的并行指令数据集，应用受控扰动来选择性破坏代码的结构或语义属性，在五个模型家族和八个规模上进行了3,331次实验。

Result: LLM对结构扰动比语义扰动更敏感；伪代码和流程图等抽象表示与真实代码效果相当；保留表面规律性的损坏代码仍具竞争力；不同编程语言在不同任务上有特定优势。

Conclusion: 代码的结构特性是影响LLM推理能力的关键因素，适当的抽象表示可以替代真实代码，这为设计高效的训练数据提供了重要启示。

Abstract: Code data has been shown to enhance the reasoning capabilities of large
language models (LLMs), but it remains unclear which aspects of code are most
responsible. We investigate this question with a systematic, data-centric
framework. We construct parallel instruction datasets in ten programming
languages and apply controlled perturbations that selectively disrupt
structural or semantic properties of code. We then finetune LLMs from five
model families and eight scales on each variant and evaluate their performance
on natural language, math, and code tasks. Across 3,331 experiments, our
results show that LLMs are more vulnerable to structural perturbations than
semantic ones, particularly on math and code tasks. Appropriate abstractions
like pseudocode and flowcharts can be as effective as code, while encoding the
same information with fewer tokens without adhering to original syntax can
often retain or even improve performance. Remarkably, even corrupted code with
misleading signals remains competitive when surface-level regularities persist.
Finally, syntactic styles also shape task-specific gains with Python favoring
natural language reasoning and lower-level languages such as Java and Rust
favoring math. Through our systematic framework, we aim to provide insight into
how different properties of code influence reasoning and inform the design of
training data for enhancing LLM reasoning capabilities.

</details>


### [12] [Agribot: agriculture-specific question answer system](https://arxiv.org/abs/2509.21535)
*Naman Jain,Pranjali Jain,Pratik Kayal,Jayakrishna Sahit,Soham Pachpande,Jayesh Choudhari*

Main category: cs.CL

TL;DR: 开发了一个基于Kisan呼叫中心数据的农业聊天机器人，用于回答农民关于天气、市场价格、植物保护和政府计划等问题的查询。


<details>
  <summary>Details</summary>
Motivation: 印度是农业经济国家，农民需要便捷获取农业实践信息以实现最优农业增长和产出。

Method: 基于句子嵌入模型构建聊天机器人系统，通过消除同义词和整合实体提取来提高准确性。

Result: 初始句子嵌入模型准确率为56%，消除同义词和整合实体提取后准确率提升至86%。

Conclusion: 该系统可24/7通过任何电子设备访问，为农民提供易于理解的农业信息，减轻呼叫中心工作负担并改善农业产出。

Abstract: India is an agro-based economy and proper information about agricultural
practices is the key to optimal agricultural growth and output. In order to
answer the queries of the farmer, we have build an agricultural chatbot based
on the dataset from Kisan Call Center. This system is robust enough to answer
queries related to weather, market rates, plant protection and government
schemes. This system is available 24* 7, can be accessed through any electronic
device and the information is delivered with the ease of understanding. The
system is based on a sentence embedding model which gives an accuracy of 56%.
After eliminating synonyms and incorporating entity extraction, the accuracy
jumps to 86%. With such a system, farmers can progress towards easier
information about farming related practices and hence a better agricultural
output. The job of the Call Center workforce would be made easier and the hard
work of various such workers can be redirected to a better goal.

</details>


### [13] [Domain-Aware Speaker Diarization On African-Accented English](https://arxiv.org/abs/2509.21554)
*Chibuzor Okocha,Kelechi Ezema,Christan Grant*

Main category: cs.CL

TL;DR: 该研究评估了非洲口音英语的说话人日志系统在不同领域（通用对话与临床对话）的性能差异，发现临床语音存在显著的领域惩罚，并提出轻量级领域适应方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索说话人日志系统在非洲口音英语的通用对话和临床对话中的领域效应差异，特别是在重叠语音评分协议下。

Method: 评估多个生产和开源系统，使用严格的DER协议评分重叠语音，通过错误分析识别问题来源，并测试在口音匹配数据上微调分割模块的轻量级领域适应方法。

Result: 临床语音存在一致的领域惩罚，主要归因于误报和漏检错误，与短轮次和频繁重叠相关。轻量级适应方法能减少错误但无法完全消除领域差距。

Conclusion: 研究提出了跨领域基准测试、错误分解和对话级分析方法，以及易于复现的适应方案，建议下一步应关注重叠感知分割和平衡临床资源。

Abstract: This study examines domain effects in speaker diarization for
African-accented English. We evaluate multiple production and open systems on
general and clinical dialogues under a strict DER protocol that scores overlap.
A consistent domain penalty appears for clinical speech and remains significant
across models. Error analysis attributes much of this penalty to false alarms
and missed detections, aligning with short turns and frequent overlap. We test
lightweight domain adaptation by fine-tuning a segmentation module on
accent-matched data; it reduces error but does not eliminate the gap. Our
contributions include a controlled benchmark across domains, a concise approach
to error decomposition and conversation-level profiling, and an adaptation
recipe that is easy to reproduce. Results point to overlap-aware segmentation
and balanced clinical resources as practical next steps.

</details>


### [14] [Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution](https://arxiv.org/abs/2509.21557)
*Yash Saxena,Raviteja Bommireddy,Ankur Padia,Manas Gaur*

Main category: cs.CL

TL;DR: 本文比较了两种大语言模型引用范式：生成时引用(G-Cite)和后处理引用(P-Cite)，发现在高风险应用中推荐采用检索中心、P-Cite优先的方法。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域如医疗、法律、学术和金融中，可信赖的大语言模型必须引用可验证的来源，因为即使是小错误也可能带来严重后果。

Method: 引入两种引用范式：G-Cite（在解码时同时生成答案和引用）和P-Cite（先起草答案再添加引用），并在四个流行归因数据集上从零样本到高级检索增强方法进行全面评估。

Result: 结果显示引用覆盖率和正确性之间存在一致权衡，检索是两种范式归因质量的主要驱动因素。P-Cite方法实现高覆盖率且具有竞争性正确性和适中延迟，而G-Cite方法以覆盖率和速度为代价优先考虑精度。

Conclusion: 推荐高风险应用采用检索中心、P-Cite优先的方法，在严格声明验证等精度关键场景中保留G-Cite的使用。

Abstract: Trustworthy Large Language Models (LLMs) must cite human-verifiable sources
in high-stakes domains such as healthcare, law, academia, and finance, where
even small errors can have severe consequences. Practitioners and researchers
face a choice: let models generate citations during decoding, or let models
draft answers first and then attach appropriate citations. To clarify this
choice, we introduce two paradigms: Generation-Time Citation (G-Cite), which
produces the answer and citations in one pass, and Post-hoc Citation (P-Cite),
which adds or verifies citations after drafting. We conduct a comprehensive
evaluation from zero-shot to advanced retrieval-augmented methods across four
popular attribution datasets and provide evidence-based recommendations that
weigh trade-offs across use cases. Our results show a consistent trade-off
between coverage and citation correctness, with retrieval as the main driver of
attribution quality in both paradigms. P-Cite methods achieve high coverage
with competitive correctness and moderate latency, whereas G-Cite methods
prioritize precision at the cost of coverage and speed. We recommend a
retrieval-centric, P-Cite-first approach for high-stakes applications,
reserving G-Cite for precision-critical settings such as strict claim
verification. Our codes and human evaluation results are available at
https://anonymous.4open.science/r/Citation_Paradigms-BBB5/

</details>


### [15] [Comparative Personalization for Multi-document Summarization](https://arxiv.org/abs/2509.21562)
*Haoyuan Li,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: 提出了ComPSum个性化多文档摘要框架，通过比较用户偏好差异生成结构化分析来指导个性化摘要生成，并构建了评估框架AuthorMap和数据集PerMSum。


<details>
  <summary>Details</summary>
Motivation: 个性化多文档摘要需要识别用户偏好的细粒度差异，通过比较不同用户的偏好来实现有效个性化。

Method: ComPSum框架：1）通过比较用户偏好生成结构化分析；2）使用该分析指导个性化摘要生成。同时提出AuthorMap评估框架和PerMSum数据集。

Result: 在PerMSum数据集上使用AuthorMap评估，ComPSum优于强基线方法。

Conclusion: 通过比较用户偏好差异进行结构化分析，能够有效指导个性化摘要生成，ComPSum在个性化多文档摘要任务中表现优异。

Abstract: Personalized multi-document summarization (MDS) is essential for meeting
individual user preferences of writing style and content focus for summaries.
In this paper, we propose that for effective personalization, it is important
to identify fine-grained differences between users' preferences by comparing
the given user's preferences with other users' preferences.Motivated by this,
we propose ComPSum, a personalized MDS framework. It first generates a
structured analysis of a user by comparing their preferences with other users'
preferences. The generated structured analysis is then used to guide the
generation of personalized summaries. To evaluate the performance of ComPSum,
we propose AuthorMap, a fine-grained reference-free evaluation framework for
personalized MDS. It evaluates the personalization of a system based on the
authorship attribution between two personalized summaries generated for
different users. For robust evaluation of personalized MDS, we construct
PerMSum, a personalized MDS dataset in the review and news domain. We evaluate
the performance of ComPSum on PerMSum using AuthorMap, showing that it
outperforms strong baselines.

</details>


### [16] [Vision Language Models Cannot Plan, but Can They Formalize?](https://arxiv.org/abs/2509.21576)
*Muyu He,Yuxi Zheng,Yuchen Liu,Zijian An,Bill Cai,Jiani Huang,Lifeng Zhou,Feng Liu,Ziyang Li,Li Zhang*

Main category: cs.CL

TL;DR: 本文提出了五种VLM作为形式化器的管道，用于处理一次性、开放词汇和多模态的PDDL形式化，在包含真实、多视角和低质量图像的基准测试中显著优于端到端规划生成。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在简单多模态规划任务中表现良好，但在需要长序列动作的长视野规划任务中表现不足。文本领域通过将规划问题转化为PDDL形式化语言并使用形式化求解器取得了显著进展，但在多模态环境中相关研究仍然稀缺。

Method: 开发了五种VLM作为形式化器的管道，将多模态输入转化为PDDL规划领域和问题定义，然后调用形式化求解器生成可验证的规划方案。

Result: VLM作为形式化器的方法在多个基准测试中显著优于端到端规划生成方法，但发现视觉能力是主要瓶颈，VLMs经常无法捕捉必要的对象关系全集。

Conclusion: VLM作为形式化器在多模态规划中具有显著优势，但视觉能力限制是主要挑战。生成中间文本表示（如标题或场景图）可以部分弥补性能，但其不一致的收益为未来多模态规划形式化研究留下了空间。

Abstract: The advancement of vision language models (VLMs) has empowered embodied
agents to accomplish simple multimodal planning tasks, but not long-horizon
ones requiring long sequences of actions. In text-only simulations,
long-horizon planning has seen significant improvement brought by repositioning
the role of LLMs. Instead of directly generating action sequences, LLMs
translate the planning domain and problem into a formal planning language like
the Planning Domain Definition Language (PDDL), which can call a formal solver
to derive the plan in a verifiable manner. In multimodal environments, research
on VLM-as-formalizer remains scarce, usually involving gross simplifications
such as predefined object vocabulary or overly similar few-shot examples. In
this work, we present a suite of five VLM-as-formalizer pipelines that tackle
one-shot, open-vocabulary, and multimodal PDDL formalization. We evaluate those
on an existing benchmark while presenting another two that for the first time
account for planning with authentic, multi-view, and low-quality images. We
conclude that VLM-as-formalizer greatly outperforms end-to-end plan generation.
We reveal the bottleneck to be vision rather than language, as VLMs often fail
to capture an exhaustive set of necessary object relations. While generating
intermediate, textual representations such as captions or scene graphs
partially compensate for the performance, their inconsistent gain leaves
headroom for future research directions on multimodal planning formalization.

</details>


### [17] ["Be My Cheese?": Assessing Cultural Nuance in Multilingual LLM Translations](https://arxiv.org/abs/2509.21577)
*Madison Van Doren,Cory Holland*

Main category: cs.CL

TL;DR: 本研究评估了多语言AI模型在翻译比喻性语言时的本地化能力，发现尽管模型能产生语法正确的翻译，但在文化适宜性方面仍有明显不足，需要大量人工修正。


<details>
  <summary>Details</summary>
Motivation: 现有LLM翻译研究和行业基准主要关注语法准确性和词汇正确性，而忽视了文化适宜性和整体本地化质量这些对实际应用至关重要的因素。

Method: 评估了87个LLM生成的电商营销邮件翻译，涵盖20种语言的24种区域方言，由母语评审员提供定量评分和定性反馈。

Result: 领先模型通常能产生语法正确的翻译，但文化细微的语言仍是明显短板。即使是高资源语言也经常误译比喻性表达和文字游戏。

Conclusion: 挑战了数据量是机器翻译质量最可靠预测因子的假设，引入文化适宜性作为多语言LLM性能的关键决定因素，为大规模研究提供了概念验证。

Abstract: This pilot study explores the localisation capabilities of state-of-the-art
multilingual AI models when translating figurative language, such as idioms and
puns, from English into a diverse range of global languages. It expands on
existing LLM translation research and industry benchmarks, which emphasise
grammatical accuracy and token-level correctness, by focusing on cultural
appropriateness and overall localisation quality - critical factors for
real-world applications like marketing and e-commerce.
  To investigate these challenges, this project evaluated a sample of 87
LLM-generated translations of e-commerce marketing emails across 24 regional
dialects of 20 languages. Human reviewers fluent in each target language
provided quantitative ratings and qualitative feedback on faithfulness to the
original's tone, meaning, and intended audience. Findings suggest that, while
leading models generally produce grammatically correct translations, culturally
nuanced language remains a clear area for improvement, often requiring
substantial human refinement. Notably, even high-resource global languages,
despite topping industry benchmark leaderboards, frequently mistranslated
figurative expressions and wordplay.
  This work challenges the assumption that data volume is the most reliable
predictor of machine translation quality and introduces cultural
appropriateness as a key determinant of multilingual LLM performance - an area
currently underexplored in existing academic and industry benchmarks. As a
proof of concept, this pilot highlights limitations of current multilingual AI
systems for real-world localisation use cases. Results of this pilot support
the opportunity for expanded research at greater scale to deliver generalisable
insights and inform deployment of reliable machine translation workflows in
culturally diverse contexts.

</details>


### [18] [Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective](https://arxiv.org/abs/2509.21613)
*Lingxiao Kong,Cong Yang,Oya Deniz Beyan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 该论文提出了多目标强化学习在大型语言模型优化中的应用框架，包括分类法、方法分析和未来研究方向，特别关注元策略MORL的开发。


<details>
  <summary>Details</summary>
Motivation: 多目标强化学习在LLM优化中面临挑战，需要解决效率、灵活性、个性化功能和复杂性问题，以提升LLM性能。

Method: 提出MORL分类法，分析各种MORL方法在LLM优化中的优缺点，并设计MORL基准框架来评估不同方法对目标关系的影响。

Result: 识别了当前MORL方法在LLM应用中的局限性，提出了元策略MORL作为未来研究方向，通过双层学习范式提高效率和灵活性。

Conclusion: MORL在LLM优化中具有重要潜力，元策略MORL开发是提升LLM性能的关键方向，需要解决相关研究问题并探索潜在解决方案。

Abstract: Multi-Objective Reinforcement Learning (MORL) presents significant challenges
and opportunities for optimizing multiple objectives in Large Language Models
(LLMs). We introduce a MORL taxonomy and examine the advantages and limitations
of various MORL methods when applied to LLM optimization, identifying the need
for efficient and flexible approaches that accommodate personalization
functionality and inherent complexities in LLMs and RL. We propose a vision for
a MORL benchmarking framework that addresses the effects of different methods
on diverse objective relationships. As future research directions, we focus on
meta-policy MORL development that can improve efficiency and flexibility
through its bi-level learning paradigm, highlighting key research questions and
potential solutions for improving LLM performance.

</details>


### [19] [OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule](https://arxiv.org/abs/2509.21623)
*Yuxuan Zhu,David H. Yang,Mohammad Mohammadi Amiri,Keerthiram Murugesan,Tejaswini Pedapati,Pin-Yu Chen*

Main category: cs.CL

TL;DR: OjaKV是一个解决LLM长上下文推理中KV缓存内存瓶颈的框架，通过混合存储策略和在线子空间自适应，在保持精度的同时实现高压缩比。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法使用静态离线学习的子空间，在数据分布变化时性能下降，无法适应动态上下文变化。

Method: 采用混合存储策略：完整保留首尾关键token作为锚点，对中间token使用基于Oja算法的在线PCA进行低秩压缩，在预填充阶段全面更新，解码阶段轻量级定期更新。

Result: 在高压缩比下保持甚至提升零样本准确率，在需要复杂推理的超长上下文基准测试中表现最佳，与FlashAttention等现代注意力模块完全兼容。

Conclusion: OjaKV是一个无需模型微调的即插即用解决方案，为内存高效的长上下文推理提供了实用框架。

Abstract: The expanding long-context capabilities of large language models are
constrained by a significant memory bottleneck: the key-value (KV) cache
required for autoregressive generation. This bottleneck is substantial; for
instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of
4 requires approximately 16GB for its KV cache, a size exceeding the model's
weights. While KV-cache compression via low-rank projection is a promising
direction, existing methods rely on a static, offline-learned subspace that
performs poorly under data distribution shifts. To overcome these limitations,
we introduce OjaKV, a novel framework that integrates a strategic hybrid
storage policy with online subspace adaptation. First, OjaKV recognizes that
not all tokens are equally important for compression; it preserves the crucial
first and most recent tokens in full-rank, maintaining high-fidelity anchors
for attention. Second, for the vast majority of intermediate tokens, it applies
low-rank compression by incrementally adapting the projection basis using Oja's
algorithm for online principal component analysis. This adaptation involves a
comprehensive update during prompt prefilling and lightweight periodic updates
during decoding, ensuring the subspace remains aligned with the evolving
context. Crucially, our framework is fully compatible with modern attention
modules like FlashAttention. Experiments demonstrate that OjaKV maintains or
even improves zero-shot accuracy at high compression ratios. In particular,
OjaKV achieves its strongest gains on very long-context benchmarks that require
complex reasoning, highlighting the importance of online subspace adaptation in
dynamically tracking context shifts. These results establish our hybrid
framework as a practical, plug-and-play solution for memory-efficient
long-context inference without requiring model fine-tuning.

</details>


### [20] [Towards Transparent AI: A Survey on Explainable Language Models](https://arxiv.org/abs/2509.21631)
*Avash Palikhe,Zichong Wang,Zhipeng Yin,Rui Guo,Qiang Duan,Jie Yang,Wenbin Zhang*

Main category: cs.CL

TL;DR: 这篇综述论文系统回顾了针对语言模型的可解释人工智能方法，特别关注不同Transformer架构下的XAI技术，并评估其合理性和忠实性。


<details>
  <summary>Details</summary>
Motivation: 语言模型的黑盒特性在关键领域应用中带来了可解释性挑战，而现有的XAI方法在应用于复杂语言模型时存在诸多局限，需要针对不同架构进行专门研究。

Method: 按Transformer架构类型（仅编码器、仅解码器、编码器-解码器）组织XAI技术，分析每种架构下方法的适应性，并通过合理性和忠实性双重标准评估技术效果。

Result: 提供了针对不同语言模型架构的XAI技术分类框架，揭示了各种方法在不同架构下的优势和局限性。

Conclusion: 识别了开放研究挑战，并提出了未来发展方向，旨在推动开发更稳健、透明和可解释的语言模型XAI方法。

Abstract: Language Models (LMs) have significantly advanced natural language processing
and enabled remarkable progress across diverse domains, yet their black-box
nature raises critical concerns about the interpretability of their internal
mechanisms and decision-making processes. This lack of transparency is
particularly problematic for adoption in high-stakes domains, where
stakeholders need to understand the rationale behind model outputs to ensure
accountability. On the other hand, while explainable artificial intelligence
(XAI) methods have been well studied for non-LMs, they face many limitations
when applied to LMs due to their complex architectures, considerable training
corpora, and broad generalization abilities. Although various surveys have
examined XAI in the context of LMs, they often fail to capture the distinct
challenges arising from the architectural diversity and evolving capabilities
of these models. To bridge this gap, this survey presents a comprehensive
review of XAI techniques with a particular emphasis on LMs, organizing them
according to their underlying transformer architectures: encoder-only,
decoder-only, and encoder-decoder, and analyzing how methods are adapted to
each while assessing their respective strengths and limitations. Furthermore,
we evaluate these techniques through the dual lenses of plausibility and
faithfulness, offering a structured perspective on their effectiveness.
Finally, we identify open research challenges and outline promising future
directions, aiming to guide ongoing efforts toward the development of robust,
transparent, and interpretable XAI methods for LMs.

</details>


### [21] [ReviewScore: Misinformed Peer Review Detection with Large Language Models](https://arxiv.org/abs/2509.21679)
*Hyun Ryu,Doohyuk Jang,Hyemin S. Lee,Joonhyun Jeong,Gyeongman Kim,Donghyeon Cho,Gyouk Chu,Minyeong Hwang,Hyeongwon Jang,Changhun Kim,Haechan Kim,Jina Kim,Joowon Kim,Yoonjeon Kim,Kwanhyung Lee,Chanjae Park,Heecheol Yun,Gregor Betz,Eunho Yang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Peer review serves as a backbone of academic research, but in most AI
conferences, the review quality is degrading as the number of submissions
explodes. To reliably detect low-quality reviews, we define misinformed review
points as either "weaknesses" in a review that contain incorrect premises, or
"questions" in a review that can be already answered by the paper. We verify
that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce
ReviewScore indicating if a review point is misinformed. To evaluate the
factuality of each premise of weaknesses, we propose an automated engine that
reconstructs every explicit and implicit premise from a weakness. We build a
human expert-annotated ReviewScore dataset to check the ability of LLMs to
automate ReviewScore evaluation. Then, we measure human-model agreements on
ReviewScore using eight current state-of-the-art LLMs and verify moderate
agreements. We also prove that evaluating premise-level factuality shows
significantly higher agreements than evaluating weakness-level factuality. A
thorough disagreement analysis further supports a potential of fully automated
ReviewScore evaluation.

</details>


### [22] [GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic Discovery in Financial Disclosures](https://arxiv.org/abs/2509.21698)
*Ying Li,Tiejun Ma*

Main category: cs.CL

TL;DR: GRAB是一个金融领域专用的基准测试，包含161万条来自10-K文件的句子，使用弱监督方法自动标注风险类别，用于评估无监督主题模型在金融风险披露分类中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏评估无监督主题模型在10-K风险披露分类任务上的公共基准，这影响了监管和投资决策的有效性。

Method: 结合FinBERT token attention、YAKE关键短语信号和基于分类学的搭配匹配，无需人工标注即可生成句子标签。标签基于包含193个术语映射到21个细粒度类型的风险分类体系。

Result: 创建了GRAB基准数据集，包含固定数据分割和稳健的评估指标（准确率、宏F1、主题BERTScore和基于熵的有效主题数）。

Conclusion: GRAB提供了可复现、标准化的比较框架，支持在金融披露数据上评估经典、基于嵌入、神经和混合主题模型。

Abstract: Risk categorization in 10-K risk disclosures matters for oversight and
investment, yet no public benchmark evaluates unsupervised topic models for
this task. We present GRAB, a finance-specific benchmark with 1.61M sentences
from 8,247 filings and span-grounded sentence labels produced without manual
annotation by combining FinBERT token attention, YAKE keyphrase signals, and
taxonomy-aware collocation matching. Labels are anchored in a risk taxonomy
mapping 193 terms to 21 fine-grained types nested under five macro classes; the
21 types guide weak supervision, while evaluation is reported at the macro
level. GRAB unifies evaluation with fixed dataset splits and robust
metrics--Accuracy, Macro-F1, Topic BERTScore, and the entropy-based Effective
Number of Topics. The dataset, labels, and code enable reproducible,
standardized comparison across classical, embedding-based, neural, and hybrid
topic models on financial disclosures.

</details>


### [23] [Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval](https://arxiv.org/abs/2509.21710)
*Xiaojun Wu,Cehao Yang,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Hui Xiong,Jia Li,Jian Guo*

Main category: cs.CL

TL;DR: ToG-3通过多智能体上下文演进检索机制，解决了传统图增强检索生成方法中静态图索引的局限性，实现了查询和子图的双重动态演进。


<details>
  <summary>Details</summary>
Motivation: 现有图增强检索生成方法面临根本性权衡：依赖高质量图结构但实际受限。手动构建知识图谱成本高昂，自动提取的图谱受限于底层LLM提取器性能，特别是使用小型本地部署模型时。

Method: 提出Think-on-Graph 3.0框架，采用多智能体上下文演进检索机制，动态构建和优化Chunk-Triplets-Community异构图索引，实现查询演进和子图演进的双重演进机制。多智能体系统包括构建器、检索器、反射器和响应器。

Result: 广泛实验表明ToG-3在深度和广度推理基准上均优于对比基线，消融研究证实了MACER框架各组成部分的有效性。

Conclusion: ToG-3通过双重演进的多智能体系统，在推理过程中自适应构建目标图索引，缓解了静态一次性图构建的固有缺陷，即使使用轻量级LLM也能实现深度精确推理。

Abstract: Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the
important paradigm for enhancing Large Language Models (LLMs) with external
knowledge. However, existing approaches face a fundamental trade-off. While
graph-based methods are inherently dependent on high-quality graph structures,
they face significant practical constraints: manually constructed knowledge
graphs are prohibitively expensive to scale, while automatically extracted
graphs from corpora are limited by the performance of the underlying LLM
extractors, especially when using smaller, local-deployed models. This paper
presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces
Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these
limitations. Our core innovation is the dynamic construction and refinement of
a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly
incorporates a dual-evolution mechanism of Evolving Query and Evolving
Sub-Graph for precise evidence retrieval. This approach addresses a critical
limitation of prior Graph-based RAG methods, which typically construct a static
graph index in a single pass without adapting to the actual query. A
multi-agent system, comprising Constructor, Retriever, Reflector, and Responser
agents, collaboratively engages in an iterative process of evidence retrieval,
answer generation, sufficiency reflection, and, crucially, evolving query and
subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively
build a targeted graph index during reasoning, mitigating the inherent
drawbacks of static, one-time graph construction and enabling deep, precise
reasoning even with lightweight LLMs. Extensive experiments demonstrate that
ToG-3 outperforms compared baselines on both deep and broad reasoning
benchmarks, and ablation studies confirm the efficacy of the components of
MACER framework.

</details>


### [24] [ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation](https://arxiv.org/abs/2509.21730)
*Jiho Kim,Junseong Choi,Woosog Chay,Daeun Kyung,Yeonsu Kwon,Yohan Jo,Edward Choi*

Main category: cs.CL

TL;DR: 提出了ProPerSim任务和仿真框架，用于开发能够在家庭场景中提供及时个性化推荐的AI助手，以及基于此的ProPerAssistant系统，通过用户反馈持续学习和适应。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在日常生活中的应用增加，需要不仅反应式而且主动式和个性化的AI助手。虽然主动性和个性化各自有进展，但两者的结合研究不足。

Method: 构建ProPerSim仿真环境，用户代理具有丰富角色，与助手互动并提供偏好评分。助手目标是利用这些评分学习适应以获得更高分数。提出ProPerAssistant，一个检索增强、偏好对齐的助手，通过用户反馈持续学习。

Result: 在32个不同角色上的实验表明，ProPerAssistant能够调整策略并稳步提高用户满意度。

Conclusion: 结合主动性和个性化具有良好前景，ProPerSim框架和ProPerAssistant系统展示了通过用户反馈持续学习和适应的有效性。

Abstract: As large language models (LLMs) become increasingly integrated into daily
life, there is growing demand for AI assistants that are not only reactive but
also proactive and personalized. While recent advances have pushed forward
proactivity and personalization individually, their combination remains
underexplored. To bridge this gap, we introduce ProPerSim, a new task and
simulation framework for developing assistants capable of making timely,
personalized recommendations in realistic home scenarios. In our simulation
environment, a user agent with a rich persona interacts with the assistant,
providing ratings on how well each suggestion aligns with its preferences and
context. The assistant's goal is to use these ratings to learn and adapt to
achieve higher scores over time. Built on ProPerSim, we propose
ProPerAssistant, a retrieval-augmented, preference-aligned assistant that
continually learns and adapts through user feedback. Experiments across 32
diverse personas show that ProPerAssistant adapts its strategy and steadily
improves user satisfaction, highlighting the promise of uniting proactivity and
personalization.

</details>


### [25] [How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?](https://arxiv.org/abs/2509.21732)
*Xiliang Zhu,Shi Zong,David Rossouw*

Main category: cs.CL

TL;DR: 该论文研究了在长上下文环境中使用大语言模型进行多问题回答的挑战，发现经过微调的小型公开模型在准确性上可以超越GPT-4o等大型专有模型。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中，基于相同上下文回答多个问题时，大语言模型面临高计算成本和延迟的挑战，需要寻找更高效、成本更低的解决方案。

Method: 对专有和公开模型进行广泛实验和基准测试，评估它们在相同对话上下文中回答多个问题的能力。

Result: 虽然GPT-4o等专有模型整体表现最佳，但经过微调的8B参数公开模型在准确性上可以超越GPT-4o。

Conclusion: 微调的小型公开模型具有在实际应用中实现透明且成本效益高的部署潜力。

Abstract: Deploying Large Language Models (LLMs) for question answering (QA) over
lengthy contexts is a significant challenge. In industrial settings, this
process is often hindered by high computational costs and latency, especially
when multiple questions must be answered based on the same context. In this
work, we explore the capabilities of LLMs to answer multiple questions based on
the same conversational context. We conduct extensive experiments and benchmark
a range of both proprietary and public models on this challenging task. Our
findings highlight that while strong proprietary LLMs like GPT-4o achieve the
best overall performance, fine-tuned public LLMs with up to 8 billion
parameters can surpass GPT-4o in accuracy, which demonstrates their potential
for transparent and cost-effective deployment in real-world applications.

</details>


### [26] [Self-Speculative Biased Decoding for Faster Live Translation](https://arxiv.org/abs/2509.21740)
*Linxiao Zeng,Haoyun Deng,Kangyuan Shu,Shizhen Wang*

Main category: cs.CL

TL;DR: 提出了一种自推测偏置解码方法，用于加速流式应用中的文本生成，无需草稿计算，在保持质量的同时实现1.7倍加速和80%的闪烁减少。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在流式应用（如实时翻译）中需要不断更新输出的挑战，避免重复从头生成输出，同时满足延迟要求。

Method: 使用最近输出作为草稿，在验证阶段偏向草稿标记以提高接受率，无需草稿计算，是模型无关的即插即用解决方案。

Result: 在同时文本重翻译任务中，相比传统自回归重翻译实现1.7倍加速，质量无损失，结合显示掩码技术减少80%闪烁。

Conclusion: 自推测偏置解码是一种有效的流式应用加速方法，无需额外草稿计算，显著改善用户体验和系统性能。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in various text generation tasks. However, it remains challenging
to use them off-the-shelf in streaming applications (such as live translation),
where the output must continually update as the input context expands, while
still maintaining a reasonable computational cost to meet the latency
requirement.
  In this work, we reexamine the re-translation approach to simultaneous
translation and propose Self-Speculative Biased Decoding, a novel inference
paradigm designed to avoid repeatedly generating output from scratch for a
consistently growing input stream. We propose using the most recent output as a
draft for the current growing input context. During the verification stage, the
output will be biased towards the draft token for a higher draft acceptance
rate. This strategy not only minimizes flickering that might distract users but
also leads to higher speedups. Conventional decoding may take charge from the
point of divergence after draft verification and continue until the end
condition is met.
  Unlike existing speculative decoding strategies, our approach eliminates the
need for draft computations, making it a model-agnostic and plug-and-play
solution for accelerating latency-sensitive streaming applications.
Experimental results on simultaneous text-to-text re-translation demonstrate
that our approach achieves up to 1.7x speedup compared to conventional
auto-regressive re-translation without compromising quality. Additionally, it
significantly reduces flickering by 80% by incorporating the display-only
mask-k technique.

</details>


### [27] [Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2509.21749)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Junsong Yuan,Yiwei Wang*

Main category: cs.CL

TL;DR: TwS框架通过音频思维链（Audio CoT）增强大型音频语言模型，使其能够在复杂声学场景中进行音频推理，显著提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大型音频语言模型在复杂声学场景中的音频推理任务上表现不佳，缺乏噪声抑制、源分离等声学工具支持，需要增强其音频分析能力。

Method: 提出Thinking-with-Sound框架，结合语言推理和实时音频域分析，使模型能够主动对音频信号进行数值分析和数字处理。

Result: 在MELD-Hard1k基准测试中，TwS显著提升模型鲁棒性，小模型准确率提升24.73%，大模型提升36.61%，而现有SOTA模型在扰动音频上准确率下降超过50%。

Conclusion: Audio CoT无需重新训练即可显著增强音频理解系统的鲁棒性，为开发更稳健的音频理解系统开辟了新方向。

Abstract: Recent Large Audio-Language Models (LALMs) have shown strong performance on
various audio understanding tasks such as speech translation and Audio Q\&A.
However, they exhibit significant limitations on challenging audio reasoning
tasks in complex acoustic scenarios. These situations would greatly benefit
from the use of acoustic tools like noise suppression, source separation, and
precise temporal alignment, but current LALMs lack access to such tools. To
address this limitation, we introduce Thinking-with-Sound (TwS), a framework
that equips LALMs with Audio CoT by combining linguistic reasoning with
on-the-fly audio-domain analysis. Unlike existing approaches that treat audio
as static input, TwS enables models to actively think with audio signals,
performing numerical analysis and digital manipulation through multimodal
reasoning. To evaluate this approach, we construct MELD-Hard1k, a new
robustness benchmark created by introducing various acoustic perturbations.
Experiments reveal that state-of-the-art LALMs suffer dramatic performance
degradation on MELD-Hard1k, with accuracy dropping by more than $50\%$ compared
to clean audio. TwS achieves substantial improvements in robustness,
demonstrating both effectiveness and scalability: small models gain $24.73\%$
absolute accuracy, with improvements scaling consistently up to $36.61\%$ for
larger models. Our findings demonstrate that Audio CoT can significantly
enhance robustness without retraining, opening new directions for developing
more robust audio understanding systems.

</details>


### [28] [SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation](https://arxiv.org/abs/2509.21777)
*Vianne R. Gao,Chen Xue,Marc Versage,Xie Zhou,Zhongruo Wang,Chao Li,Yeon Seonwoo,Nan Chen,Zhen Ge,Gourab Kundu,Weiqi Zhang,Tian Wang,Qingjun Cui,Trishul Chilimbi*

Main category: cs.CL

TL;DR: SynerGen是一个统一的生成式推荐模型，通过单一生成架构同时处理个性化搜索和推荐任务，在检索和排序任务上都表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决传统检索-排序管道中的校准问题和工程开销，以及现有生成模型在统一个性化搜索和推荐时的性能权衡问题。

Method: 使用基于行为序列训练的仅解码器Transformer，结合InfoNCE检索损失和混合点对-配对排序损失进行联合优化，并提出时间感知旋转位置嵌入。

Result: 在广泛采用的推荐和搜索基准测试中，相比强大的生成式推荐器和联合搜索推荐基线，取得了显著改进。

Conclusion: 这项工作证明了单一生成基础模型在工业级统一信息访问中的可行性。

Abstract: The dominant retrieve-then-rank pipeline in large-scale recommender systems
suffers from mis-calibration and engineering overhead due to its architectural
split and differing optimization objectives. While recent generative sequence
models have shown promise in unifying retrieval and ranking by
auto-regressively generating ranked items, existing solutions typically address
either personalized search or query-free recommendation, often exhibiting
performance trade-offs when attempting to unify both. We introduce
\textit{SynerGen}, a novel generative recommender model that bridges this
critical gap by providing a single generative backbone for both personalized
search and recommendation, while simultaneously excelling at retrieval and
ranking tasks. Trained on behavioral sequences, our decoder-only Transformer
leverages joint optimization with InfoNCE for retrieval and a hybrid
pointwise-pairwise loss for ranking, allowing semantic signals from search to
improve recommendation and vice versa. We also propose a novel time-aware
rotary positional embedding to effectively incorporate time information into
the attention mechanism. \textit{SynerGen} achieves significant improvements on
widely adopted recommendation and search benchmarks compared to strong
generative recommender and joint search and recommendation baselines. This work
demonstrates the viability of a single generative foundation model for
industrial-scale unified information access.

</details>


### [29] [Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference](https://arxiv.org/abs/2509.21791)
*Han Yuan,Yue Zhao,Li Zhang,Wuqiong Luo,Zheng Ma*

Main category: cs.CL

TL;DR: 使用因果推断方法分析结构化输出对LLMs生成质量的影响，发现粗粒度指标显示混合结果，但因果分析显示在大多数情况下结构化输出没有因果影响。


<details>
  <summary>Details</summary>
Motivation: 先前研究对结构化输出对LLM生成质量的影响存在矛盾结论，且评估方法存在测试场景受限、比较设置控制不足和依赖粗粒度指标等局限性。

Method: 基于一个假设和两个保证约束，推导出五种潜在因果结构，在七个公共任务和一个开发推理任务上应用因果推断分析。

Result: 粗粒度指标显示结构化输出对GPT-4o生成有正、负或中性影响，但因果推断显示48个场景中43个没有因果影响，其余5个中3个涉及由具体指令影响的多方面因果结构。

Conclusion: 结构化输出对LLM生成质量的影响比先前研究显示的更为复杂，因果分析揭示了粗粒度指标可能误导的潜在机制。

Abstract: Structured output from large language models (LLMs) has enhanced efficiency
in processing generated information and is increasingly adopted in industrial
applications. Prior studies have investigated the impact of structured output
on LLMs' generation quality, often presenting one-way findings. Some suggest
that structured format enhances completeness and factual accuracy, while others
argue that it restricts the reasoning capacity of LLMs and leads to reductions
in standard evaluation metrics. Potential limitations of these assessments
include restricted testing scenarios, weakly controlled comparative settings,
and reliance on coarse metrics. In this work, we present a refined analysis
using causal inference. Based on one assumed and two guaranteed constraints, we
derive five potential causal structures characterizing the influence of
structured output on LLMs' generation: (1) collider without m-bias, (2)
collider with m-bias, (3) single cause from instruction, (4) single cause from
output format, and (5) independence. Across seven public and one developed
reasoning tasks, we find that coarse metrics report positive, negative, or
neutral effects of structured output on GPT-4o's generation. However, causal
inference reveals no causal impact in 43 out of 48 scenarios. In the remaining
5, 3 involve multifaceted causal structures influenced by concrete
instructions.

</details>


### [30] [Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment](https://arxiv.org/abs/2509.21798)
*Hongbin Zhang,Kehai Chen,Xuefeng Bai,Yang Xiang,Min Zhang*

Main category: cs.CL

TL;DR: 提出了CARB基准来评估奖励模型的文化意识，发现现有模型存在缺陷，并提出了Think-as-Locals方法通过强化学习来改进文化意识奖励建模。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估缺乏文化意识评估数据集，需要推进LLM的全球对齐。

Method: 构建CARB基准覆盖10种文化4个领域，提出Think-as-Locals方法通过RLVR进行文化推理。

Result: 评估显示现有RM在文化意识建模上存在缺陷，Think-as-Locals能有效缓解虚假特征干扰。

Conclusion: CARB基准能有效评估文化意识，Think-as-Locals方法能推进文化意识奖励建模。

Abstract: Reward models (RMs) are crucial for aligning large language models (LLMs)
with diverse cultures. Consequently, evaluating their cultural awareness is
essential for further advancing global alignment of LLMs. However, existing RM
evaluations fall short in assessing cultural awareness due to the scarcity of
culturally relevant evaluation datasets. To fill this gap, we propose Cultural
Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures
across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs
reveals their deficiencies in modeling cultural awareness and demonstrates a
positive correlation between performance on CARB and downstream multilingual
cultural alignment tasks. Further analysis identifies the spurious correlations
within culture-aware reward modeling, wherein RM's scoring relies predominantly
on surface-level features rather than authentic cultural nuance understanding.
To address these, we propose Think-as-Locals to elicit deeper culturally
grounded reasoning from generative RMs via reinforcement learning from
verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate
preference judgments and high-quality structured evaluation criteria
generation. Experimental results validate its efficacy in mitigating spurious
features interference and advancing culture-aware reward modeling.

</details>


### [31] [Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies](https://arxiv.org/abs/2509.21801)
*Qianen Zhang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本文提出了一种扩展动作空间的同步机器翻译方法，通过引入四种自适应动作来改善实时翻译的质量和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 传统的编码器-解码器策略仅包含READ/WRITE动作，无法完全满足同步机器翻译在严格实时约束下的高质量翻译需求。

Method: 在仅解码器的大型语言模型框架中实现四种自适应动作：句子切分、丢弃、部分摘要和代词化，并通过动作感知提示构建训练参考。

Result: 在ACL60/60英中和英德基准测试中，该框架持续改善了语义指标（如COMET-KIWI）并实现了更低的延迟（通过平均滞后衡量）。

Conclusion: 丰富基于LLM的同步机器翻译的动作空间为弥合人机口译差距提供了一个有前景的方向。

Abstract: Simultaneous Machine Translation (SiMT) requires high-quality translations
under strict real-time constraints, which traditional encoder-decoder policies
with only READ/WRITE actions cannot fully address. We extend the action space
of SiMT with four adaptive actions: SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION
and PRONOMINALIZATION, which enable real-time restructuring, omission, and
simplification while preserving semantic fidelity. We implement these actions
in a decoder-only large language model (LLM) framework and construct training
references through action-aware prompting. To evaluate both quality and
latency, we further develop a latency-aware TTS pipeline that maps textual
outputs to speech with realistic timing. Experiments on the ACL60/60
English-Chinese and English-German benchmarks show that our framework
consistently improves semantic metrics (e.g., COMET-KIWI) and achieves lower
delay (measured by Average Lagging) compared to reference translations and
salami-based baselines. Notably, combining DROP and SENTENCE_CUT yields the
best overall balance between fluency and latency. These results demonstrate
that enriching the action space of LLM-based SiMT provides a promising
direction for bridging the gap between human and machine interpretation.

</details>


### [32] [Towards Minimal Causal Representations for Human Multimodal Language Understanding](https://arxiv.org/abs/2509.21805)
*Menghua Jiang,Yuncheng Jiang,Haifeng Hu,Sijie Mai*

Main category: cs.CL

TL;DR: 提出CaMIB模型，通过因果多模态信息瓶颈方法解决传统多模态学习中的数据集偏差问题，提升模型在分布外场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于"学习注意力"的多模态理解方法容易受到数据集偏差影响，将统计捷径与真实因果特征混淆，导致分布外泛化性能下降。

Method: 使用信息瓶颈过滤单模态输入中的任务无关噪声，通过参数化掩码生成器将多模态表示解耦为因果和捷径子表示，并引入工具变量约束和后门调整来稳定因果估计。

Result: 在多模态情感分析、幽默检测和讽刺检测任务上的广泛实验表明CaMIB的有效性，特别是在分布外测试集上表现优异。

Conclusion: CaMIB模型通过因果原则而非传统似然方法，显著提升了多模态语言理解的泛化能力和可解释性。

Abstract: Human Multimodal Language Understanding (MLU) aims to infer human intentions
by integrating related cues from heterogeneous modalities. Existing works
predominantly follow a ``learning to attend" paradigm, which maximizes mutual
information between data and labels to enhance predictive performance. However,
such methods are vulnerable to unintended dataset biases, causing models to
conflate statistical shortcuts with genuine causal features and resulting in
degraded out-of-distribution (OOD) generalization. To alleviate this issue, we
introduce a Causal Multimodal Information Bottleneck (CaMIB) model that
leverages causal principles rather than traditional likelihood. Concretely, we
first applies the information bottleneck to filter unimodal inputs, removing
task-irrelevant noise. A parameterized mask generator then disentangles the
fused multimodal representation into causal and shortcut subrepresentations. To
ensure global consistency of causal features, we incorporate an instrumental
variable constraint, and further adopt backdoor adjustment by randomly
recombining causal and shortcut features to stabilize causal estimation.
Extensive experiments on multimodal sentiment analysis, humor detection, and
sarcasm detection, along with OOD test sets, demonstrate the effectiveness of
CaMIB. Theoretical and empirical analyses further highlight its
interpretability and soundness.

</details>


### [33] [Can LLMs Solve and Generate Linguistic Olympiad Puzzles?](https://arxiv.org/abs/2509.21820)
*Neh Majmudar,Elena Filatova*

Main category: cs.CL

TL;DR: 本文研究了语言谜题的解决与生成，扩展了现有基准，探索了LLM在解决语言谜题中的表现，并基于此引导了谜题生成的新任务。


<details>
  <summary>Details</summary>
Motivation: 自动化谜题生成有望扩大对语言学的兴趣，向更广泛的受众介绍该领域，并支持关于稀有和未充分研究语言的知识传播。

Method: 扩展了解决语言谜题的现有基准，探索了包括OpenAI o1在内的LLM在各种语言主题上的表现，并利用谜题解决实验的见解来指导谜题生成任务。

Result: LLM在大多数谜题类型上优于人类，除了以书写系统为中心的谜题和未充分研究语言的谜题。

Conclusion: 语言谜题生成作为一个研究任务具有重要意义，这类谜题不仅能促进语言学发展，还能支持关于稀有语言的知识传播。

Abstract: In this paper, we introduce a combination of novel and exciting tasks: the
solution and generation of linguistic puzzles. We focus on puzzles used in
Linguistic Olympiads for high school students. We first extend the existing
benchmark for the task of solving linguistic puzzles. We explore the use of
Large Language Models (LLMs), including recent state-of-the-art models such as
OpenAI's o1, for solving linguistic puzzles, analyzing their performance across
various linguistic topics. We demonstrate that LLMs outperform humans on most
puzzles types, except for those centered on writing systems, and for the
understudied languages. We use the insights from puzzle-solving experiments to
direct the novel task of puzzle generation. We believe that automating puzzle
generation, even for relatively simple puzzles, holds promise for expanding
interest in linguistics and introducing the field to a broader audience. This
finding highlights the importance of linguistic puzzle generation as a research
task: such puzzles can not only promote linguistics but also support the
dissemination of knowledge about rare and understudied languages.

</details>


### [34] [ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models](https://arxiv.org/abs/2509.21826)
*Zihan Lin,Xiaohan Wang,Jie Cao,Jiajun Chai,Guojun Yin,Wei Lin,Ran He*

Main category: cs.CL

TL;DR: 提出了ResT方法，通过基于熵的token重加权来重塑策略梯度，优化LLM工具使用任务的训练效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在优化LLM工具使用策略时仅依赖稀疏结果奖励，缺乏对工具使用任务特殊性的考虑，导致策略梯度方差大、训练效率低。

Method: ResT通过熵感知的token重加权来重塑策略梯度，在训练过程中逐步增加推理token的权重，实现从结构正确性到语义推理的平滑过渡。

Result: 在BFCL和API-Bank上的评估显示，ResT达到最先进结果，比先前方法提升高达8.76%。在4B基础LLM上微调后，ResT在单轮任务上超过GPT-4o 4.11%，在多轮基础任务上超过1.50%。

Conclusion: ResT通过熵感知的token级策略梯度重塑，有效解决了工具使用任务中的训练稳定性问题，显著提升了LLM作为目标导向代理的性能。

Abstract: Large language models (LLMs) transcend passive generation and act as
goal-directed agents by invoking external tools. Reinforcement learning (RL)
offers a principled framework for optimizing these emergent tool-use policies,
yet the prevailing paradigm relies exclusively on sparse outcome rewards and
lacks consideration of the particularity of tool-use tasks, inflating
policy-gradient variance and resulting in inefficient training. To better
understand and address these challenges, we first establish a theoretical link
between policy entropy and training stability of tool-use tasks, which reveals
that structured, low-entropy tokens are primary determinants of rewards.
Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level
policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy
gradient through entropy-informed token reweighting, progressively upweighting
reasoning tokens as training proceeds. This entropy-aware scheme enables a
smooth shift from structural correctness to semantic reasoning and stabilizes
convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows
that ResT achieves state-of-the-art results, outperforming prior methods by up
to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by
$4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.

</details>


### [35] [Semantic Agreement Enables Efficient Open-Ended LLM Cascades](https://arxiv.org/abs/2509.21837)
*Duncan Soiffer,Steven Kolawole,Virginia Smith*

Main category: cs.CL

TL;DR: 提出基于语义一致性的级联系统，通过模型输出的语义共识来判断可靠性，实现成本降低40%、延迟减少60%的效果


<details>
  <summary>Details</summary>
Motivation: 级联系统面临在开放文本生成中判断输出可靠性的挑战，因为生成质量是连续谱且存在多个有效响应

Method: 使用语义一致性作为无需训练的信号，当不同模型输出在语义层面达成共识时，将其作为可靠性指标

Result: 在500M到70B参数模型上验证，语义级联以40%成本达到或超过目标模型质量，延迟减少高达60%

Conclusion: 该方法无需模型内部信息，适用于黑盒API，对模型更新具有鲁棒性，为实际LLM部署提供了实用基准

Abstract: Cascade systems route computational requests to smaller models when possible
and defer to larger models only when necessary, offering a promising approach
to balance cost and quality in LLM deployment. However, they face a fundamental
challenge in open-ended text generation: determining output reliability when
generation quality lies on a continuous spectrum, often with multiple valid
responses. To address this, we propose semantic agreement -- meaning-level
consensus between ensemble outputs -- as a training-free signal for reliable
deferral. We show that when diverse model outputs agree semantically, their
consensus is a stronger reliability signal than token-level confidence.
Evaluated from 500M to 70B-parameter models, we find that semantic cascades
match or surpass target-model quality at 40% of the cost and reduce latency by
up to 60%. Our method requires no model internals, works across black-box APIs,
and remains robust to model updates, making it a practical baseline for
real-world LLM deployment.

</details>


### [36] [Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models](https://arxiv.org/abs/2509.21849)
*Ziqi Liu,Ziyang Zhou,Yilin Li,Haiyang Zhang,Yangbin Chen*

Main category: cs.CL

TL;DR: 提出了TRACE框架，通过将共情任务分解为分析和合成的管道，将共情建模为结构化认知过程，在生成前建立全面理解，统一深度分析和表达生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在专业模型的分析深度和大型语言模型的生成流畅性之间存在核心权衡，需要解决这一矛盾。

Method: TRACE框架，任务分解推理用于情感沟通和共情，将共情任务分解为分析和合成的管道，建模为结构化认知过程。

Result: 实验结果显示该框架在自动和基于LLM的评估中显著优于强基线。

Conclusion: 结构化分解是创建更强大和可解释共情代理的有前景范式。

Abstract: Empathetic response generation is a crucial task for creating more human-like
and supportive conversational agents. However, existing methods face a core
trade-off between the analytical depth of specialized models and the generative
fluency of Large Language Models (LLMs). To address this, we propose TRACE,
Task-decomposed Reasoning for Affective Communication and Empathy, a novel
framework that models empathy as a structured cognitive process by decomposing
the task into a pipeline for analysis and synthesis. By building a
comprehensive understanding before generation, TRACE unites deep analysis with
expressive generation. Experimental results show that our framework
significantly outperforms strong baselines in both automatic and LLM-based
evaluations, confirming that our structured decomposition is a promising
paradigm for creating more capable and interpretable empathetic agents. Our
code is available at https://anonymous.4open.science/r/TRACE-18EF/README.md.

</details>


### [37] [KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues](https://arxiv.org/abs/2509.21856)
*Junhao Chen,Yu Huang,Siyuan Li,Rui Yao,Hanqian Li,Hanyu Zhang,Jungang Li,Jian Chen,Bowen Wang,Xuming Hu*

Main category: cs.CL

TL;DR: 提出了第一个专门评估多轮长格式问答（MT-LFQA）的基准KnowMT-Bench，用于测试大语言模型在医学、金融和法律等知识密集型领域的表现。研究发现多轮对话会降低模型的事实准确性和信息效率，而检索增强生成（RAG）可以有效缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注单轮对话，而多轮对话基准通常评估其他能力而非知识密集型的事实准确性。为了填补这一关键空白，需要开发专门评估MT-LFQA的基准。

Method: 使用动态评估设置，让模型基于逻辑递进的问题序列生成自己的多轮对话历史，然后通过人工验证的自动化流程评估最终轮答案的事实能力和信息传递效率。

Result: 多轮上下文会降低性能：事实能力因自生成历史中的上下文噪声而下降，信息效率随着对话长度增加而降低（模型变得更啰嗦）。检索增强生成（RAG）可以有效缓解甚至逆转这种事实退化。

Conclusion: KnowMT-Bench基准对于评估和增强LLMs在现实世界知识密集型应用中的对话事实能力具有重要意义。

Abstract: Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application
paradigm of Large Language Models (LLMs) in knowledge-intensive domains.
However, existing benchmarks are limited to single-turn dialogue, while
multi-turn dialogue benchmarks typically assess other orthogonal capabilities
rather than knowledge-intensive factuality. To bridge this critical gap, we
introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to
systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields,
including medicine, finance, and law. To faithfully assess the model's
real-world performance, KnowMT-Bench employs a dynamic evaluation setting where
models generate their own multi-turn dialogue histories given logically
progressive question sequences. The factual capability and information delivery
efficiency of the \textit{final-turn} answer are then evaluated using a
human-validated automated pipeline. Our experiments reveal that multi-turn
contexts degrade performance: factual capability declines due to the contextual
noise from self-generated histories, while information efficiency drops as
models become more verbose with increasing dialogue length. We then investigate
mitigation strategies, demonstrating that retrieval-augmented generation (RAG)
can effectively alleviate and even reverse this factual degradation. These
findings underscore the importance of our benchmark in evaluating and enhancing
the conversational factual capabilities of LLMs in real-world
knowledge-intensive applications. Code is available at
\href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.

</details>


### [38] [Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations](https://arxiv.org/abs/2509.21870)
*Guanzhi Deng,Mingyang Liu,Dapeng Wu,Yinqiao Li,Linqi Song*

Main category: cs.CL

TL;DR: LoRAN是LoRA的非线性扩展，通过轻量级变换增强低秩更新的表达能力，并引入Sinter正弦激活函数，在多个任务上优于QLoRA。


<details>
  <summary>Details</summary>
Motivation: LoRA的线性特性限制了表达能力，需要非线性扩展来提升性能。

Method: 提出LoRAN方法，对低秩更新应用轻量级非线性变换，并引入Sinter正弦激活函数，在不增加参数的情况下添加结构化扰动。

Result: 在摘要生成和分类任务上的实验表明，LoRAN持续优于QLoRA。消融研究显示Sinter优于Sigmoid、ReLU和Tanh等标准激活函数。

Conclusion: 激活函数设计在低秩调优中至关重要，LoRAN通过非线性扩展有效提升了LoRA的性能。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning method for large language models. However, its linear nature limits
expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies
lightweight transformations to the low-rank updates. We further introduce
Sinter, a sine-based activation that adds structured perturbations without
increasing parameter count. Experiments across summarization and classification
tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal
that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh,
highlighting the importance of activation design in lowrank tuning.

</details>


### [39] [LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals](https://arxiv.org/abs/2509.21875)
*Min-Hsuan Yeh,Yixuan Li,Tanwi Mallick*

Main category: cs.CL

TL;DR: LUMINA是一个检测RAG系统中幻觉的新框架，通过量化外部上下文利用和内部知识利用来识别幻觉，无需大量超参数调优，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: RAG系统即使提供正确充分的上下文仍会产生幻觉，现有方法需要大量超参数调优，限制了通用性。

Method: 通过分布距离量化外部上下文利用，通过跟踪transformer层间预测token演变测量内部知识利用，并引入统计验证框架。

Result: 在常见RAG幻觉基准测试和四个开源LLM上，LUMINA实现了高AUROC和AUPRC分数，在HalluRAG上比现有方法提升高达+13% AUROC。

Conclusion: LUMINA在检索质量和模型匹配的宽松假设下保持鲁棒性，兼具有效性和实用性。

Abstract: Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large
language models (LLMs) by grounding responses in retrieved documents. Yet,
RAG-based LLMs still hallucinate even when provided with correct and sufficient
context. A growing line of work suggests that this stems from an imbalance
between how models use external context and their internal knowledge, and
several approaches have attempted to quantify these signals for hallucination
detection. However, existing methods require extensive hyperparameter tuning,
limiting their generalizability. We propose LUMINA, a novel framework that
detects hallucinations in RAG systems through context-knowledge signals:
external context utilization is quantified via distributional distance, while
internal knowledge utilization is measured by tracking how predicted tokens
evolve across transformer layers. We further introduce a framework for
statistically validating these measurements. Experiments on common RAG
hallucination benchmarks and four open-source LLMs show that LUMINA achieves
consistently high AUROC and AUPRC scores, outperforming prior utilization-based
methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under
relaxed assumptions about retrieval quality and model matching, offering both
effectiveness and practicality.

</details>


### [40] [No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping](https://arxiv.org/abs/2509.21880)
*Thanh-Long V. Le,Myeongho Jeon,Kim Vu,Viet Lai,Eunho Yang*

Main category: cs.CL

TL;DR: RL-ZVP是一种新的强化学习算法，能够从零方差提示中提取学习信号，在数学推理基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法如GRPO只关注模型对同一输入产生不同正确性的响应，而忽略了所有响应获得相同奖励的零方差提示。本文认为这些提示并非无用，实际上可以为策略优化提供有意义的反馈。

Method: 引入RL-ZVP算法，直接从零方差提示中提取学习信号。该方法直接奖励正确性并惩罚错误，即使没有对比响应，通过调节基于标记级别特征的反馈来保留信息丰富、细致的信号。

Result: 在六个数学推理基准测试中，RL-ZVP相比GRPO在准确率上提升高达8.61个百分点，通过率提升7.77个百分点，同时持续优于过滤掉零方差提示的其他基线方法。

Conclusion: 这些结果凸显了在RLVR中从零方差提示学习的未开发潜力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework
for improving the reasoning abilities of Large Language Models (LLMs). However,
current methods such as GRPO rely only on problems where the model responses to
the same input differ in correctness, while ignoring those where all responses
receive the same reward - so-called zero-variance prompts. In this work, we
argue that such prompts are not useless but can, in fact, provide meaningful
feedback for policy optimization. To this end, we introduce RL with
Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals
from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes
errors even without contrasting responses, modulating feedback with token-level
characteristics to preserve informative, nuanced signals. Across six math
reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61
points in accuracy and 7.77 points in pass rate over GRPO, while consistently
outperforming other baselines that filter out zero-variance prompts. These
results highlight the untapped potential of learning from zero-variance prompts
in RLVR.

</details>


### [41] [QoNext: Towards Next-generation QoE for Foundation Models](https://arxiv.org/abs/2509.21889)
*Yijin Guo,Ye Shen,Farong Wen,Junying Wang,Zicheng Zhang,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: QoNext框架将网络和多媒体领域的QoE原则应用于基础模型评估，关注用户交互体验而非仅输出正确性，通过实验构建QoE数据库并训练预测模型来估计用户感知体验。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型评估方法只关注输出正确性，忽略了用户满意度来自响应质量和交互体验的相互作用，无法解释用户体验的底层机制。

Method: 引入QoNext框架，识别影响用户体验的因素，在受控实验中收集不同配置下的人类评分，构建QoE导向数据库并训练预测模型。

Result: QoNext实现了主动和细粒度的评估，并为实际产品化服务中的基础模型优化提供了可操作的指导。

Conclusion: QoNext框架成功将QoE原则应用于基础模型评估，填补了现有方法在用户体验评估方面的空白。

Abstract: Existing evaluations of foundation models, including recent human-centric
approaches, fail to capture what truly matters: user's experience during
interaction. Current methods treat evaluation as a matter of output correctness
alone, overlooking that user satisfaction emerges from the interplay between
response quality and interaction, which limits their ability to account for the
mechanisms underlying user experience. To address this gap, we introduce
QoNext, the first framework that adapts Quality of Experience (QoE) principles
from networking and multimedia to the assessment of foundation models. QoNext
identifies experiential factors that shape user experience and incorporates
them into controlled experiments, where human ratings are collected under
varied configurations. From these studies we construct a QoE-oriented database
and train predictive models that estimate perceived user experience from
measurable system parameters. Our results demonstrate that QoNext not only
enables proactive and fine-grained evaluation but also provides actionable
guidance for productized services of optimizing foundation models in practice.

</details>


### [42] [Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts](https://arxiv.org/abs/2509.21892)
*Naibin Gu,Zhenyu Zhang,Yuchen Feng,Yilong Chen,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Main category: cs.CL

TL;DR: EMoE训练框架解决了MoE模型在推理时激活更多专家导致性能下降的问题，通过训练专家在多样化组合中协作，将有效性能扩展范围提升至训练时k值的2-3倍。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型在推理时增加激活专家数量反而导致性能下降，这源于专家之间缺乏学习到的协作能力。

Method: 提出Elastic Mixture-of-Experts (EMoE)训练框架，同时训练专家在多样化组合中协作，并鼓励路由器进行高质量选择，无需额外训练开销。

Result: EMoE显著扩展了有效性能扩展范围，可达训练时k值的2-3倍，同时将模型峰值性能推向更高水平。

Conclusion: EMoE框架使MoE模型能够在推理时灵活扩展激活专家数量，同时保持稳健性能，解决了传统MoE的扩展限制问题。

Abstract: Mixture-of-Experts (MoE) models typically fix the number of activated experts
$k$ at both training and inference. Intuitively, activating more experts at
inference $k'$ (where $k'> k$) means engaging a larger set of model parameters
for the computation and thus is expected to improve performance. However,
contrary to this intuition, we find the scaling range to be so narrow that
performance begins to degrade rapidly after only a slight increase in the
number of experts. Further investigation reveals that this degradation stems
from a lack of learned collaboration among experts. To address this, we
introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that
enables MoE models to scale the number of activated experts at inference
without incurring additional training overhead. By simultaneously training
experts to collaborate in diverse combinations and encouraging the router for
high-quality selections, EMoE ensures robust performance across computational
budgets at inference. We conduct extensive experiments on various MoE settings.
Our results show that EMoE significantly expands the effective
performance-scaling range, extending it to as much as 2-3$\times$ the
training-time $k$, while also pushing the model's peak performance to a higher
level.

</details>


### [43] [A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs](https://arxiv.org/abs/2509.21907)
*Kemal Sami Karaca,Bahaeddin Eravcı*

Main category: cs.CL

TL;DR: 本文针对土耳其语等黏着性语言的引文意图分类问题，提出了系统化方法并创建了首个土耳其语引文意图数据集，通过DSPy框架自动优化提示工程，采用堆叠泛化集成方法实现了91.3%的最先进准确率。


<details>
  <summary>Details</summary>
Motivation: 理解引文的定性意图对于学术研究评估至关重要，但土耳其语等黏着性语言在此任务上面临独特挑战，需要专门的数据集和方法来解决这一问题。

Method: 首先创建了公开可用的土耳其语引文意图数据集，然后使用DSPy框架构建可编程分类管道来自动优化提示工程，最后采用堆叠泛化集成方法（XGBoost元模型）聚合多个优化模型的输出。

Result: 提出的堆叠泛化集成方法实现了91.3%的最先进准确率，显著优于标准上下文学习方法，解决了手动设计提示导致的不一致性问题。

Conclusion: 本研究为土耳其NLP社区和更广泛的学术界提供了基础数据集和稳健的分类框架，为未来的定性引文研究铺平了道路。

Abstract: Understanding the qualitative intent of citations is essential for a
comprehensive assessment of academic research, a task that poses unique
challenges for agglutinative languages like Turkish. This paper introduces a
systematic methodology and a foundational dataset to address this problem. We
first present a new, publicly available dataset of Turkish citation intents,
created with a purpose-built annotation tool. We then evaluate the performance
of standard In-Context Learning (ICL) with Large Language Models (LLMs),
demonstrating that its effectiveness is limited by inconsistent results caused
by manually designed prompts. To address this core limitation, we introduce a
programmable classification pipeline built on the DSPy framework, which
automates prompt optimization systematically. For final classification, we
employ a stacked generalization ensemble to aggregate outputs from multiple
optimized models, ensuring stable and reliable predictions. This ensemble, with
an XGBoost meta-model, achieves a state-of-the-art accuracy of 91.3\%.
Ultimately, this study provides the Turkish NLP community and the broader
academic circles with a foundational dataset and a robust classification
framework paving the way for future qualitative citation studies.

</details>


### [44] [AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition](https://arxiv.org/abs/2509.21910)
*Yun Wang,Zhaojun Ding,Xuansheng Wu,Siyue Sun,Ninghao Liu,Xiaoming Zhai*

Main category: cs.CL

TL;DR: AutoSCORE是一个多智能体LLM框架，通过评分标准对齐的结构化组件识别来提升自动评分性能，解决了传统LLM评分中的准确性低、提示敏感、可解释性差和评分标准不对齐等问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动评分中面临准确性低、提示敏感、可解释性差和评分标准不对齐等挑战，阻碍了其在评估实践中的应用。

Method: 提出AutoSCORE框架，包含两个智能体：评分标准组件提取智能体从学生回答中提取相关组件并编码为结构化表示，评分智能体基于此分配最终分数，模拟人类评分过程。

Result: 在ASAP基准的四个数据集上评估，使用GPT-4o、LLaMA-3.1-8B和LLaMA-3.1-70B模型，AutoSCORE相比单智能体基线在评分准确性、人机一致性（QWK、相关性）和误差指标（MAE、RMSE）上均有提升，特别是在复杂多维评分标准和小型LLM上表现更佳。

Conclusion: 结构化组件识别与多智能体设计相结合，为自动评分提供了可扩展、可靠且可解释的解决方案。

Abstract: Automated scoring plays a crucial role in education by reducing the reliance
on human raters, offering scalable and immediate evaluation of student work.
While large language models (LLMs) have shown strong potential in this task,
their use as end-to-end raters faces challenges such as low accuracy, prompt
sensitivity, limited interpretability, and rubric misalignment. These issues
hinder the implementation of LLM-based automated scoring in assessment
practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM
framework enhancing automated scoring via rubric-aligned Structured COmponent
REcognition. With two agents, AutoSCORE first extracts rubric-relevant
components from student responses and encodes them into a structured
representation (i.e., Scoring Rubric Component Extraction Agent), which is then
used to assign final scores (i.e., Scoring Agent). This design ensures that
model reasoning follows a human-like grading process, enhancing
interpretability and robustness. We evaluate AutoSCORE on four benchmark
datasets from the ASAP benchmark, using both proprietary and open-source LLMs
(GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics,
AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK,
correlations), and error metrics (MAE, RMSE) compared to single-agent
baselines, with particularly strong benefits on complex, multi-dimensional
rubrics, and especially large relative gains on smaller LLMs. These results
demonstrate that structured component recognition combined with multi-agent
design offers a scalable, reliable, and interpretable solution for automated
scoring.

</details>


### [45] [SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2509.21932)
*Haotian Tan,Hiroki Ouchi,Sakriani Sakti*

Main category: cs.CL

TL;DR: 提出SimulSense框架，通过感知新语义单元来触发写决策，实现同时语音翻译中类似人类译员的读写决策


<details>
  <summary>Details</summary>
Motivation: 现有系统将SimulST建模为多轮对话任务，需要专门的交错训练数据和计算昂贵的LLM推理，效率低下

Method: 模拟人类译员，持续读取输入语音并在感知到新语义单元时触发写决策来产生翻译

Result: 相比两个最先进的基线系统，实现了更优的质量-延迟权衡，决策速度提升高达9.6倍

Conclusion: SimulSense框架在同时语音翻译中实现了高效的人类译员式读写决策

Abstract: How to make human-interpreter-like read/write decisions for simultaneous
speech translation (SimulST) systems? Current state-of-the-art systems
formulate SimulST as a multi-turn dialogue task, requiring specialized
interleaved training data and relying on computationally expensive large
language model (LLM) inference for decision-making. In this paper, we propose
SimulSense, a novel framework for SimulST that mimics human interpreters by
continuously reading input speech and triggering write decisions to produce
translation when a new sense unit is perceived. Experiments against two
state-of-the-art baseline systems demonstrate that our proposed method achieves
a superior quality-latency tradeoff and substantially improved real-time
efficiency, where its decision-making is up to 9.6x faster than the baselines.

</details>


### [46] [Why Chain of Thought Fails in Clinical Text Understanding](https://arxiv.org/abs/2509.21933)
*Jiageng Wu,Kevin Xie,Bowen Gu,Nils Krüger,Kueiyu Joshua Lin,Jie Yang*

Main category: cs.CL

TL;DR: 在临床文本理解任务中，思维链提示反而导致86.3%的大型语言模型性能下降，与在其他领域的表现相反。


<details>
  <summary>Details</summary>
Motivation: 评估思维链提示在临床电子健康记录文本理解中的有效性，因为临床领域需要高准确性和透明推理。

Method: 对95个先进LLM在87个真实世界临床文本任务上进行大规模系统研究，涵盖9种语言和8种任务类型。

Result: 86.3%的模型在思维链设置下性能下降，能力较强的模型相对稳健，而较弱模型下降显著。

Conclusion: 思维链在临床文本任务中增强可解释性但可能损害可靠性，揭示了临床推理策略的悖论。

Abstract: Large language models (LLMs) are increasingly being applied to clinical care,
a domain where both accuracy and transparent reasoning are critical for safe
and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits
step-by-step reasoning, has demonstrated improvements in performance and
interpretability across a wide range of tasks. However, its effectiveness in
clinical contexts remains largely unexplored, particularly in the context of
electronic health records (EHRs), the primary source of clinical documentation,
which are often lengthy, fragmented, and noisy. In this work, we present the
first large-scale systematic study of CoT for clinical text understanding. We
assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9
languages and 8 task types. Contrary to prior findings in other domains, we
observe that 86.3\% of models suffer consistent performance degradation in the
CoT setting. More capable models remain relatively robust, while weaker ones
suffer substantial declines. To better characterize these effects, we perform
fine-grained analyses of reasoning length, medical concept alignment, and error
profiles, leveraging both LLM-as-a-judge evaluation and clinical expert
evaluation. Our results uncover systematic patterns in when and why CoT fails
in clinical contexts, which highlight a critical paradox: CoT enhances
interpretability but may undermine reliability in clinical text tasks. This
work provides an empirical basis for clinical reasoning strategies of LLMs,
highlighting the need for transparent and trustworthy approaches.

</details>


### [47] [Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration](https://arxiv.org/abs/2509.21946)
*Kasidit Sermsri,Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: 提出了ThaiFACTUAL框架，通过反事实数据增强和基于理由的监督来减轻泰语政治立场检测中的偏见，无需微调即可提高LLMs的公平性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决泰语政治立场检测中LLMs的系统性偏见问题，包括情感泄漏和实体偏袒，这些偏见在低资源和文化复杂环境中尤为严重。

Method: 使用反事实数据增强和基于理由的监督来解耦情感与立场，减少偏见；发布了首个高质量的泰语政治立场数据集。

Result: ThaiFACTUAL显著减少了伪相关性，增强了零样本泛化能力，并提高了多个LLMs的公平性。

Conclusion: 这项工作强调了针对代表性不足语言的文化基础去偏见技术的重要性。

Abstract: Political stance detection in low-resource and culturally complex settings
poses a critical challenge for large language models (LLMs). In the Thai
political landscape - marked by indirect language, polarized figures, and
entangled sentiment and stance - LLMs often display systematic biases such as
sentiment leakage and favoritism toward entities. These biases undermine
fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic
calibration framework that mitigates political bias without requiring
fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and
rationale-based supervision to disentangle sentiment from stance and reduce
bias. We also release the first high-quality Thai political stance dataset,
annotated with stance, sentiment, rationales, and bias markers across diverse
entities and events. Experimental results show that ThaiFACTUAL significantly
reduces spurious correlations, enhances zero-shot generalization, and improves
fairness across multiple LLMs. This work highlights the importance of
culturally grounded debiasing techniques for underrepresented languages.

</details>


### [48] [MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation](https://arxiv.org/abs/2509.21978)
*Xinping Lei,Tong Zhou,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 提出MotivGraph-SoIQ框架，通过整合动机知识图谱和苏格拉底式对话来增强LLM的学术创意生成能力，解决创意落地和确认偏误问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在加速学术创意生成方面潜力巨大，但面临创意落地困难和确认偏误的挑战，需要新的方法来提供基础支撑和实际改进步骤。

Method: 整合动机知识图谱（存储问题、挑战和解决方案三类节点）和Q驱动的苏格拉底创意生成器（双代理系统使用苏格拉底式提问），为LLM创意过程提供动机基础和严格精炼流程。

Result: 在ICLR25论文主题数据集上，MotivGraph-SoIQ在基于LLM的评分、ELO排名和人工评估指标上均优于现有最先进方法。

Conclusion: 该框架通过结构化动机知识和苏格拉底式精炼，有效提升了LLM创意生成的新颖性、实验严谨性和动机合理性，为学术创意生成提供了实用解决方案。

Abstract: Large Language Models (LLMs) hold substantial potential for accelerating
academic ideation but face critical challenges in grounding ideas and
mitigating confirmation bias for further refinement. We propose integrating
motivational knowledge graphs and socratic dialogue to address these
limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework
provides essential grounding and practical idea improvement steps for LLM
ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a
Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node
types(problem, challenge and solution) to offer motivation grounding for the
LLM ideation process. The Ideator is a dual-agent system utilizing Socratic
questioning, which facilitates a rigorous refinement process that mitigates
confirmation bias and improves idea quality across novelty, experimental rigor,
and motivational rationality dimensions. On the ICLR25 paper topics dataset,
MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art
approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.

</details>


### [49] [Black-Box Hallucination Detection via Consistency Under the Uncertain Expression](https://arxiv.org/abs/2509.21999)
*Seongho Joo,Kyungmin Min,Jahyun Koo,Kyomin Jung*

Main category: cs.CL

TL;DR: 提出了一种基于不确定性的黑盒幻觉检测方法，通过分析LLMs在不确定性表达下的行为模式来检测事实性错误


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法需要外部资源或模型内部状态，但在API受限和外部资源有限的情况下，急需建立有效的黑盒检测方法

Method: 通过研究LLMs在不确定性表达下的行为，发现事实性回答具有一致性，而非事实性回答则不一致，据此提出基于不确定性的黑盒检测指标

Result: 实验表明，该指标在预测模型回答的事实性方面优于使用LLMs内部知识的基线方法

Conclusion: 该方法为黑盒环境下的幻觉检测提供了有效的解决方案，无需依赖模型内部状态或外部资源

Abstract: Despite the great advancement of Language modeling in recent days, Large
Language Models (LLMs) such as GPT3 are notorious for generating non-factual
responses, so-called "hallucination" problems. Existing methods for detecting
and alleviating this hallucination problem require external resources or the
internal state of LLMs, such as the output probability of each token. Given the
LLM's restricted external API availability and the limited scope of external
resources, there is an urgent demand to establish the Black-Box approach as the
cornerstone for effective hallucination detection. In this work, we propose a
simple black-box hallucination detection metric after the investigation of the
behavior of LLMs under expression of uncertainty. Our comprehensive analysis
reveals that LLMs generate consistent responses when they present factual
responses while non-consistent responses vice versa. Based on the analysis, we
propose an efficient black-box hallucination detection metric with the
expression of uncertainty. The experiment demonstrates that our metric is more
predictive of the factuality in model responses than baselines that use
internal knowledge of LLMs.

</details>


### [50] [GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2509.22009)
*Cehao Yang,Xiaojun Wu,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Jia Li,Hui Xiong,Jian Guo*

Main category: cs.CL

TL;DR: GraphSearch是一个新颖的图检索增强生成方法，通过双通道检索策略和模块化框架解决现有GraphRAG方法的浅层检索和低效图数据利用问题。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG方法存在两个核心限制：浅层检索无法获取所有关键证据，以及预构建结构图数据的低效利用，阻碍了复杂查询的有效推理。

Method: 提出GraphSearch，采用代理深度搜索工作流和双通道检索策略，包含六个模块的模块化框架，支持多轮交互和迭代推理。双通道检索包括基于分块文本数据的语义查询和基于结构图数据的关系查询。

Result: 在六个多跳RAG基准测试上的实验结果表明，GraphSearch相比传统策略持续提高了答案准确性和生成质量。

Conclusion: GraphSearch是推进图检索增强生成的一个有前景的方向。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in
LLMs by structurally modeling knowledge through graph-based representations.
However, existing GraphRAG approaches face two core limitations: shallow
retrieval that fails to surface all critical evidence, and inefficient
utilization of pre-constructed structural graph data, which hinders effective
reasoning from complex queries. To address these challenges, we propose
\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel
retrieval for GraphRAG. \textsc{GraphSearch} organizes the retrieval process
into a modular framework comprising six modules, enabling multi-turn
interactions and iterative reasoning. Furthermore, \textsc{GraphSearch} adopts
a dual-channel retrieval strategy that issues semantic queries over chunk-based
text data and relational queries over structural graph data, enabling
comprehensive utilization of both modalities and their complementary strengths.
Experimental results across six multi-hop RAG benchmarks demonstrate that
\textsc{GraphSearch} consistently improves answer accuracy and generation
quality over the traditional strategy, confirming \textsc{GraphSearch} as a
promising direction for advancing graph retrieval-augmented generation.

</details>


### [51] [From Outliers to Topics in Language Models: Anticipating Trends in News Corpora](https://arxiv.org/abs/2509.22030)
*Evangelia Zve,Benjamin Icard,Alice Breton,Lila Sainero,Gauvain Bourgne,Jean-Gabriel Ganascia*

Main category: cs.CL

TL;DR: 论文研究发现，在动态新闻语料库中，被主题建模视为噪声的异常值实际上可以作为新兴主题的弱信号，它们会随时间演变成连贯的主题。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模通常将异常值视为噪声而忽略，但本研究旨在探索这些异常值是否可能预示着新兴主题的出现。

Method: 使用最先进语言模型的向量嵌入和累积聚类方法，在法语和英语新闻数据集（关注企业社会责任和气候变化）中跟踪异常值随时间的变化。

Result: 结果显示，在两种模型和语言中都存在一致模式：异常值倾向于随时间演变成连贯的主题。

Conclusion: 异常值不应被简单地视为噪声，而是可以作为检测新兴主题的重要信号源。

Abstract: This paper examines how outliers, often dismissed as noise in topic modeling,
can act as weak signals of emerging topics in dynamic news corpora. Using
vector embeddings from state-of-the-art language models and a cumulative
clustering approach, we track their evolution over time in French and English
news datasets focused on corporate social responsibility and climate change.
The results reveal a consistent pattern: outliers tend to evolve into coherent
topics over time across both models and languages.

</details>


### [52] [Taxonomy of Comprehensive Safety for Clinical Agents](https://arxiv.org/abs/2509.22041)
*Jean Seo,Hyunkyung Lee,Gibaeg Kim,Wooseok Han,Jaehyo Yoo,Seungseop Lim,Kihun Shin,Eunho Yang*

Main category: cs.CL

TL;DR: TACOS是一个专为临床智能体设计的21类安全分类法，将安全过滤和工具选择整合到单一用户意图分类步骤中，解决了现有方法在临床领域安全防护不足的问题。


<details>
  <summary>Details</summary>
Motivation: 临床聊天机器人应用中，不准确或有害的响应可能导致严重后果，现有防护方法如护栏和工具调用难以满足临床领域的精细需求。

Method: 提出TACOS分类法，通过21个细粒度类别整合安全过滤和工具选择，明确建模不同的安全阈值和外部工具依赖关系。

Result: 通过TACOS标注数据集进行广泛实验，验证了新分类法在临床智能体设置中的价值，并揭示了训练数据分布和基础模型预训练知识的有用见解。

Conclusion: TACOS分类法为临床智能体提供了更全面的安全保障，能够覆盖广泛的临床和非临床查询，是临床领域安全防护的有效解决方案。

Abstract: Safety is a paramount concern in clinical chatbot applications, where
inaccurate or harmful responses can lead to serious consequences. Existing
methods--such as guardrails and tool calling--often fall short in addressing
the nuanced demands of the clinical domain. In this paper, we introduce TACOS
(TAxonomy of COmprehensive Safety for Clinical Agents), a fine-grained,
21-class taxonomy that integrates safety filtering and tool selection into a
single user intent classification step. TACOS is a taxonomy that can cover a
wide spectrum of clinical and non-clinical queries, explicitly modeling varying
safety thresholds and external tool dependencies. To validate our framework, we
curate a TACOS-annotated dataset and perform extensive experiments. Our results
demonstrate the value of a new taxonomy specialized for clinical agent
settings, and reveal useful insights about train data distribution and
pretrained knowledge of base models.

</details>


### [53] [Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity](https://arxiv.org/abs/2509.22054)
*Ping Chen,Xiang Liu,Zhaoxiang Liu,Zezhou Chen,Xingpeng Zhang,Huan Hu,Zipeng Wang,Kai Wang,Shuming Shi,Shiguo Lian*

Main category: cs.CL

TL;DR: 提出了模糊推理链（FRC）框架，将LLM语义先验与连续模糊隶属度结合，在概率推理和模糊隶属推理之间建立显式交互，有效处理文本中的歧义、多义和不确定性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然语言处理方面取得了显著进展，但在处理具有歧义、多义或不确定性的文本时仍面临重大挑战。

Method: 引入模糊推理链（FRC）框架，集成LLM语义先验与连续模糊隶属度，实现概率推理和模糊隶属推理之间的显式交互。

Result: 在情感分析任务上的验证表明，FRC确保了稳定的推理过程，并促进了不同模型规模间的知识迁移。

Conclusion: FRC为管理细微和模糊表达提供了一种通用机制，具有更好的可解释性和鲁棒性。

Abstract: With the rapid advancement of large language models (LLMs), natural language
processing (NLP) has achieved remarkable progress. Nonetheless, significant
challenges remain in handling texts with ambiguity, polysemy, or uncertainty.
We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM
semantic priors with continuous fuzzy membership degrees, creating an explicit
interaction between probability-based reasoning and fuzzy membership reasoning.
This transition allows ambiguous inputs to be gradually transformed into clear
and interpretable decisions while capturing conflicting or uncertain signals
that traditional probability-based methods cannot. We validate FRC on sentiment
analysis tasks, where both theoretical analysis and empirical results show that
it ensures stable reasoning and facilitates knowledge transfer across different
model scales. These findings indicate that FRC provides a general mechanism for
managing subtle and ambiguous expressions with improved interpretability and
robustness.

</details>


### [54] [RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media](https://arxiv.org/abs/2509.22055)
*Yudong Li,Yufei Sun,Yuhan Yao,Peiru Yang,Wanyue Li,Jiajun Zou,Yongfeng Huang,Linlin Shen*

Main category: cs.CL

TL;DR: 提出了首个社交媒体AI生成文本的纵向数据集RedNote-Vibe和基于心理语言学特征的检测框架PLAD，用于分析AIGT的时间动态和用户互动模式。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注静态AIGT检测，而社交媒体上的内容动态由用户参与驱动并随时间演变，需要研究其时间动态和用户互动模式。

Method: 引入RedNote-Vibe数据集（5年纵向数据，来自小红书平台），并提出PLAD检测框架，利用心理语言学特征进行可解释的AIGT检测。

Result: PLAD实现了优越的检测性能，揭示了人类与AI生成内容的区分特征，并发现了这些语言特征与社交媒体参与度之间的复杂关系。

Conclusion: RedNote-Vibe数据集和PLAD框架为社交媒体AIGT分析提供了重要工具，有助于理解AIGT的时间动态和用户互动模式。

Abstract: The proliferation of Large Language Models (LLMs) has led to widespread
AI-Generated Text (AIGT) on social media platforms, creating unique challenges
where content dynamics are driven by user engagement and evolve over time.
However, existing datasets mainly depict static AIGT detection. In this work,
we introduce RedNote-Vibe, the first longitudinal (5-years) dataset for social
media AIGT analysis. This dataset is sourced from Xiaohongshu platform,
containing user engagement metrics (e.g., likes, comments) and timestamps
spanning from the pre-LLM period to July 2025, which enables research into the
temporal dynamics and user interaction patterns of AIGT. Furthermore, to detect
AIGT in the context of social media, we propose PsychoLinguistic AIGT Detection
Framework (PLAD), an interpretable approach that leverages psycholinguistic
features. Our experiments show that PLAD achieves superior detection
performance and provides insights into the signatures distinguishing human and
AI-generated content. More importantly, it reveals the complex relationship
between these linguistic features and social media engagement. The dataset is
available at https://github.com/testuser03158/RedNote-Vibe.

</details>


### [55] [The QCET Taxonomy of Standard Quality Criterion Names and Definitions for the Evaluation of NLP Systems](https://arxiv.org/abs/2509.22064)
*Anya Belz,Simon Mille,Craig Thomson*

Main category: cs.CL

TL;DR: 该论文提出了QCET质量评估分类法，旨在解决NLP领域中质量评估标准名称不一致的问题，通过创建标准化的质量准则名称和定义来确保评估结果的可比性。


<details>
  <summary>Details</summary>
Motivation: NLP领域中相同质量准则名称（如"流畅性"）在不同评估中可能衡量不同方面，这种名称隐含的可比性具有误导性，阻碍了基于多个独立评估得出可靠结论的能力，影响了整个领域的科学进展。

Method: 采用严格描述性方法，从NLP报告中三个评估调查中推导出标准化的质量准则名称和定义，并将其组织成层次结构，每个父节点捕捉其子节点的共同方面。

Result: 开发了QCET质量评估分类法及其相关资源，该分类法包含标准化的质量准则名称和定义层次结构。

Conclusion: QCET分类法有三个主要用途：(i)建立现有评估的可比性，(ii)指导新评估的设计，(iii)评估监管合规性，为NLP领域的质量评估提供了标准化框架。

Abstract: Prior work has shown that two NLP evaluation experiments that report results
for the same quality criterion name (e.g. Fluency) do not necessarily evaluate
the same aspect of quality, and the comparability implied by the name can be
misleading. Not knowing when two evaluations are comparable in this sense means
we currently lack the ability to draw reliable conclusions about system quality
on the basis of multiple, independently conducted evaluations. This in turn
hampers the ability of the field to progress scientifically as a whole, a
pervasive issue in NLP since its beginning (Sparck Jones, 1981). It is hard to
see how the issue of unclear comparability can be fully addressed other than by
the creation of a standard set of quality criterion names and definitions that
the several hundred quality criterion names actually in use in the field can be
mapped to, and grounded in. Taking a strictly descriptive approach, the QCET
Quality Criteria for Evaluation Taxonomy derives a standard set of quality
criterion names and definitions from three surveys of evaluations reported in
NLP, and structures them into a hierarchy where each parent node captures
common aspects of its child nodes. We present QCET and the resources it
consists of, and discuss its three main uses in (i) establishing comparability
of existing evaluations, (ii) guiding the design of new evaluations, and (iii)
assessing regulatory compliance.

</details>


### [56] [Fine-tuning Done Right in Model Editing](https://arxiv.org/abs/2509.22072)
*Wanli Yang,Fei Sun,Rui Tang,Hongyu Zang,Du Su,Qi Cao,Jingang Wang,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文挑战了微调在模型编辑中无效的传统观点，发现失败原因在于采用了深度优先的序列化编辑流程而非标准的广度优先微调。通过恢复epoch-based的微调流程并优化参数调整位置，提出了LocFT-BF方法，显著提升了模型编辑效果。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为微调不适合模型编辑，但作者认为这种失败源于采用了不合适的深度优先编辑流程，而不是微调本身的问题。

Method: 将微调恢复为标准广度优先（epoch-based）流程，使用mini-batch优化，并通过系统分析调优位置，开发了LocFT-BF局部编辑方法。

Result: LocFT-BF在多种LLM和数据集上大幅超越现有最优方法，首次实现了10万次编辑和720亿参数模型的持续编辑，且不损害通用能力。

Conclusion: 通过澄清误解和引入原则性的局部调优策略，将微调从被低估的基线方法提升为模型编辑的领先方法，为未来研究奠定了坚实基础。

Abstract: Fine-tuning, a foundational method for adapting large language models, has
long been considered ineffective for model editing. Here, we challenge this
belief, arguing that the reported failure arises not from the inherent
limitation of fine-tuning itself, but from adapting it to the sequential nature
of the editing task, a single-pass depth-first pipeline that optimizes each
sample to convergence before moving on. While intuitive, this depth-first
pipeline coupled with sample-wise updating over-optimizes each edit and induces
interference across edits. Our controlled experiments reveal that simply
restoring fine-tuning to the standard breadth-first (i.e., epoch-based)
pipeline with mini-batch optimization substantially improves its effectiveness
for model editing. Moreover, fine-tuning in editing also suffers from
suboptimal tuning parameter locations inherited from prior methods. Through
systematic analysis of tuning locations, we derive LocFT-BF, a simple and
effective localized editing method built on the restored fine-tuning framework.
Extensive experiments across diverse LLMs and datasets demonstrate that
LocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our
knowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x
beyond prior practice, without sacrificing general capabilities. By clarifying
a long-standing misconception and introducing a principled localized tuning
strategy, we advance fine-tuning from an underestimated baseline to a leading
method for model editing, establishing a solid foundation for future research.

</details>


### [57] [COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning](https://arxiv.org/abs/2509.22075)
*Dmitriy Shopkhoev,Denis Makhov,Magauiya Zhussip,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: CoSpaDi是一种无需训练的大语言模型压缩框架，通过结构化稀疏字典学习替代传统的低秩分解，在保持模型精度的同时实现20-50%的压缩比。


<details>
  <summary>Details</summary>
Motivation: 传统的低秩权重近似方法虽然计算高效，但结构约束过于严格，会导致明显的模型精度下降。需要更灵活的压缩策略来平衡压缩率和精度。

Method: 使用结构化稀疏因子分解，将权重矩阵表示为稠密字典和列稀疏系数矩阵。利用小规模校准数据集优化分解，最小化功能重构误差而非权重近似误差。

Result: 在多个Llama和Qwen模型上测试，在20-50%压缩比下，CoSpaDi在准确性和困惑度方面均优于最先进的数据感知低秩方法。

Conclusion: 结构化稀疏字典学习是传统低秩方法的有力替代方案，为高效LLM部署提供了新的压缩范式。

Abstract: Post-training compression of large language models (LLMs) largely relies on
low-rank weight approximation, which represents each column of a weight matrix
in a shared low-dimensional subspace. While this is a computationally efficient
strategy, the imposed structural constraint is rigid and can lead to a
noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression
via Sparse Dictionary Learning), a novel training-free compression framework
that replaces low-rank decomposition with a more flexible structured sparse
factorization in which each weight matrix is represented with a dense
dictionary and a column-sparse coefficient matrix. This formulation enables a
union-of-subspaces representation: different columns of the original weight
matrix are approximated in distinct subspaces spanned by adaptively selected
dictionary atoms, offering greater expressiveness than a single invariant
basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the
factorization such that the output activations of compressed projection layers
closely match those of the original ones, thereby minimizing functional
reconstruction error rather than mere weight approximation. This data-aware
strategy preserves better model fidelity without any fine-tuning under
reasonable compression ratios. Moreover, the resulting structured sparsity
allows efficient sparse-dense matrix multiplication and is compatible with
post-training quantization for further memory and latency gains. We evaluate
CoSpaDi across multiple Llama and Qwen models under per-layer and per-group
settings at 20-50\% compression ratios, demonstrating consistent superiority
over state-of-the-art data-aware low-rank methods both in accuracy and
perplexity. Our results establish structured sparse dictionary learning as a
powerful alternative to conventional low-rank approaches for efficient LLM
deployment.

</details>


### [58] [Multilingual Dialogue Generation and Localization with Dialogue Act Scripting](https://arxiv.org/abs/2509.22086)
*Justin Vasselli,Eunike Andriani Kardinata,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 提出了对话行为脚本（DAS）框架，通过结构化意图表示来生成多语言对话，避免直接翻译带来的不自然和文化不恰当问题。


<details>
  <summary>Details</summary>
Motivation: 非英语对话数据集稀缺，现有方法通常基于英语对话翻译，这会引入人工痕迹，降低自然度和文化适宜性。

Method: 使用结构化对话行为表示来编码、本地化和生成多语言对话，从抽象意图表示直接生成目标语言对话。

Result: 在意大利语、德语和中文上的人类评估显示，DAS生成的对话在文化相关性、连贯性和情境适宜性方面优于机器和人工翻译。

Conclusion: DAS框架能有效生成更自然、文化适宜的多语言对话，缓解翻译腔问题。

Abstract: Non-English dialogue datasets are scarce, and models are often trained or
evaluated on translations of English-language dialogues, an approach which can
introduce artifacts that reduce their naturalness and cultural appropriateness.
This work proposes Dialogue Act Script (DAS), a structured framework for
encoding, localizing, and generating multilingual dialogues from abstract
intent representations. Rather than translating dialogue utterances directly,
DAS enables the generation of new dialogues in the target language that are
culturally and contextually appropriate. By using structured dialogue act
representations, DAS supports flexible localization across languages,
mitigating translationese and enabling more fluent, naturalistic conversations.
Human evaluations across Italian, German, and Chinese show that DAS-generated
dialogues consistently outperform those produced by both machine and human
translators on measures of cultural relevance, coherence, and situational
appropriateness.

</details>


### [59] [S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models](https://arxiv.org/abs/2509.22099)
*Shaoning Sun,Jiachen Yu,Zongqi Wang,Xuewei Yang,Tianle Gu,Yujiu Yang*

Main category: cs.CL

TL;DR: 本文提出了Solve-to-Judge (S2J)方法来解决生成奖励模型中存在的solve-to-judge差距问题，即模型能够解决问题但无法正确判断的情况。S2J通过同时利用模型的求解和判断能力进行监督，在模型优化过程中显式连接问题解决和评估能力。


<details>
  <summary>Details</summary>
Motivation: 研究发现大型语言模型在生成奖励模型中存在显著的solve-to-judge差距，即模型能够解决某些问题（14%-37%），但却无法对这些问题的答案做出正确判断。这种能力不匹配限制了生成奖励模型的性能。

Method: 提出S2J方法，在单个生成奖励模型的输出上同时利用其求解和判断能力进行监督，在模型优化过程中显式连接问题解决和评估能力。该方法通过自我进化实现，不依赖更强大的外部模型进行蒸馏。

Result: S2J方法有效减少了16.2%的solve-to-judge差距，将模型的判断性能提升了5.8%。在使用相同基础模型的情况下，S2J实现了最先进的性能，同时使用了显著更小的训练数据集。

Conclusion: S2J方法通过显式连接生成奖励模型的求解和判断能力，有效解决了solve-to-judge差距问题，提升了模型性能，且不依赖外部模型蒸馏，具有更好的实用性和效率。

Abstract: With the rapid development of large language models (LLMs), generative reward
models (GRMs) have been widely adopted for reward modeling and evaluation.
Previous studies have primarily focused on training specialized GRMs by
optimizing them on preference datasets with the judgment correctness as
supervision. While it's widely accepted that GRMs with stronger problem-solving
capabilities typically exhibit superior judgment abilities, we first identify a
significant solve-to-judge gap when examining individual queries. Specifically,
the solve-to-judge gap refers to the phenomenon where GRMs struggle to make
correct judgments on some queries (14%-37%), despite being fully capable of
solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to
address this problem. Specifically, S2J simultaneously leverages both the
solving and judging capabilities on a single GRM's output for supervision,
explicitly linking the GRM's problem-solving and evaluation abilities during
model optimization, thereby narrowing the gap. Our comprehensive experiments
demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%,
thereby enhancing the model's judgment performance by 5.8%. Notably, S2J
achieves state-of-the-art (SOTA) performance among GRMs built on the same base
model while utilizing a significantly smaller training dataset. Moreover, S2J
accomplishes this through self-evolution without relying on more powerful
external models for distillation.

</details>


### [60] [Think Right, Not More: Test-Time Scaling for Numerical Claim Verification](https://arxiv.org/abs/2509.22101)
*Primakov Chungkham,V Venktesh,Vinay Setty,Avishek Anand*

Main category: cs.CL

TL;DR: 本文提出VERIFIERFC验证器模型，通过扩展测试时计算来改进LLMs对复杂数值声明的事实核查，解决了推理漂移问题，并引入自适应机制提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在处理需要组合性和数值推理的真实世界数值声明事实核查时存在不足，无法理解数值细微差别，且容易发生推理漂移问题。

Method: 训练验证器模型(VERIFIERFC)，从LLM中引出多个推理路径，并选择可能得出正确结论的路径；引入自适应机制根据声明复杂性选择性执行测试时计算。

Result: 测试时计算有助于缓解推理漂移问题，在数值声明事实核查上带来显著性能提升；自适应方法比标准TTS效率提高1.8倍，性能比单次声明验证方法提升18.8%。

Conclusion: 扩展测试时计算结合验证器模型能有效改进LLMs对复杂数值声明的事实核查能力，自适应机制在保持性能的同时提高了计算效率。

Abstract: Fact-checking real-world claims, particularly numerical claims, is inherently
complex that require multistep reasoning and numerical reasoning for verifying
diverse aspects of the claim. Although large language models (LLMs) including
reasoning models have made tremendous advances, they still fall short on
fact-checking real-world claims that require a combination of compositional and
numerical reasoning. They are unable to understand nuance of numerical aspects,
and are also susceptible to the reasoning drift issue, where the model is
unable to contextualize diverse information resulting in misinterpretation and
backtracking of reasoning process. In this work, we systematically explore
scaling test-time compute (TTS) for LLMs on the task of fact-checking complex
numerical claims, which entails eliciting multiple reasoning paths from an LLM.
We train a verifier model (VERIFIERFC) to navigate this space of possible
reasoning paths and select one that could lead to the correct verdict. We
observe that TTS helps mitigate the reasoning drift issue, leading to
significant performance gains for fact-checking numerical claims. To improve
compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS
selectively based on the perceived complexity of the claim. This approach
achieves 1.8x higher efficiency than standard TTS, while delivering a notable
18.8% performance improvement over single-shot claim verification methods. Our
code and data can be found at https://github.com/VenkteshV/VerifierFC

</details>


### [61] [Universal Legal Article Prediction via Tight Collaboration between Supervised Classification Model and LLM](https://arxiv.org/abs/2509.22119)
*Xiao Chi,Wenlin Zhong,Yiquan Wu,Wei Wang,Kun Kuang,Fei Wu,Minghui Xiong*

Main category: cs.CL

TL;DR: 提出了Uni-LAP框架，通过整合监督分类模型和大型语言模型的优势来解决法律条文预测任务中的挑战，在多个司法管辖区的数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有法律条文预测方法面临显著挑战：监督分类模型难以捕捉复杂事实模式，大型语言模型在预测性场景中表现不佳，且大多数方法缺乏跨司法管辖区的通用性。

Method: 提出Uni-LAP通用框架，通过紧密协作整合SCM和LLM的优势：SCM使用新颖的Top-K损失函数生成候选条文，LLM采用三段论启发式推理来优化最终预测。

Result: 在多个司法管辖区数据集上的实证结果表明，Uni-LAP方法持续优于现有基线方法，展示了其有效性和泛化能力。

Conclusion: Uni-LAP框架成功解决了法律条文预测中的关键挑战，通过整合不同模型的优势实现了更好的性能和跨司法管辖区的通用性。

Abstract: Legal Article Prediction (LAP) is a critical task in legal text
classification, leveraging natural language processing (NLP) techniques to
automatically predict relevant legal articles based on the fact descriptions of
cases. As a foundational step in legal decision-making, LAP plays a pivotal
role in determining subsequent judgments, such as charges and penalties.
Despite its importance, existing methods face significant challenges in
addressing the complexities of LAP. Supervised classification models (SCMs),
such as CNN and BERT, struggle to fully capture intricate fact patterns due to
their inherent limitations. Conversely, large language models (LLMs), while
excelling in generative tasks, perform suboptimally in predictive scenarios due
to the abstract and ID-based nature of legal articles. Furthermore, the
diversity of legal systems across jurisdictions exacerbates the issue, as most
approaches are tailored to specific countries and lack broader applicability.
To address these limitations, we propose Uni-LAP, a universal framework for
legal article prediction that integrates the strengths of SCMs and LLMs through
tight collaboration. Specifically, in Uni-LAP, the SCM is enhanced with a novel
Top-K loss function to generate accurate candidate articles, while the LLM
employs syllogism-inspired reasoning to refine the final predictions. We
evaluated Uni-LAP on datasets from multiple jurisdictions, and empirical
results demonstrate that our approach consistently outperforms existing
baselines, showcasing its effectiveness and generalizability.

</details>


### [62] [Multilingual Vision-Language Models, A Survey](https://arxiv.org/abs/2509.22123)
*Andrei-Alexandru Manea,Jindřich Libovický*

Main category: cs.CL

TL;DR: 本文调查了多语言视觉语言模型，分析了31个模型和21个基准测试，揭示了语言中立性与文化意识之间的张力。


<details>
  <summary>Details</summary>
Motivation: 研究多语言视觉语言模型在处理跨语言文本和图像时的表现，探索语言中立性与文化意识之间的平衡问题。

Method: 通过系统性地回顾31个模型和21个基准测试，分析编码器架构和生成式架构，比较不同训练方法和评估策略。

Result: 发现当前训练方法通过对比学习倾向于语言中立性，而文化意识依赖于多样化数据；三分之二的评估基准使用基于翻译的方法强调语义一致性。

Conclusion: 存在跨语言能力的不一致性，以及训练目标与评估目标之间的差距，需要更平衡地处理语言中立性和文化适应性。

Abstract: This survey examines multilingual vision-language models that process text
and images across languages. We review 31 models and 21 benchmarks, spanning
encoder-only and generative architectures, and identify a key tension between
language neutrality (consistent cross-lingual representations) and cultural
awareness (adaptation to cultural contexts). Current training methods favor
neutrality through contrastive learning, while cultural awareness depends on
diverse data. Two-thirds of evaluation benchmarks use translation-based
approaches prioritizing semantic consistency, though recent work incorporates
culturally grounded content. We find discrepancies in cross-lingual
capabilities and gaps between training objectives and evaluation goals.

</details>


### [63] [FoodSEM: Large Language Model Specialized in Food Named-Entity Linking](https://arxiv.org/abs/2509.22125)
*Ana Gjorgjevikj,Matej Martinc,Gjorgjina Cenikj,Sašo Džeroski,Barbara Koroušić Seljak,Tome Eftimov*

Main category: cs.CL

TL;DR: FoodSEM是一个专门针对食品领域命名实体链接任务进行微调的开源大语言模型，在多个食品相关本体上实现了最先进的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有的通用大语言模型和定制领域特定模型/系统无法准确解决食品命名实体链接任务，需要专门针对食品领域进行优化。

Method: 通过指令-响应场景，FoodSEM将文本中提到的食品相关实体链接到多个本体（FoodOn、SNOMED-CT、Hansard分类法），并采用微调方法。

Result: FoodSEM在多个本体和数据集上实现了最先进的性能，某些本体上的F1分数甚至达到98%，显著优于零样本、单样本和少样本提示的基线模型。

Conclusion: 该研究的主要贡献包括发布适合LLM微调/评估的食品标注语料库、发布用于推进食品领域语义理解的鲁棒模型，以及为未来基准测试提供食品NEL的强基线。

Abstract: This paper introduces FoodSEM, a state-of-the-art fine-tuned open-source
large language model (LLM) for named-entity linking (NEL) to food-related
ontologies. To the best of our knowledge, food NEL is a task that cannot be
accurately solved by state-of-the-art general-purpose (large) language models
or custom domain-specific models/systems. Through an instruction-response (IR)
scenario, FoodSEM links food-related entities mentioned in a text to several
ontologies, including FoodOn, SNOMED-CT, and the Hansard taxonomy. The FoodSEM
model achieves state-of-the-art performance compared to related models/systems,
with F1 scores even reaching 98% on some ontologies and datasets. The presented
comparative analyses against zero-shot, one-shot, and few-shot LLM prompting
baselines further highlight FoodSEM's superior performance over its
non-fine-tuned version. By making FoodSEM and its related resources publicly
available, the main contributions of this article include (1) publishing a
food-annotated corpora into an IR format suitable for LLM
fine-tuning/evaluation, (2) publishing a robust model to advance the semantic
understanding of text in the food domain, and (3) providing a strong baseline
on food NEL for future benchmarking.

</details>


### [64] [R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2509.22131)
*Hongyu Shan,Mingyang Song,Chang Dai,Di Liang,Han Chen*

Main category: cs.CL

TL;DR: 提出Reasoning Capsule框架，通过将推理计划压缩为少量潜在标记，在保持显式推理透明度的同时提高效率，平衡了效率、准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-Thought提示在复杂推理中导致的延迟、内存使用增加以及早期错误传播问题，寻求结合潜在推理效率和显式推理透明度的解决方案。

Method: 基于信息瓶颈原理，将高层推理计划压缩为少量学习到的潜在标记（推理胶囊），同时保持执行步骤轻量级或显式，通过双目标损失函数确保胶囊的最小充分性。

Result: 在复杂基准测试中，减少了推理的可见标记足迹，同时保持或提高了准确性。

Conclusion: Reasoning Capsule框架在效率、准确性和可解释性之间取得了良好平衡，为复杂推理任务提供了更优的解决方案。

Abstract: Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle
complex reasoning by eliciting explicit step-by-step rationales. However, CoT's
verbosity increases latency and memory usage and may propagate early errors
across long chains. We propose the Reasoning Capsule (R-Capsule), a framework
that aims to combine the efficiency of latent reasoning with the transparency
of explicit CoT. The core idea is to compress the high-level plan into a small
set of learned latent tokens (a Reasoning Capsule) while keeping execution
steps lightweight or explicit. This hybrid approach is inspired by the
Information Bottleneck (IB) principle, where we encourage the capsule to be
approximately minimal yet sufficient for the task. Minimality is encouraged via
a low-capacity bottleneck, which helps improve efficiency. Sufficiency is
encouraged via a dual objective: a primary task loss for answer accuracy and an
auxiliary plan-reconstruction loss that encourages the capsule to faithfully
represent the original textual plan. The reconstruction objective helps ground
the latent space, thereby improving interpretability and reducing the use of
uninformative shortcuts. Our framework strikes a balance between efficiency,
accuracy, and interpretability, thereby reducing the visible token footprint of
reasoning while maintaining or improving accuracy on complex benchmarks. Our
codes are available at:
https://anonymous.4open.science/r/Reasoning-Capsule-7BE0

</details>


### [65] [Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding](https://arxiv.org/abs/2509.22134)
*Shijing Hu,Jingyang Li,Zhihui Lu,Pan Zhou*

Main category: cs.CL

TL;DR: GTO通过对齐训练与解码时的树策略来解决推测解码中的策略不对齐问题，使用采样自由的草稿树奖励和基于组的草稿策略训练，显著提高了接受长度和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有训练目标只优化单条贪婪草稿路径，而解码时采用树策略验证多个分支，这种草稿策略不对齐限制了加速效果。

Method: 提出组树优化(GTO)，包含草稿树奖励（衡量草稿树在目标模型下的期望接受长度）和基于组的草稿策略训练（通过对比当前和参考草稿模型的树形成去偏组标准化优势，应用PPO风格代理进行稳健更新）。

Result: 在对话、代码和数学任务上，GTO将接受长度提高了7.4%，相比之前最优方法EAGLE-3额外带来7.7%的加速。

Conclusion: 通过弥合草稿策略不对齐，GTO为高效LLM推理提供了实用、通用的解决方案。

Abstract: Speculative decoding accelerates large language model (LLM) inference by
letting a lightweight draft model propose multiple tokens that the target model
verifies in parallel. Yet existing training objectives optimize only a single
greedy draft path, while decoding follows a tree policy that re-ranks and
verifies multiple branches. This draft policy misalignment limits achievable
speedups. We introduce Group Tree Optimization (GTO), which aligns training
with the decoding-time tree policy through two components: (i) Draft Tree
Reward, a sampling-free objective equal to the expected acceptance length of
the draft tree under the target model, directly measuring decoding performance;
(ii) Group-based Draft Policy Training, a stable optimization scheme that
contrasts trees from the current and a frozen reference draft model, forming
debiased group-standardized advantages and applying a PPO-style surrogate along
the longest accepted sequence for robust updates. We further prove that
increasing our Draft Tree Reward provably improves acceptance length and
speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and
multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B,
DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4% and
yields an additional 7.7% speedup over prior state-of-the-art EAGLE-3. By
bridging draft policy misalignment, GTO offers a practical, general solution
for efficient LLM inference.

</details>


### [66] [NFDI4DS Shared Tasks for Scholarly Document Processing](https://arxiv.org/abs/2509.22141)
*Raia Abu Ahmad,Rana Abdulla,Tilahun Abedissa Taffa,Soeren Auer,Hamed Babaei Giglou,Ekaterina Borisova,Zongxiong Chen,Stefan Dietze,Jennifer DSouza,Mayra Elwes,Genet-Asefa Gesese,Shufan Jiang,Ekaterina Kutafina,Philipp Mayr,Georg Rehm,Sameer Sadruddin,Sonja Schimmler,Daniel Schneider,Kanishka Silva,Sharmila Upadhyaya,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 本文介绍了德国国家研究数据基础设施（NFDI4DS）联盟开发和主办的12个共享任务，这些任务旨在通过社区标准化评估推进学术文档处理领域的研究，促进FAIR原则和可重复研究实践。


<details>
  <summary>Details</summary>
Motivation: 共享任务作为通过社区标准化评估推进研究的强大工具，在促进可查找、可访问、可互操作和可重用（FAIR）以及透明和可重复研究实践方面发挥着关键作用。

Method: 在NFDI4DS联盟下开发和主办12个共享任务，涵盖学术文档处理中的多样化挑战，并在领先的学术场所举办，促进方法创新并为更广泛的研究社区贡献开放获取数据集、模型和工具。

Result: 这些任务成功促进了方法创新，并为研究社区贡献了开放获取的数据集、模型和工具，这些资源已整合到联盟的研究数据基础设施中。

Conclusion: 共享任务是推进研究的重要工具，通过NFDI4DS联盟的12个共享任务，有效促进了学术文档处理领域的研究进展和FAIR原则的实施。

Abstract: Shared tasks are powerful tools for advancing research through
community-based standardised evaluation. As such, they play a key role in
promoting findable, accessible, interoperable, and reusable (FAIR), as well as
transparent and reproducible research practices. This paper presents an updated
overview of twelve shared tasks developed and hosted under the German National
Research Data Infrastructure for Data Science and Artificial Intelligence
(NFDI4DS) consortium, covering a diverse set of challenges in scholarly
document processing. Hosted at leading venues, the tasks foster methodological
innovations and contribute open-access datasets, models, and tools for the
broader research community, which are integrated into the consortium's research
data infrastructure.

</details>


### [67] [From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement](https://arxiv.org/abs/2509.22144)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Zike Yuan,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: MACC框架通过多轮自适应压缩CoT推理过程，利用token弹性现象，在保持准确性的同时显著减少推理延迟和token长度。


<details>
  <summary>Details</summary>
Motivation: CoT推理虽然能提升复杂任务性能，但因其冗长性导致推理延迟显著增加，需要一种有效的压缩方法。

Method: 提出多轮自适应CoT压缩框架，利用token弹性现象，通过多轮精炼逐步压缩CoT，为每个输入确定最优压缩深度。

Result: 相比最先进基线方法，平均准确率提升5.6%，CoT长度平均减少47个token，显著降低延迟。测试时性能可通过可解释特征可靠预测。

Conclusion: CoT压缩既有效又可预测，支持高效模型选择和预测，无需重复微调。

Abstract: Chain-of-Thought (CoT) reasoning improves performance on complex tasks but
introduces significant inference latency due to verbosity. We propose
Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that
leverages the token elasticity phenomenon--where overly small token budgets can
paradoxically increase output length--to progressively compress CoTs via
multiround refinement. This adaptive strategy allows MACC to determine the
optimal compression depth for each input. Our method achieves an average
accuracy improvement of 5.6 percent over state-of-the-art baselines, while also
reducing CoT length by an average of 47 tokens and significantly lowering
latency. Furthermore, we show that test-time performance--accuracy and token
length--can be reliably predicted using interpretable features like perplexity
and compression rate on the training set. Evaluated across different models,
our method enables efficient model selection and forecasting without repeated
fine-tuning, demonstrating that CoT compression is both effective and
predictable. Our code will be released in https://github.com/Leon221220/MACC.

</details>


### [68] [Mixture of Detectors: A Compact View of Machine-Generated Text Detection](https://arxiv.org/abs/2509.22147)
*Sai Teja Lekkala,Yadagiri Annepaka,Arun Kumar Challa,Samatha Reddy Machireddy,Partha Pakray,Chukhu Chunka*

Main category: cs.CL

TL;DR: 该论文研究机器生成文本检测，提出了BMAS English数据集，支持文档级二元/多元分类、句子级分割和对抗攻击检测。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，需要解决人类作品真实性和创造力保护的问题，以及机器生成文本的检测挑战。

Method: 引入BMAS English数据集，包含二元分类（人机文本区分）、多元分类（生成器识别）、句子级分割（人机协作文本边界检测）和对抗攻击检测。

Result: 提出了一个全面的机器生成文本检测框架，能够处理多种检测场景。

Conclusion: 该工作以更有意义的方式解决了机器生成文本检测问题，为相关研究提供了新的数据集和方法。

Abstract: Large Language Models (LLMs) are gearing up to surpass human creativity. The
veracity of the statement needs careful consideration. In recent developments,
critical questions arise regarding the authenticity of human work and the
preservation of their creativity and innovative abilities. This paper
investigates such issues. This paper addresses machine-generated text detection
across several scenarios, including document-level binary and multiclass
classification or generator attribution, sentence-level segmentation to
differentiate between human-AI collaborative text, and adversarial attacks
aimed at reducing the detectability of machine-generated text. We introduce a
new work called BMAS English: an English language dataset for binary
classification of human and machine text, for multiclass classification, which
not only identifies machine-generated text but can also try to determine its
generator, and Adversarial attack addressing where it is a common act for the
mitigation of detection, and Sentence-level segmentation, for predicting the
boundaries between human and machine-generated text. We believe that this paper
will address previous work in Machine-Generated Text Detection (MGTD) in a more
meaningful way.

</details>


### [69] [Context Parametrization with Compositional Adapters](https://arxiv.org/abs/2509.22158)
*Josip Jukić,Martin Tutek,Jan Šnajder*

Main category: cs.CL

TL;DR: CompAs是一个元学习框架，通过将上下文转换为具有组合结构的适配器参数，实现指令、演示或检索段落的代数合并，从而降低推理成本、提高长上下文稳定性，并解决输入超出模型上下文窗口的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型适应方法（上下文学习和监督微调）存在关键限制：上下文学习在处理大量演示时效率低下，监督微调会产生训练开销且牺牲灵活性。需要一种能够整合多个信息块的替代方案。

Method: 提出CompAs框架，将上下文转换为具有组合结构的适配器参数。生成的适配器可以代数合并，无需重新处理长提示即可无缝组合指令、演示或检索段落。该方法还支持通过解码器恢复输入上下文。

Result: 在多样化的多项选择和抽取式问答任务上的实证结果显示，CompAs在扩展到更多输入时，优于上下文学习和先前的基于生成器的方法。

Conclusion: CompAs建立了可组合适配器生成作为扩展LLM部署的实用高效替代方案，具有较低推理成本、对长上下文不稳定的鲁棒性，以及输入超出模型上下文窗口时的原则性解决方案。

Abstract: Large language models (LLMs) often seamlessly adapt to new tasks through
in-context learning (ICL) or supervised fine-tuning (SFT). However, both of
these approaches face key limitations: ICL is inefficient when handling many
demonstrations, and SFT incurs training overhead while sacrificing flexibility.
Mapping instructions or demonstrations from context directly into adapter
parameters offers an appealing alternative. While prior work explored
generating adapters based on a single input context, it has overlooked the need
to integrate multiple chunks of information. To address this gap, we introduce
CompAs, a meta-learning framework that translates context into adapter
parameters with a compositional structure. Adapters generated this way can be
merged algebraically, enabling instructions, demonstrations, or retrieved
passages to be seamlessly combined without reprocessing long prompts.
Critically, this approach yields three benefits: lower inference cost,
robustness to long-context instability, and establishes a principled solution
when input exceeds the model's context window. Furthermore, CompAs encodes
information into adapter parameters in a reversible manner, enabling recovery
of input context through a decoder, facilitating safety and security. Empirical
results on diverse multiple-choice and extractive question answering tasks show
that CompAs outperforms ICL and prior generator-based methods, especially when
scaling to more inputs. Our work establishes composable adapter generation as a
practical and efficient alternative for scaling LLM deployment.

</details>


### [70] [When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance](https://arxiv.org/abs/2509.22193)
*Nicolas Boizard,Hippolyte Gisserot-Boukhlef,Kevin El-Haddad,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: 该研究通过合成数据蒸馏框架，比较了不同规模的指令微调模型和推理模型在数学任务和通用任务上的表现，发现推理模型在性能上能与更大规模的IFT模型相媲美甚至超越，且随着模型规模增大，推理模型在推理密集型任务上的价值更加显著。


<details>
  <summary>Details</summary>
Motivation: 尽管具有推理能力的大语言模型在多种任务上取得了最佳性能，但推理在哪些任务和模型规模下变得有效，以及其训练和推理成本仍未被充分探索。

Method: 使用合成数据蒸馏框架进行大规模监督研究，比较不同规模的指令微调模型和推理模型在数学任务和通用任务上的表现，评估多项选择和开放式两种格式。

Result: 推理持续提升模型性能，通常能与显著更大的IFT系统相匹敌甚至超越。IFT在训练和推理成本方面保持帕累托最优，但推理模型随着模型规模增大而价值增加，在推理密集型和开放式任务上克服了IFT的性能限制。

Conclusion: 推理模型在性能上具有显著优势，特别是在大规模模型和推理密集型任务中，虽然IFT在成本效率上更优，但推理模型在复杂任务上的表现突破了对IFT的性能限制。

Abstract: Large Language Models (LLMs) with reasoning capabilities have achieved
state-of-the-art performance on a wide range of tasks. Despite its empirical
success, the tasks and model scales at which reasoning becomes effective, as
well as its training and inference costs, remain underexplored. In this work,
we rely on a synthetic data distillation framework to conduct a large-scale
supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models
of varying sizes, on a wide range of math-centric and general-purpose tasks,
evaluating both multiple-choice and open-ended formats. Our analysis reveals
that reasoning consistently improves model performance, often matching or
surpassing significantly larger IFT systems. Notably, while IFT remains
Pareto-optimal in training and inference costs, reasoning models become
increasingly valuable as model size scales, overcoming IFT performance limits
on reasoning-intensive and open-ended tasks.

</details>


### [71] [The Outputs of Large Language Models are Meaningless](https://arxiv.org/abs/2509.22206)
*Anandi Hattiangadi,Anders J. Schoubye*

Main category: cs.CL

TL;DR: 论文论证大型语言模型的输出无意义，基于两个前提：(a)需要特定意图才能赋予输出字面意义，(b)LLM不可能具备这种意图。作者反驳了各种反对观点，并解释了为何LLM输出看似有意义且能传递知识。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型输出的语义地位问题，分析LLM是否真正产生有意义的语言内容。

Method: 基于哲学论证方法，提出两个关键前提并为其辩护，反驳语义外在主义和内在主义的反对观点。

Result: 论证支持LLM输出无字面意义的结论，但承认其表面意义和实用价值。

Conclusion: 虽然LLM输出在严格意义上无意义，但在实践中仍能产生看似有意义的输出并传递知识。

Abstract: In this paper, we offer a simple argument for the conclusion that the outputs
of large language models (LLMs) are meaningless. Our argument is based on two
key premises: (a) that certain kinds of intentions are needed in order for
LLMs' outputs to have literal meanings, and (b) that LLMs cannot plausibly have
the right kinds of intentions. We defend this argument from various types of
responses, for example, the semantic externalist argument that deference can be
assumed to take the place of intentions and the semantic internalist argument
that meanings can be defined purely in terms of intrinsic relations between
concepts, such as conceptual roles. We conclude the paper by discussing why,
even if our argument is sound, the outputs of LLMs nevertheless seem meaningful
and can be used to acquire true beliefs and even knowledge.

</details>


### [72] [Question-Driven Analysis and Synthesis: Building Interpretable Thematic Trees with LLMs for Text Clustering and Controllable Generation](https://arxiv.org/abs/2509.22211)
*Tiago Fernandes Tavares*

Main category: cs.CL

TL;DR: RTP是一个利用LLM构建二进制树的无监督文本分析框架，通过自然语言问题对数据进行语义划分，生成可解释的分类法，比传统关键词主题模型更易理解。


<details>
  <summary>Details</summary>
Motivation: 解决传统主题模型在数据稀缺领域表现不佳、关键词列表需要大量人工解释且缺乏语义连贯性的问题。

Method: 使用大型语言模型交互式构建二进制树，每个节点都是自然语言问题，对数据进行语义划分。

Result: RTP的问题驱动层次结构比BERTopic等基线模型的关键词主题更具可解释性，且在数据主题与任务标签相关时，这些簇在下游分类任务中表现优异。

Conclusion: RTP引入了数据探索的新范式，从统计模式发现转向知识驱动的主题分析，其主题路径可作为生成模型的结构化可控提示，实现源语料库特征的持续模仿。

Abstract: Unsupervised analysis of text corpora is challenging, especially in
data-scarce domains where traditional topic models struggle. While these models
offer a solution, they typically describe clusters with lists of keywords that
require significant manual effort to interpret and often lack semantic
coherence. To address this critical interpretability gap, we introduce
Recursive Thematic Partitioning (RTP), a novel framework that leverages Large
Language Models (LLMs) to interactively build a binary tree. Each node in the
tree is a natural language question that semantically partitions the data,
resulting in a fully interpretable taxonomy where the logic of each cluster is
explicit. Our experiments demonstrate that RTP's question-driven hierarchy is
more interpretable than the keyword-based topics from a strong baseline like
BERTopic. Furthermore, we establish the quantitative utility of these clusters
by showing they serve as powerful features in downstream classification tasks,
particularly when the data's underlying themes correlate with the task labels.
RTP introduces a new paradigm for data exploration, shifting the focus from
statistical pattern discovery to knowledge-driven thematic analysis.
Furthermore, we demonstrate that the thematic paths from the RTP tree can serve
as structured, controllable prompts for generative models. This transforms our
analytical framework into a powerful tool for synthesis, enabling the
consistent imitation of specific characteristics discovered in the source
corpus.

</details>


### [73] [StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs](https://arxiv.org/abs/2509.22220)
*Yuhan Song,Linhao Zhang,Chuhan Wu,Aiwei Liu,Wei Jia,Houfeng Wang,Xiao Zhou*

Main category: cs.CL

TL;DR: StableToken是一种稳定的语音分词器，通过多分支架构和位级投票机制解决现有语义语音分词器对声学扰动的脆弱性问题，显著提升分词稳定性并在下游任务中改善SpeechLLMs的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有语义语音分词器在捕获语言内容时对声学扰动异常脆弱，即使在高信噪比下语音完全可理解时，输出分词序列也会剧烈变化，增加了下游LLMs的学习负担。

Method: 提出StableToken分词器，采用多分支并行处理音频的架构，通过强大的位级投票机制合并这些表示，形成单一稳定的分词序列。

Result: StableToken在分词稳定性方面达到新的最先进水平，在各种噪声条件下显著降低单元编辑距离（UED），这种基础稳定性直接转化为下游效益。

Conclusion: StableToken显著提升了SpeechLLMs在各种任务上的鲁棒性，为解决语音分词器脆弱性问题提供了有效方案。

Abstract: Prevalent semantic speech tokenizers, designed to capture linguistic content,
are surprisingly fragile. We find they are not robust to meaning-irrelevant
acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech
is perfectly intelligible, their output token sequences can change drastically,
increasing the learning burden for downstream LLMs. This instability stems from
two flaws: a brittle single-path quantization architecture and a distant
training signal indifferent to intermediate token stability. To address this,
we introduce StableToken, a tokenizer that achieves stability through a
consensus-driven mechanism. Its multi-branch architecture processes audio in
parallel, and these representations are merged via a powerful bit-wise voting
mechanism to form a single, stable token sequence. StableToken sets a new
state-of-the-art in token stability, drastically reducing Unit Edit Distance
(UED) under diverse noise conditions. This foundational stability translates
directly to downstream benefits, significantly improving the robustness of
SpeechLLMs on a variety of tasks.

</details>


### [74] [Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data](https://arxiv.org/abs/2509.22224)
*Zishan Ahmad,Saisubramaniam Gopalakrishnan*

Main category: cs.CL

TL;DR: 提出复合推理(CR)方法，让大语言模型动态组合多种推理风格，在科学和医学问答任务中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型依赖单一推理范式，难以处理需要多种认知策略的复杂问题

Method: 复合推理(CR)方法，动态探索和组合演绎、归纳、溯因等多种推理风格

Result: 在科学和医学问答基准测试中超越CoT和DeepSeek-R1等方法，具有更好的样本效率和适当的token使用

Conclusion: 通过培养内部推理风格的多样性，大语言模型可以获得更强大、自适应和高效的问题解决能力

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, rely on
singular, pre-dominant reasoning paradigms, hindering their performance on
intricate problems that demand diverse cognitive strategies. To address this,
we introduce Composite Reasoning (CR), a novel reasoning approach empowering
LLMs to dynamically explore and combine multiple reasoning styles like
deductive, inductive, and abductive for more nuanced problem-solving. Evaluated
on scientific and medical question-answering benchmarks, our approach
outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses
the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while
demonstrating superior sample efficiency and adequate token usage. Notably, CR
adaptively emphasizes domain-appropriate reasoning styles. It prioritizes
abductive and deductive reasoning for medical question answering, but shifts to
causal, deductive, and inductive methods for scientific reasoning. Our findings
highlight that by cultivating internal reasoning style diversity, LLMs acquire
more robust, adaptive, and efficient problem-solving abilities.

</details>


### [75] [In Their Own Words: Reasoning Traces Tailored for Small Models Make Them Better Reasoners](https://arxiv.org/abs/2509.22230)
*Jaehoon Kim,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 提出反向推测解码(RSD)方法解决大模型向小模型迁移推理能力时的分布不对齐问题，通过让学生模型基于自身概率分布过滤教师模型生成的低概率token，显著提升推理能力迁移效果。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调方法在将大语言模型的推理能力迁移到小模型时往往失败，因为教师模型的推理轨迹中包含学生模型分布中低概率的token，超出了小模型的内部表示能力。

Method: 提出反向推测解码(RSD)机制：教师模型生成候选token，但由学生模型基于自身概率分布决定是否接受，过滤掉低概率token，生成学生友好的推理轨迹。

Result: 在Qwen3-0.6B上的实验表明，直接蒸馏推理轨迹数据使平均性能下降20.5%，而使用RSD生成的推理轨迹训练则提升4.9%。RSD轨迹具有模型特异性，需要针对每个学生架构定制分布对齐。

Conclusion: 低概率token是推理能力迁移的关键瓶颈，RSD通过分布对齐有效解决了这一问题，但需要针对特定学生模型架构进行定制化处理。

Abstract: Transferring reasoning capabilities from larger language models to smaller
ones through supervised fine-tuning often fails counterintuitively, with
performance degrading despite access to high-quality teacher demonstrations. We
identify that this failure stems from distributional misalignment: reasoning
traces from larger models contain tokens that are low probability under the
student's distribution, exceeding the internal representation capacity of
smaller architectures and creating learning barriers rather than helpful
guidance. We propose Reverse Speculative Decoding (RSD), a mechanism for
generating student-friendly reasoning traces in which the teacher model
proposes candidate tokens but the student model determines acceptance based on
its own probability distributions, filtering low probability tokens. When
applied to Qwen3-0.6B, direct distillation of s1K-1.1 reasoning trace data
degrades average performance across major reasoning benchmarks by 20.5\%, while
the same model trained on RSD-generated reasoning traces achieves meaningful
improvements of 4.9\%. Our analysis reveals that low probability tokens
constitute the critical bottleneck in reasoning ability transfer. However,
cross-model experiments demonstrate that RSD traces are model-specific rather
than universally applicable, indicating that distributional alignment must be
tailored for each student architecture's unique internal representation.

</details>


### [76] [FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding](https://arxiv.org/abs/2509.22237)
*Haorui Chen,Chengze Li,Jia Li*

Main category: cs.CL

TL;DR: 提出了FeatBench基准测试，专门评估大语言模型在"氛围编程"范式下的功能实现能力，填补了现有代码生成基准测试的空白。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试存在偏差，要么需要代码级规范，要么只关注问题解决，忽视了氛围编程范式下的功能实现场景。

Method: 构建纯自然语言提示的基准测试，采用多级过滤流程确保质量，包含失败转通过和通过转通过测试用例，涵盖多样化应用领域。

Result: 评估显示氛围编程范式下的功能实现具有显著挑战性，最高成功率仅为29.94%，并发现"激进实现"策略会导致关键失败但产生更优软件设计的矛盾现象。

Conclusion: FeatBench基准测试揭示了氛围编程中功能实现的挑战性，为社区研究提供了重要工具和洞见。

Abstract: The rapid advancement of Large Language Models (LLMs) has given rise to a
novel software development paradigm known as "vibe coding," where users
interact with coding agents through high-level natural language. However,
existing evaluation benchmarks for code generation inadequately assess an
agent's vibe coding capabilities. Existing benchmarks are misaligned, as they
either require code-level specifications or focus narrowly on issue-solving,
neglecting the critical scenario of feature implementation within the vibe
coding paradiam. To address this gap, we propose FeatBench, a novel benchmark
for vibe coding that focuses on feature implementation. Our benchmark is
distinguished by several key features: 1. Pure Natural Language Prompts. Task
inputs consist solely of abstract natural language descriptions, devoid of any
code or structural hints. 2. A Rigorous & Evolving Data Collection Process.
FeatBench is built on a multi-level filtering pipeline to ensure quality and a
fully automated pipeline to evolve the benchmark, mitigating data
contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass
(F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent
regressions. 4. Diverse Application Domains. The benchmark includes
repositories from diverse domains to ensure it reflects real-world scenarios.
We evaluate two state-of-the-art agent frameworks with four leading LLMs on
FeatBench. Our evaluation reveals that feature implementation within the vibe
coding paradigm is a significant challenge, with the highest success rate of
only 29.94%. Our analysis also reveals a tendency for "aggressive
implementation," a strategy that paradoxically leads to both critical failures
and superior software design. We release FeatBench, our automated collection
pipeline, and all experimental results to facilitate further community
research.

</details>


### [77] [FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction](https://arxiv.org/abs/2509.22243)
*Yuan Ge,Saihan Chen,Jingqi Xiao,Xiaoqian Liu,Tong Xiao,Yan Xiang,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: FLEXI是首个专门评估全双工LLM-人类语音交互的基准测试，特别关注紧急情况下的模型中断能力，揭示了开源与商业模型在紧急意识、轮次终止和交互延迟方面的显著差距。


<details>
  <summary>Details</summary>
Motivation: 全双工语音到语音LLM是实现自然人机交互的基础，但目前缺乏有效的基准测试和建模方法，特别是在紧急中断场景下的评估。

Method: 开发FLEXI基准测试，通过六个多样化的人-LLM交互场景系统评估实时对话的延迟、质量和对话效果，特别关注模型中断能力。

Result: 发现开源模型与商业模型在紧急意识、轮次终止和交互延迟方面存在显著差距，商业模型表现更好。

Conclusion: 下一对令牌预测为实现真正无缝和类人全双工交互提供了有前景的路径。

Abstract: Full-Duplex Speech-to-Speech Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling real-time spoken dialogue systems.
However, benchmarking and modeling these models remains a fundamental
challenge. We introduce FLEXI, the first benchmark for full-duplex LLM-human
spoken interaction that explicitly incorporates model interruption in emergency
scenarios. FLEXI systematically evaluates the latency, quality, and
conversational effectiveness of real-time dialogue through six diverse
human-LLM interaction scenarios, revealing significant gaps between open source
and commercial models in emergency awareness, turn terminating, and interaction
latency. Finally, we suggest that next token-pair prediction offers a promising
path toward achieving truly seamless and human-like full-duplex interaction.

</details>


### [78] [Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance](https://arxiv.org/abs/2509.22250)
*Wenbin Hu,Huihao Jing,Haochen Shi,Haoran Li,Yangqiu Song*

Main category: cs.CL

TL;DR: 该论文从法律合规角度解决LLM安全问题，提出安全合规概念，基于欧盟AI法案和GDPR构建新基准，并通过GRPO方法训练Compliance Reasoner，在基准上分别提升10.45%和11.85%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全方法依赖临时分类法，缺乏系统化保护，无法确保现代LLM系统复杂行为的安全性。

Method: 1) 基于法律框架构建安全合规基准；2) 使用GRPO方法对齐Qwen3-8B模型，构建Compliance Reasoner安全推理器。

Result: Compliance Reasoner在新基准上表现优异，欧盟AI法案平均提升10.45%，GDPR平均提升11.85%。

Conclusion: 从法律合规角度解决LLM安全问题是有效的，能够系统化提升模型安全性。

Abstract: The proliferation of Large Language Models (LLMs) has demonstrated remarkable
capabilities, elevating the critical importance of LLM safety. However,
existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic
protection, failing to ensure safety for the nuanced and complex behaviors of
modern LLM systems. To address this problem, we solve LLM safety from legal
compliance perspectives, named safety compliance. In this work, we posit
relevant established legal frameworks as safety standards for defining and
measuring safety compliance, including the EU AI Act and GDPR, which serve as
core legal frameworks for AI safety and data security in Europe. To bridge the
gap between LLM safety and legal compliance, we first develop a new benchmark
for safety compliance by generating realistic LLM safety scenarios seeded with
legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization
(GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively
aligns LLMs with legal standards to mitigate safety risks. Our comprehensive
experiments demonstrate that the Compliance Reasoner achieves superior
performance on the new benchmark, with average improvements of +10.45% for the
EU AI Act and +11.85% for GDPR.

</details>


### [79] [Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs](https://arxiv.org/abs/2509.22251)
*Yifang Zhang,Pengfei Duan,Yiwen Yang,Shengwu Xiong*

Main category: cs.CL

TL;DR: SSKG-LLM是一个创新模型架构，旨在将知识图谱的结构和语义信息有效整合到LLMs的推理过程中，解决LLMs处理知识图谱时只关注语义而忽略结构的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs处理幻觉问题的主要方法是结合知识图谱，但LLMs通常将KGs视为纯文本，只提取语义信息而忽略了KGs的关键结构方面。另一个挑战是KGs编码器和LLMs文本嵌入空间之间的差距，阻碍了结构化知识的有效整合。

Method: SSKG-LLM包含知识图谱检索(KGR)模块和知识图谱编码(KGE)模块来保持语义同时利用结构，然后加入知识图谱适应(KGA)模块使LLMs能够理解KGs嵌入。

Result: 通过广泛实验和详细分析，探索了整合KGs结构信息如何增强LLMs的事实推理能力。

Conclusion: SSKG-LLM成功解决了LLMs整合知识图谱时结构信息利用不足的问题，通过创新的模块设计实现了结构和语义信息的有效融合，提升了LLMs的事实推理能力。

Abstract: Currently, the main approach for Large Language Models (LLMs) to tackle the
hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs
typically treat KGs as plain text, extracting only semantic information and
limiting their use of the crucial structural aspects of KGs. Another challenge
is the gap between the embedding spaces of KGs encoders and LLMs text
embeddings, which hinders the effective integration of structured knowledge. To
overcome these obstacles, we put forward the SSKG-LLM, an innovative model
architecture that is designed to efficiently integrate both the Structural and
Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM
incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph
Encoding (KGE) module to preserve semantics while utilizing structure. Then,
the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to
understand KGs embeddings. We conduct extensive experiments and provide a
detailed analysis to explore how incorporating the structural information of
KGs can enhance the factual reasoning abilities of LLMs. Our code are available
at https://github.com/yfangZhang/SSKG-LLM.

</details>


### [80] [Bridging Fairness and Explainability: Can Input-Based Explanations Promote Fairness in Hate Speech Detection?](https://arxiv.org/abs/2509.22291)
*Yifan Wang,Mayank Jobanputra,Ji-Ung Lee,Soyoung Oh,Isabel Valera,Vera Demberg*

Main category: cs.CL

TL;DR: 本文首次系统研究了仇恨言论检测中可解释性与公平性的关系，发现基于输入的解释能有效识别偏见预测并帮助训练减偏，但不可靠于选择公平模型。


<details>
  <summary>Details</summary>
Motivation: NLP模型常从训练数据中复制或放大社会偏见，而其黑盒特性使用户难以识别偏见预测，开发者难以有效减轻偏见。现有关于公平NLP中可解释性的研究多为定性，缺乏大规模定量分析。

Method: 在仇恨言论检测中系统研究可解释性与公平性的关系，关注编码器和仅解码器模型。考察三个关键维度：(1)识别偏见预测，(2)选择公平模型，(3)训练中减轻偏见。

Result: 基于输入的解释能有效检测偏见预测，并作为减少训练偏见的有效监督，但在候选模型中选择公平模型时不可靠。

Conclusion: 基于输入的解释在识别偏见预测和训练减偏方面有效，但不适合用于模型选择，这为公平NLP中可解释性的应用提供了重要指导。

Abstract: Natural language processing (NLP) models often replicate or amplify social
bias from training data, raising concerns about fairness. At the same time,
their black-box nature makes it difficult for users to recognize biased
predictions and for developers to effectively mitigate them. While some studies
suggest that input-based explanations can help detect and mitigate bias, others
question their reliability in ensuring fairness. Existing research on
explainability in fair NLP has been predominantly qualitative, with limited
large-scale quantitative analysis. In this work, we conduct the first
systematic study of the relationship between explainability and fairness in
hate speech detection, focusing on both encoder- and decoder-only models. We
examine three key dimensions: (1) identifying biased predictions, (2) selecting
fair models, and (3) mitigating bias during model training. Our findings show
that input-based explanations can effectively detect biased predictions and
serve as useful supervision for reducing bias during training, but they are
unreliable for selecting fair models among candidates.

</details>


### [81] [Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs](https://arxiv.org/abs/2509.22338)
*Felix Vossel,Till Mossakowski,Björn Gehrke*

Main category: cs.CL

TL;DR: 本文系统评估了微调LLM将自然语言翻译为一阶逻辑(FOL)的任务，发现Flan-T5-XXL在提供谓词列表时达到70%准确率，优于GPT-4o和符号系统。


<details>
  <summary>Details</summary>
Motivation: 自动化自然语言到一阶逻辑的翻译对于知识表示和形式方法至关重要，但目前仍具挑战性。

Method: 使用MALLS和Willow数据集，比较编码器-解码器与仅解码器架构，探索词汇扩展、谓词条件化和多语言训练等技术，引入精确匹配、逻辑等价和谓词对齐等指标。

Result: 微调Flan-T5-XXL在提供谓词列表时达到70%准确率，优于GPT-4o和DeepSeek-R1-0528模型；谓词可用性提升性能15-20%；T5模型优于更大的仅解码器LLM；模型能泛化到未见过的逻辑论证。

Conclusion: 结构逻辑翻译表现稳健，但谓词提取成为主要瓶颈。

Abstract: Automating the translation of natural language to first-order logic (FOL) is
crucial for knowledge representation and formal methods, yet remains
challenging. We present a systematic evaluation of fine-tuned LLMs for this
task, comparing architectures (encoder-decoder vs. decoder-only) and training
strategies. Using the MALLS and Willow datasets, we explore techniques like
vocabulary extension, predicate conditioning, and multilingual training,
introducing metrics for exact match, logical equivalence, and predicate
alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate
lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT
reasoning ability as well as symbolic systems like ccg2lambda. Key findings
show: (1) predicate availability boosts performance by 15-20%, (2) T5 models
surpass larger decoder-only LLMs, and (3) models generalize to unseen logical
arguments (FOLIO dataset) without specific training. While structural logic
translation proves robust, predicate extraction emerges as the main bottleneck.

</details>


### [82] [Transformers Can Learn Connectivity in Some Graphs but Not Others](https://arxiv.org/abs/2509.22343)
*Amit Roy,Abulhair Saparov*

Main category: cs.CL

TL;DR: 本文研究了transformer模型学习传递关系推理的能力，发现模型在网格状有向图上能较好学习连通性，但高维网格和包含多个不连通分量的图对模型更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 研究transformer模型从训练样本中学习传递关系推理的能力，以及模型规模扩展对此能力的影响，这对确保大语言模型事实正确性至关重要。

Method: 生成有向图来训练不同规模的transformer模型，评估它们在各种图大小下推断传递关系的能力，特别关注网格图的维度和连通分量数量。

Result: transformer能够学习低维网格图的连通性，高维网格图更具挑战性；增大模型规模能改善在网格图上的泛化能力；对于包含多个不连通分量的非网格图，模型学习困难。

Conclusion: transformer学习传递关系推理的能力受图结构影响，在低维网格图上表现良好，但复杂图结构和高维问题仍是挑战，模型规模扩展有助于改善泛化性能。

Abstract: Reasoning capability is essential to ensure the factual correctness of the
responses of transformer-based Large Language Models (LLMs), and robust
reasoning about transitive relations is instrumental in many settings, such as
causal inference. Hence, it is essential to investigate the capability of
transformers in the task of inferring transitive relations (e.g., knowing A
causes B and B causes C, then A causes C). The task of inferring transitive
relations is equivalent to the task of connectivity in directed graphs (e.g.,
knowing there is a path from A to B, and there is a path from B to C, then
there is a path from A to C). Past research focused on whether transformers can
learn to infer transitivity from in-context examples provided in the input
prompt. However, transformers' capability to infer transitive relations from
training examples and how scaling affects the ability is unexplored. In this
study, we seek to answer this question by generating directed graphs to train
transformer models of varying sizes and evaluate their ability to infer
transitive relations for various graph sizes. Our findings suggest that
transformers are capable of learning connectivity on "grid-like'' directed
graphs where each node can be embedded in a low-dimensional subspace, and
connectivity is easily inferable from the embeddings of the nodes. We find that
the dimensionality of the underlying grid graph is a strong predictor of
transformers' ability to learn the connectivity task, where higher-dimensional
grid graphs pose a greater challenge than low-dimensional grid graphs. In
addition, we observe that increasing the model scale leads to increasingly
better generalization to infer connectivity over grid graphs. However, if the
graph is not a grid graph and contains many disconnected components,
transformers struggle to learn the connectivity task, especially when the
number of components is large.

</details>


### [83] [The InviTE Corpus: Annotating Invectives in Tudor English Texts for Computational Modeling](https://arxiv.org/abs/2509.22345)
*Sophie Spliethoff,Sanne Hoeken,Silke Schwandt,Sina Zarrieß,Özge Alaçam*

Main category: cs.CL

TL;DR: 本文开发了InviTE语料库，包含近2000个早期现代英语句子，用于研究都铎王朝英格兰宗教改革中的辱骂性语言，并比较了微调BERT模型与零样本LLMs在辱骂检测任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 将自然语言处理技术应用于历史研究，特别是研究都铎王朝英格兰宗教改革时期的宗教辱骂语言。

Method: 构建了从原始数据到预处理、数据选择和迭代标注的工作流程，创建了InviTE语料库，并比较了基于历史数据预训练的微调BERT模型与零样本指令调优LLMs的性能。

Result: 基于历史数据预训练并针对辱骂检测任务微调的模型表现优于零样本LLMs。

Conclusion: 针对特定历史领域预训练并进行任务微调的模型在历史文本分析中具有优势，为历史研究中的NLP应用提供了有效方法。

Abstract: In this paper, we aim at the application of Natural Language Processing (NLP)
techniques to historical research endeavors, particularly addressing the study
of religious invectives in the context of the Protestant Reformation in Tudor
England. We outline a workflow spanning from raw data, through pre-processing
and data selection, to an iterative annotation process. As a result, we
introduce the InviTE corpus -- a corpus of almost 2000 Early Modern English
(EModE) sentences, which are enriched with expert annotations regarding
invective language throughout 16th-century England. Subsequently, we assess and
compare the performance of fine-tuned BERT-based models and zero-shot prompted
instruction-tuned large language models (LLMs), which highlights the
superiority of models pre-trained on historical data and fine-tuned to
invective detection.

</details>


### [84] [Conversational Implicatures: Modelling Relevance Theory Probabilistically](https://arxiv.org/abs/2509.22354)
*Christoph Unger,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 本文探讨如何将贝叶斯方法应用于关联理论语用学，特别是研究通过会话含义传达隐含意义的语用现象。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯概率理论在认知科学中的应用发展，以及理性言语行为理论的成功，促使研究者探索将类似贝叶斯方法应用于关联理论语用学的可能性。

Method: 采用贝叶斯概率框架，研究会话含义这一典型的语用现象，探索如何将关联理论语用学与贝叶斯计算方法相结合。

Result: 论文探讨了贝叶斯方法在关联理论语用学中的适用性，但未提供具体的实验结果或模型验证。

Conclusion: 贝叶斯方法有望为关联理论语用学提供新的计算建模框架，特别是在理解隐含意义传达机制方面具有潜力。

Abstract: Recent advances in Bayesian probability theory and its application to
cognitive science in combination with the development of a new generation of
computational tools and methods for probabilistic computation have led to a
'probabilistic turn' in pragmatics and semantics. In particular, the framework
of Rational Speech Act theory has been developed to model broadly Gricean
accounts of pragmatic phenomena in Bayesian terms, starting with fairly simple
reference games and covering ever more complex communicative exchanges such as
verbal syllogistic reasoning. This paper explores in which way a similar
Bayesian approach might be applied to relevance-theoretic pragmatics (Sperber &
Wilson, 1995) by study a paradigmatic pragmatic phenomenon: the communication
of implicit meaning by ways of (conversational) implicatures.

</details>


### [85] [CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models](https://arxiv.org/abs/2509.22360)
*Niharika Hegde,Subarnaduti Paul,Lars Joel-Frey,Manuel Brack,Kristian Kersting,Martin Mundt,Patrick Schramowski*

Main category: cs.CL

TL;DR: CHRONOBERG是一个跨越250年的英语书籍语料库，用于研究语言的时间演变和训练具有时间感知能力的语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有语料库缺乏长期时间结构，限制了LLM理解语言语义和规范演变的能力，需要支持历时语言变化分析的时间结构化资源。

Method: 从Project Gutenberg收集英语书籍文本，构建时间结构化语料库，通过时间敏感的VAD分析量化词汇语义变化，构建历史校准的情感词典。

Result: 展示了现代LLM工具在检测歧视性语言和跨时间情感分析方面的局限性，语言模型难以编码历时意义变化。

Conclusion: 需要时间感知的训练和评估流程，CHRONOBERG为语言变化和时间泛化研究提供了可扩展资源。

Abstract: Large language models (LLMs) excel at operating at scale by leveraging social
media and various data crawled from the web. Whereas existing corpora are
diverse, their frequent lack of long-term temporal structure may however limit
an LLM's ability to contextualize semantic and normative evolution of language
and to capture diachronic variation. To support analysis and training for the
latter, we introduce CHRONOBERG, a temporally structured corpus of English book
texts spanning 250 years, curated from Project Gutenberg and enriched with a
variety of temporal annotations. First, the edited nature of books enables us
to quantify lexical semantic change through time-sensitive
Valence-Arousal-Dominance (VAD) analysis and to construct historically
calibrated affective lexicons to support temporally grounded interpretation.
With the lexicons at hand, we demonstrate a need for modern LLM-based tools to
better situate their detection of discriminatory language and contextualization
of sentiment across various time-periods. In fact, we show how language models
trained sequentially on CHRONOBERG struggle to encode diachronic shifts in
meaning, emphasizing the need for temporally aware training and evaluation
pipelines, and positioning CHRONOBERG as a scalable resource for the study of
linguistic change and temporal generalization. Disclaimer: This paper includes
language and display of samples that could be offensive to readers. Open
Access: Chronoberg is available publicly on HuggingFace at (
https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at
(https://github.com/paulsubarna/Chronoberg).

</details>


### [86] [Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance Logs using Large Language Models](https://arxiv.org/abs/2509.22366)
*Max Malyi,Jonathan Shek,Andre Biscaya*

Main category: cs.CL

TL;DR: 本文提出了一个使用大型语言模型对风力涡轮机维护日志进行深度语义分析的框架，超越了传统的文本分类方法，实现了故障模式识别、因果链推理等复杂分析任务。


<details>
  <summary>Details</summary>
Motivation: 风力涡轮机维护日志中包含大量非结构化文本信息，传统定量可靠性分析方法难以利用这些数据。现有机器学习方法通常只停留在文本分类层面，无法进行更复杂的推理分析。

Method: 引入探索性框架，利用大型语言模型执行四个分析工作流：故障模式识别、因果链推理、站点比较分析和数据质量审计，将LLMs作为"可靠性协作者"使用。

Result: 结果表明LLMs能够超越简单标签分类，综合文本信息并生成可操作的专业级假设，有效解锁非结构化数据中隐藏的运营智能。

Conclusion: 这项工作为使用LLMs作为推理工具提供了新颖且可复现的方法论，为风能行业增强运营智能开辟了新途径。

Abstract: A wealth of operational intelligence is locked within the unstructured
free-text of wind turbine maintenance logs, a resource largely inaccessible to
traditional quantitative reliability analysis. While machine learning has been
applied to this data, existing approaches typically stop at classification,
categorising text into predefined labels. This paper addresses the gap in
leveraging modern large language models (LLMs) for more complex reasoning
tasks. We introduce an exploratory framework that uses LLMs to move beyond
classification and perform deep semantic analysis. We apply this framework to a
large industrial dataset to execute four analytical workflows: failure mode
identification, causal chain inference, comparative site analysis, and data
quality auditing. The results demonstrate that LLMs can function as powerful
"reliability co-pilots," moving beyond labelling to synthesise textual
information and generate actionable, expert-level hypotheses. This work
contributes a novel and reproducible methodology for using LLMs as a reasoning
tool, offering a new pathway to enhance operational intelligence in the wind
energy sector by unlocking insights previously obscured in unstructured data.

</details>


### [87] [What Is The Political Content in LLMs' Pre- and Post-Training Data?](https://arxiv.org/abs/2509.22367)
*Tanise Ceron,Dmitry Nikolaev,Dominik Stammbach,Debora Nozza*

Main category: cs.CL

TL;DR: 分析OLMO2模型训练数据中的政治偏见，发现左倾文档在数据集中占主导地位，且训练数据中的主要政治立场与模型在政策议题上的偏见密切相关。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型已知会产生政治偏见文本，但偏见如何产生尚不清楚。需要分析训练数据中的政治内容来解答这个问题。

Method: 从OLMO2的预训练和后训练语料库中抽取大样本，自动标注文档的政治倾向，分析来源领域和内容，并评估训练数据政治内容与模型政策立场的关系。

Result: 左倾文档在所有数据集中占主导；预训练语料比后训练数据包含更多政治参与内容；左右倾向文档通过不同价值观和合法性来源框架相似话题；训练数据中的主要立场与模型政治偏见强相关。

Conclusion: 需要在未来数据整理流程中整合政治内容分析，并详细记录过滤策略以提高透明度。

Abstract: Large language models (LLMs) are known to generate politically biased text,
yet how such biases arise remains unclear. A crucial step toward answering this
question is the analysis of training data, whose political content remains
largely underexplored in current LLM research. To address this gap, we present
in this paper an analysis of the pre- and post-training corpora of OLMO2, the
largest fully open-source model released together with its complete dataset.
From these corpora, we draw large random samples, automatically annotate
documents for political orientation, and analyze their source domains and
content. We then assess how political content in the training data correlates
with models' stance on specific policy issues. Our analysis shows that
left-leaning documents predominate across datasets, with pre-training corpora
containing significantly more politically engaged content than post-training
data. We also find that left- and right-leaning documents frame similar topics
through distinct values and sources of legitimacy. Finally, the predominant
stance in the training data strongly correlates with models' political biases
when evaluated on policy issues. These findings underscore the need to
integrate political content analysis into future data curation pipelines as
well as in-depth documentation of filtering strategies for transparency.

</details>


### [88] [Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding](https://arxiv.org/abs/2509.22437)
*Ziheng Chi,Yifan Hou,Chenxi Pang,Shaobo Cui,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: Chimera测试套件揭示了当前视觉语言模型在图表理解上存在严重缺陷，其看似强大的性能主要来自三种捷径行为而非真正的图表理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有图表相关基准测试中，视觉语言模型看似表现良好，但实际上可能依赖知识、推理或模态捷径，而非真正理解和推理图表内容。

Method: 构建包含7,500个高质量维基百科图表的Chimera测试套件，每个图表标注语义三元组和多层次问题，评估实体识别、关系理解、知识基础和视觉推理四个基本方面。

Result: 评估15个开源视觉语言模型发现，其强大性能主要来自三种捷径：视觉记忆捷径影响较小，知识回忆捷径作用中等，Clever-Hans捷径贡献显著。

Conclusion: 当前视觉语言模型在复杂视觉输入（如图表）理解上存在严重局限，需要更鲁棒的评估协议来基准真正的理解能力而非问答捷径。

Abstract: Diagrams convey symbolic information in a visual format rather than a linear
stream of words, making them especially challenging for AI models to process.
While recent evaluations suggest that vision-language models (VLMs) perform
well on diagram-related benchmarks, their reliance on knowledge, reasoning, or
modality shortcuts raises concerns about whether they genuinely understand and
reason over diagrams. To address this gap, we introduce Chimera, a
comprehensive test suite comprising 7,500 high-quality diagrams sourced from
Wikipedia; each diagram is annotated with its symbolic content represented by
semantic triples along with multi-level questions designed to assess four
fundamental aspects of diagram comprehension: entity recognition, relation
understanding, knowledge grounding, and visual reasoning. We use Chimera to
measure the presence of three types of shortcuts in visual question answering:
(1) the visual-memorization shortcut, where VLMs rely on memorized visual
patterns; (2) the knowledge-recall shortcut, where models leverage memorized
factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans
shortcut, where models exploit superficial language patterns or priors without
true comprehension. We evaluate 15 open-source VLMs from 7 model families on
Chimera and find that their seemingly strong performance largely stems from
shortcut behaviors: visual-memorization shortcuts have slight impact,
knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts
contribute significantly. These findings expose critical limitations in current
VLMs and underscore the need for more robust evaluation protocols that
benchmark genuine comprehension of complex visual inputs (e.g., diagrams)
rather than question-answering shortcuts.

</details>


### [89] [Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning](https://arxiv.org/abs/2509.22472)
*Antreas Ioannou,Andreas Shiamishis,Nora Hollenstein,Nezihe Merve Gürel*

Main category: cs.CL

TL;DR: 评估LLaMA和Gemini在多语言法律任务中的表现，发现法律任务对LLM构成显著挑战，准确率常低于50%，且存在提示敏感性和对抗脆弱性。


<details>
  <summary>Details</summary>
Motivation: 在LLM主导的时代，了解其在法律等高风险领域的能力和局限性至关重要，特别是在多语言、跨司法管辖区和对抗性环境中的表现尚未充分探索。

Method: 使用LLM-as-a-Judge方法进行人类对齐评估，在多语言法律和非法律基准上测试LLaMA和Gemini，并通过字符和词级扰动评估其对抗鲁棒性。

Result: 法律推理任务准确率常低于50%（如LEXam），而通用任务如XNLI可达70%以上；Gemini平均比LLaMA高约24个百分点；英语结果更稳定但不总是更准确；语言性能与其与英语的句法相似性相关。

Conclusion: 尽管新LLM有所改进，但在关键的多语言法律应用中可靠部署仍面临挑战，需要进一步研究提升其法律推理能力和对抗鲁棒性。

Abstract: In an era dominated by Large Language Models (LLMs), understanding their
capabilities and limitations, especially in high-stakes fields like law, is
crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,
DeepSeek, and other emerging models are increasingly integrated into legal
workflows, their performance in multilingual, jurisdictionally diverse, and
adversarial contexts remains insufficiently explored. This work evaluates LLaMA
and Gemini on multilingual legal and non-legal benchmarks, and assesses their
adversarial robustness in legal tasks through character and word-level
perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.
We moreover present an open-source, modular evaluation pipeline designed to
support multilingual, task-diverse benchmarking of any combination of LLMs and
datasets, with a particular focus on legal tasks, including classification,
summarization, open questions, and general reasoning. Our findings confirm that
legal tasks pose significant challenges for LLMs with accuracies often below
50% on legal reasoning benchmarks such as LEXam, compared to over 70% on
general-purpose tasks like XNLI. In addition, while English generally yields
more stable results, it does not always lead to higher accuracy. Prompt
sensitivity and adversarial vulnerability is also shown to persist across
languages. Finally, a correlation is found between the performance of a
language and its syntactic similarity to English. We also observe that LLaMA is
weaker than Gemini, with the latter showing an average advantage of about 24
percentage points across the same task. Despite improvements in newer LLMs,
challenges remain in deploying them reliably for critical, multilingual legal
applications.

</details>


### [90] [Detecting (Un)answerability in Large Language Models with Linear Directions](https://arxiv.org/abs/2509.22449)
*Maor Juliet Lavi,Tova Milo,Mor Geva*

Main category: cs.CL

TL;DR: 提出一种通过激活空间方向检测LLM不可回答性的方法，在抽取式QA任务中有效识别无答案问题，并具有良好的跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型即使缺乏必要信息时也会自信回答，导致幻觉答案。需要解决不可回答性检测问题，特别是在抽取式问答中判断文本是否包含足够信息来回答问题。

Method: 通过在推理过程中应用激活加法，识别模型激活空间中捕获不可回答性的方向，并将隐藏激活投影到该方向进行分类。

Result: 在两个开源LLM和四个抽取式QA基准测试中，该方法有效检测不可回答问题，跨数据集泛化能力优于现有提示和分类器方法。该方向还可扩展到科学共识缺乏和主观性等其他不可回答情况。

Conclusion: 通过激活空间方向控制可以有效调节模型的弃权行为，为LLM的不可回答性检测提供了可靠解决方案。

Abstract: Large language models (LLMs) often respond confidently to questions even when
they lack the necessary information, leading to hallucinated answers. In this
work, we study the problem of (un)answerability detection, focusing on
extractive question answering (QA) where the model should determine if a
passage contains sufficient information to answer a given question. We propose
a simple approach for identifying a direction in the model's activation space
that captures unanswerability and uses it for classification. This direction is
selected by applying activation additions during inference and measuring their
impact on the model's abstention behavior. We show that projecting hidden
activations onto this direction yields a reliable score for (un)answerability
classification. Experiments on two open-weight LLMs and four extractive QA
benchmarks show that our method effectively detects unanswerable questions and
generalizes better across datasets than existing prompt-based and
classifier-based approaches. Moreover, the obtained directions extend beyond
extractive QA to unanswerability that stems from factors, such as lack of
scientific consensus and subjectivity. Last, causal interventions show that
adding or ablating the directions effectively controls the abstention behavior
of the model.

</details>


### [91] [Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving](https://arxiv.org/abs/2509.22480)
*Hang Li,Kaiqi Yang,Yucheng Chu,Hui Liu,Jiliang Tang*

Main category: cs.CL

TL;DR: 论文提出解决方案分歧度作为衡量LLM问题解决能力的新指标，发现更高的分歧度与更好的性能正相关，并证明该指标能有效提升监督微调和强化学习的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖监督微调或强化学习来提升LLM的问题解决能力，本文从新的视角研究LLM对同一问题生成不同解决方案的多样性，探索解决方案分歧度与模型性能的关系。

Method: 基于解决方案分歧度与问题解决能力的正相关关系，提出将解决方案分歧度作为新指标来支持监督微调和强化学习策略，并在三个代表性问题领域进行验证。

Result: 实验表明，使用解决方案分歧度指标能够持续提高LLM在各种问题上的成功率，证明该指标的有效性。

Conclusion: 解决方案分歧度是一个简单但有效的工具，可用于推进LLM的训练和评估，为提升模型问题解决能力提供了新思路。

Abstract: Large language models (LLMs) have been widely used for problem-solving tasks.
Most recent work improves their performance through supervised fine-tuning
(SFT) with labeled data or reinforcement learning (RL) from task feedback. In
this paper, we study a new perspective: the divergence in solutions generated
by LLMs for a single problem. We show that higher solution divergence is
positively related to better problem-solving abilities across various models.
Based on this finding, we propose solution divergence as a novel metric that
can support both SFT and RL strategies. We test this idea on three
representative problem domains and find that using solution divergence
consistently improves success rates. These results suggest that solution
divergence is a simple but effective tool for advancing LLM training and
evaluation.

</details>


### [92] [InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models](https://arxiv.org/abs/2509.22536)
*Wenjun Wang,Shuo Cai,Congkai Xie,Mingfa Feng,Yiming Zhang,Zhen Li,Kejing Yang,Ming Li,Jiannong Cao,Yuan Xie,Hongxia Yang*

Main category: cs.CL

TL;DR: 提出了一个端到端的FP8训练方案，通过细粒度混合粒度量化策略，在保持数值精度的同时显著提升训练效率，证明FP8可作为BF16的实用替代方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练的巨大计算成本阻碍了创新，FP8训练虽有理论效率优势但缺乏开源训练方案，需要填补这一空白。

Method: 采用端到端FP8训练方案，结合持续预训练和监督微调，使用细粒度混合粒度量化策略来平衡数值保真度和计算效率。

Result: 在160B token语料库上的持续预训练实验表明，该方案稳定且几乎无损，推理基准测试性能与BF16基线相当，训练时间减少22%，峰值内存使用降低14%，吞吐量提升19%。

Conclusion: FP8是BF16的实用且鲁棒的替代方案，将发布相关代码以促进大规模模型训练的民主化。

Abstract: The immense computational cost of training Large Language Models (LLMs)
presents a major barrier to innovation. While FP8 training offers a promising
solution with significant theoretical efficiency gains, its widespread adoption
has been hindered by the lack of a comprehensive, open-source training recipe.
To bridge this gap, we introduce an end-to-end FP8 training recipe that
seamlessly integrates continual pre-training and supervised fine-tuning. Our
methodology employs a fine-grained, hybrid-granularity quantization strategy to
maintain numerical fidelity while maximizing computational efficiency. Through
extensive experiments, including the continue pre-training of models on a
160B-token corpus, we demonstrate that our recipe is not only remarkably stable
but also essentially lossless, achieving performance on par with the BF16
baseline across a suite of reasoning benchmarks. Crucially, this is achieved
with substantial efficiency improvements, including up to a 22% reduction in
training time, a 14% decrease in peak memory usage, and a 19% increase in
throughput. Our results establish FP8 as a practical and robust alternative to
BF16, and we will release the accompanying code to further democratize
large-scale model training.

</details>


### [93] [NeLLCom-Lex: A Neural-agent Framework to Study the Interplay between Lexical Systems and Language Use](https://arxiv.org/abs/2509.22479)
*Yuqing Zhang,Ecesu Ürker,Tessa Verhoef,Gemma Boleda,Arianna Bisazza*

Main category: cs.CL

TL;DR: NeLLCom-Lex是一个神经代理框架，通过将代理置于真实词汇系统中并操纵其交际需求来模拟语义变化，特别在颜色命名任务中研究词汇系统演化。


<details>
  <summary>Details</summary>
Motivation: 现有观察方法无法揭示语义变化的因果机制，而人类实验方法难以应用于历时过程。需要新方法来研究语义变化机制。

Method: 使用神经代理框架，首先将代理置于真实词汇系统（如英语），然后系统性地操纵其交际需求，通过监督学习和强化学习管道训练代理。

Result: 训练来'说'现有语言的神经代理能够在颜色命名中显著再现人类模式，支持NeLLCom-Lex用于阐明语义变化机制。

Conclusion: NeLLCom-Lex框架能够有效模拟语义变化，神经代理可以再现人类词汇行为模式，为研究语义变化机制提供了新途径。

Abstract: Lexical semantic change has primarily been investigated with observational
and experimental methods; however, observational methods (corpus analysis,
distributional semantic modeling) cannot get at causal mechanisms, and
experimental paradigms with humans are hard to apply to semantic change due to
the extended diachronic processes involved. This work introduces NeLLCom-Lex, a
neural-agent framework designed to simulate semantic change by first grounding
agents in a real lexical system (e.g. English) and then systematically
manipulating their communicative needs. Using a well-established color naming
task, we simulate the evolution of a lexical system within a single generation,
and study which factors lead agents to: (i) develop human-like naming behavior
and lexicons, and (ii) change their behavior and lexicons according to their
communicative needs. Our experiments with different supervised and
reinforcement learning pipelines show that neural agents trained to 'speak' an
existing language can reproduce human-like patterns in color naming to a
remarkable extent, supporting the further use of NeLLCom-Lex to elucidate the
mechanisms of semantic change.

</details>


### [94] [Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation](https://arxiv.org/abs/2509.22565)
*Wenyuan Chen,Fateme Nateghi Haredasht,Kameron C. Black,Francois Grolleau,Emily Alsentzer,Jonathan H. Chen,Stephen P. Ma*

Main category: cs.CL

TL;DR: 开发了一个基于检索增强的评估管道(RAEC)，利用历史消息-响应对来改进LLM生成的患者消息回复质量评估，通过两阶段DSPy管道实现可扩展的错误检测。


<details>
  <summary>Details</summary>
Motivation: EHR门户中的异步患者-临床医生消息增加了临床医生工作量，LLM可用于辅助起草回复，但其输出可能存在临床不准确、遗漏或语气不匹配等问题，需要稳健的评估方法。

Method: 引入包含5个领域和59个粒度错误代码的临床基础错误本体论；开发检索增强评估管道(RAEC)，利用语义相似的历史消息-响应对；采用两阶段DSPy提示架构进行分层错误检测。

Result: 在1,500多条患者消息上测试，检索上下文改进了临床完整性和工作流程适当性等领域的错误识别。人工验证显示上下文增强标签在一致性(50% vs 33%)和性能(F1=0.500 vs 0.256)上均优于基线。

Conclusion: RAEC管道可作为患者消息的AI护栏，检索上下文显著提高了LLM生成回复的评估质量。

Abstract: Asynchronous patient-clinician messaging via EHR portals is a growing source
of clinician workload, prompting interest in large language models (LLMs) to
assist with draft responses. However, LLM outputs may contain clinical
inaccuracies, omissions, or tone mismatches, making robust evaluation
essential. Our contributions are threefold: (1) we introduce a clinically
grounded error ontology comprising 5 domains and 59 granular error codes,
developed through inductive coding and expert adjudication; (2) we develop a
retrieval-augmented evaluation pipeline (RAEC) that leverages semantically
similar historical message-response pairs to improve judgment quality; and (3)
we provide a two-stage prompting architecture using DSPy to enable scalable,
interpretable, and hierarchical error detection. Our approach assesses the
quality of drafts both in isolation and with reference to similar past
message-response pairs retrieved from institutional archives. Using a two-stage
DSPy pipeline, we compared baseline and reference-enhanced evaluations on over
1,500 patient messages. Retrieval context improved error identification in
domains such as clinical completeness and workflow appropriateness. Human
validation on 100 messages demonstrated superior agreement (concordance = 50%
vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.
baseline, supporting the use of our RAEC pipeline as AI guardrails for patient
messaging.

</details>


### [95] [StateX: Enhancing RNN Recall via Post-training State Expansion](https://arxiv.org/abs/2509.22630)
*Xingyu Shen,Yingfa Chen,Zhen Leng Thai,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: StateX是一种训练流程，通过后训练高效扩展预训练RNN的状态大小，提升长上下文召回能力，而不显著增加参数或训练成本。


<details>
  <summary>Details</summary>
Motivation: Transformer模型处理长上下文成本高，而RNN虽然复杂度低但状态大小固定，难以准确召回长上下文信息。直接训练大状态RNN成本高昂。

Method: 为线性注意力和状态空间模型设计后训练架构修改，扩展状态大小但不增加或仅轻微增加模型参数。

Result: 在1.3B参数模型上的实验表明，StateX能高效提升RNN的召回和上下文学习能力，不产生高后训练成本或损害其他能力。

Conclusion: StateX提供了一种高效扩展RNN状态大小的方法，解决了RNN在长上下文召回方面的局限性。

Abstract: While Transformer-based models have demonstrated remarkable language modeling
performance, their high complexities result in high costs when processing long
contexts. In contrast, recurrent neural networks (RNNs) such as linear
attention and state space models have gained popularity due to their constant
per-token complexities. However, these recurrent models struggle with tasks
that require accurate recall of contextual information from long contexts,
because all contextual information is compressed into a constant-size recurrent
state. Previous works have shown that recall ability is positively correlated
with the recurrent state size, yet directly training RNNs with larger recurrent
states results in high training costs. In this paper, we introduce StateX, a
training pipeline for efficiently expanding the states of pre-trained RNNs
through post-training. For two popular classes of RNNs, linear attention and
state space models, we design post-training architectural modifications to
scale up the state size with no or negligible increase in model parameters.
Experiments on models up to 1.3B parameters demonstrate that StateX efficiently
enhances the recall and in-context learning ability of RNNs without incurring
high post-training costs or compromising other capabilities.

</details>


### [96] [JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited Resources for Slavic Languages: MT and QA](https://arxiv.org/abs/2509.22490)
*Hossain Shaikh Saadi,Minh Duc Bui,Mario Sanz-Guerrero,Katharina von der Wense*

Main category: cs.CL

TL;DR: JGU Mainz团队在WMT25有限资源斯拉夫语言任务中，使用Qwen2.5-3B-Instruct模型联合微调乌克兰语、上索布语和下索布语的机器翻译和问答任务，通过参数高效微调、数据增强和集成方法取得了优于基线的结果。


<details>
  <summary>Details</summary>
Motivation: 针对斯拉夫语言在有限资源条件下的机器翻译和问答任务挑战，特别是乌克兰语、上索布语和下索布语这些资源相对稀缺的语言。

Method: 使用Qwen2.5-3B-Instruct模型进行参数高效联合微调，整合额外的翻译和多选题问答数据，对乌克兰语问答采用检索增强生成，对上索布语和下索布语问答应用集成方法。

Result: 实验表明，所提出的模型在两个任务上都优于基线模型。

Conclusion: 通过参数高效微调、数据增强和任务特定优化策略，可以在有限资源条件下有效提升斯拉夫语言的机器翻译和问答性能。

Abstract: This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs
with Limited Resources for Slavic Languages: Machine Translation and Question
Answering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each
language, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with
parameter-efficient finetuning. Our pipeline integrates additional translation
and multiple-choice question answering (QA) data. For Ukrainian QA, we further
use retrieval-augmented generation. We also apply ensembling for QA in Upper
and Lower Sorbian. Experiments show that our models outperform the baseline on
both tasks.

</details>


### [97] [Variational Reasoning for Language Models](https://arxiv.org/abs/2509.22637)
*Xiangxin Zhou,Zichen Liu,Haonan Wang,Chao Du,Min Lin,Chongxuan Li,Liang Wang,Tianyu Pang*

Main category: cs.CL

TL;DR: 提出了一种将思维轨迹作为隐变量的变分推理框架，通过变分推理优化语言模型的推理能力，统一了变分推理和强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 为语言模型的推理能力提供原则性的概率视角，统一变分推理与强化学习方法，获得更稳定的训练目标。

Method: 从证据下界(ELBO)出发，扩展到多轨迹目标以获得更紧的边界，提出前向KL公式来稳定变分后验的训练，并将拒绝采样微调和二元奖励RL解释为局部前向KL目标。

Result: 在Qwen 2.5和Qwen 3模型系列上进行了广泛的推理任务验证，发现模型准确度隐含的权重揭示了之前未被注意的偏向简单问题的偏差。

Conclusion: 该工作提供了一个原则性的概率视角，统一了变分推理与强化学习方法，并为提高语言模型推理能力提供了稳定的目标。

Abstract: We introduce a variational reasoning framework for language models that
treats thinking traces as latent variables and optimizes them through
variational inference. Starting from the evidence lower bound (ELBO), we extend
it to a multi-trace objective for tighter bounds and propose a forward-KL
formulation that stabilizes the training of the variational posterior. We
further show that rejection sampling finetuning and binary-reward RL, including
GRPO, can be interpreted as local forward-KL objectives, where an implicit
weighting by model accuracy naturally arises from the derivation and reveals a
previously unnoticed bias toward easier questions. We empirically validate our
method on the Qwen 2.5 and Qwen 3 model families across a wide range of
reasoning tasks. Overall, our work provides a principled probabilistic
perspective that unifies variational inference with RL-style methods and yields
stable objectives for improving the reasoning ability of language models. Our
code is available at https://github.com/sail-sg/variational-reasoning.

</details>


### [98] [Representing LLMs in Prompt Semantic Task Space](https://arxiv.org/abs/2509.22506)
*Idan Kashani,Avi Mendelson,Yaniv Nemcovsky*

Main category: cs.CL

TL;DR: 提出一种无需训练的方法，将大语言模型表示为提示语义任务空间中的线性算子，提供高度可解释的模型表示，具有良好的可扩展性和实时适应性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在扩展性、重训练成本和可解释性方面的局限性，以更高效地识别最适合特定任务的LLM。

Method: 利用几何属性的闭式计算，将LLM表示为提示语义任务空间中的线性算子，无需训练即可获得模型表示。

Result: 在成功预测和模型选择任务中取得竞争性或最先进的结果，特别是在样本外场景中表现突出。

Conclusion: 该方法提供了一种高效、可扩展且高度可解释的LLM表示方式，能够实时适应动态扩展的模型库。

Abstract: Large language models (LLMs) achieve impressive results over various tasks,
and ever-expanding public repositories contain an abundance of pre-trained
models. Therefore, identifying the best-performing LLM for a given task is a
significant challenge. Previous works have suggested learning LLM
representations to address this. However, these approaches present limited
scalability and require costly retraining to encompass additional models and
datasets. Moreover, the produced representation utilizes distinct spaces that
cannot be easily interpreted. This work presents an efficient, training-free
approach to representing LLMs as linear operators within the prompts' semantic
task space, thus providing a highly interpretable representation of the models'
application. Our method utilizes closed-form computation of geometrical
properties and ensures exceptional scalability and real-time adaptability to
dynamically expanding repositories. We demonstrate our approach on success
prediction and model selection tasks, achieving competitive or state-of-the-art
results with notable performance in out-of-sample scenarios.

</details>


### [99] [Language Models Can Learn from Verbal Feedback Without Scalar Rewards](https://arxiv.org/abs/2509.22638)
*Renjie Luo,Zichen Liu,Xiangyan Liu,Chao Du,Min Lin,Wenhu Chen,Wei Lu,Tianyu Pang*

Main category: cs.CL

TL;DR: 提出反馈条件策略（FCP），将语言反馈作为条件信号而非标量奖励，通过最大似然训练学习反馈条件后验，并引入在线自举阶段来优化策略。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法将细粒度反馈压缩为标量奖励，导致信息丢失和尺度不平衡问题。

Method: FCP从响应-反馈对中直接学习，通过离线最大似然训练近似反馈条件后验，并加入在线自举阶段在正向条件下生成并接收新反馈来优化策略。

Result: 将反馈驱动学习重新定义为条件生成而非奖励优化，为LLMs提供了更丰富的从语言反馈中学习的方式。

Conclusion: FCP框架为语言模型从语言反馈中学习提供了更富表现力的方法，代码已开源。

Abstract: LLMs are often trained with RL from human or AI feedback, yet such methods
typically compress nuanced feedback into scalar rewards, discarding much of
their richness and inducing scale imbalance. We propose treating verbal
feedback as a conditioning signal. Inspired by language priors in text-to-image
generation, which enable novel outputs from unseen prompts, we introduce the
feedback-conditional policy (FCP). FCP learns directly from response-feedback
pairs, approximating the feedback-conditional posterior through maximum
likelihood training on offline data. We further develop an online bootstrapping
stage where the policy generates under positive conditions and receives fresh
feedback to refine itself. This reframes feedback-driven learning as
conditional generation rather than reward optimization, offering a more
expressive way for LLMs to directly learn from verbal feedback. Our code is
available at https://github.com/sail-sg/feedback-conditional-policy.

</details>


### [100] [We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong](https://arxiv.org/abs/2509.22510)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 提出AMBS方法，通过两阶段1对N框架实现多目标对齐，解决传统方法中的灾难性遗忘和推理碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法存在灾难性遗忘（优化一个目标会覆盖其他目标）和推理碎片化（各目标输出不一致）的问题，需要统一高效的多目标对齐方案。

Method: 两阶段1对N框架：阶段I计算共享表示，阶段II通过策略参考机制在并行分支中进行目标特定控制，保持跨目标一致性。

Result: 在Alpaca、BeaverTails和TruthfulQA上的实验显示，AMBS在多个7B LLM上持续改进HHH对齐，在DeepSeek-7B上平均对齐分数提升32.4%，不安全输出减少11.0%。

Conclusion: AMBS能够有效统一多目标对齐，在保持与最先进方法竞争力的同时，显著改善对齐性能和输出安全性。

Abstract: Alignment of Large Language Models (LLMs) along multiple
objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe
and reliable deployment. Prior work has used steering vector-small control
signals injected into hidden states-to guide LLM outputs, typically via
one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single
alignment objective can inadvertently overwrite representations learned for
other objectives, leading to catastrophic forgetting. More recent approaches
extend steering vectors via one-to-many (1-to-N) Transformer decoders. While
this alleviates catastrophic forgetting, naive multi-branch designs optimize
each objective independently, which can cause inference fragmentation-outputs
across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch
Steering (AMBS), a two-stage 1-to-N framework for unified and efficient
multi-objective alignment. In Stage I, post-attention hidden states of the
Transformer layer are computed once to form a shared representation. In Stage
II, this representation is cloned into parallel branches and steered via a
policy-reference mechanism, enabling objective-specific control while
maintaining cross-objective consistency. Empirical evaluations on Alpaca,
BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment
across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves
average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared
to a naive 1-to-N baseline, while remaining competitive with state-of-the-art
methods.

</details>


### [101] [Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity](https://arxiv.org/abs/2509.22641)
*Arkadiy Saakyan,Najoung Kim,Smaranda Muresan,Tuhin Chakrabarty*

Main category: cs.CL

TL;DR: 该论文研究了n-gram新颖性作为创造力衡量指标的局限性，发现虽然与专家评定的创造力正相关，但91%的高n-gram新颖性表达不被认为是创造性的，且开源LLM的高n-gram新颖性与低实用性相关。


<details>
  <summary>Details</summary>
Motivation: 研究n-gram新颖性作为创造力衡量指标的充分性，因为创造力理论强调新颖性和适当性的双重性质，而n-gram新颖性只关注新颖性。

Method: 通过7542个专家作家注释（n=26）对人工和AI生成文本的新颖性、实用性和合理性进行细读分析，并测试零样本、少样本和微调模型识别创造性表达和非实用性表达的能力。

Result: n-gram新颖性与专家评定的创造力正相关，但91%的高n-gram新颖性表达不被认为是创造性的；开源LLM的高n-gram新颖性与低实用性相关；前沿闭源模型产生创造性表达的可能性低于人类；LLM-as-a-Judge新颖性评分能预测专家偏好。

Conclusion: 不应单独依赖n-gram新颖性来衡量创造力，因为其无法充分捕捉创造力的双重性质（新颖性和适当性），且LLM在识别非实用性表达方面仍有改进空间。

Abstract: N-gram novelty is widely used to evaluate language models' ability to
generate text outside of their training data. More recently, it has also been
adopted as a metric for measuring textual creativity. However, theoretical work
on creativity suggests that this approach may be inadequate, as it does not
account for creativity's dual nature: novelty (how original the text is) and
appropriateness (how sensical and pragmatic it is). We investigate the
relationship between this notion of creativity and n-gram novelty through 7542
expert writer annotations (n=26) of novelty, pragmaticality, and sensicality
via close reading of human and AI-generated text. We find that while n-gram
novelty is positively associated with expert writer-judged creativity, ~91% of
top-quartile expressions by n-gram novelty are not judged as creative,
cautioning against relying on n-gram novelty alone. Furthermore, unlike
human-written text, higher n-gram novelty in open-source LLMs correlates with
lower pragmaticality. In an exploratory study with frontier close-source
models, we additionally confirm that they are less likely to produce creative
expressions than humans. Using our dataset, we test whether zero-shot,
few-shot, and finetuned models are able to identify creative expressions (a
positive aspect of writing) and non-pragmatic ones (a negative aspect).
Overall, frontier LLMs exhibit performance much higher than random but leave
room for improvement, especially struggling to identify non-pragmatic
expressions. We further find that LLM-as-a-Judge novelty scores from the
best-performing model were predictive of expert writer preferences.

</details>


### [102] [WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning](https://arxiv.org/abs/2509.22644)
*Zimu Lu,Houxing Ren,Yunqiao Yang,Ke Wang,Zhuofan Zong,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: WebGen-Agent是一个利用多层次视觉反馈来迭代生成和优化网站代码库的新型网站生成代理系统，通过视觉语言模型和GUI代理测试提供详细的文本描述和评分，结合回溯和选择最佳机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的代码代理在网站代码生成任务中仅依赖简单的代码执行反馈，无法准确评估生成代码的实际质量，特别是在视觉效果和用户交互方面。

Method: 提出WebGen-Agent系统，使用视觉语言模型分析网站截图和GUI代理测试结果，生成详细的文本描述和建议，并量化评分；结合回溯和选择最佳机制；引入Step-GRPO训练方法，将截图和GUI代理评分作为过程监督奖励信号。

Result: 在WebGen-Bench数据集上，WebGen-Agent将Claude-3.5-Sonnet的准确率从26.4%提升到51.9%，外观评分从3.0提升到3.9；Step-GRPO训练使Qwen2.5-Coder-7B-Instruct的准确率从38.9%提升到45.4%，外观评分从3.4提升到3.7。

Conclusion: WebGen-Agent通过综合的视觉反馈机制显著提升了网站代码生成的质量和准确性，证明了视觉反馈在代码生成任务中的重要性，同时Step-GRPO训练方法有效提升了模型性能。

Abstract: Agent systems powered by large language models (LLMs) have demonstrated
impressive performance on repository-level code-generation tasks. However, for
tasks such as website codebase generation, which depend heavily on visual
effects and user-interaction feedback, current code agents rely only on simple
code execution for feedback and verification. This approach fails to capture
the actual quality of the generated code. In this paper, we propose
WebGen-Agent, a novel website-generation agent that leverages comprehensive and
multi-level visual feedback to iteratively generate and refine the website
codebase. Detailed and expressive text descriptions and suggestions regarding
the screenshots and GUI-agent testing of the websites are generated by a visual
language model (VLM), together with scores that quantify their quality. The
screenshot and GUI-agent scores are further integrated with a backtracking and
select-best mechanism, enhancing the performance of the agent. Utilizing the
accurate visual scores inherent in the WebGen-Agent workflow, we further
introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve
the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using
the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we
provide a dense and reliable process supervision signal, which effectively
improves the model's website-generation ability. On the WebGen-Bench dataset,
WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%
and its appearance score from 3.0 to 3.9, outperforming the previous
state-of-the-art agent system. Additionally, our Step-GRPO training approach
increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and
raises the appearance score from 3.4 to 3.7.

</details>


### [103] [Think Socially via Cognitive Reasoning](https://arxiv.org/abs/2509.22546)
*Jinfeng Zhou,Zheyu Chen,Shuai Wang,Quanyu Dai,Zhenhua Dong,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 提出了认知推理范式CogFlow，通过模拟人类社会认知过程来增强LLM在社交情境中的推理能力，使用树状规划生成认知流数据，并通过监督微调和强化学习优化模型表现。


<details>
  <summary>Details</summary>
Motivation: 传统逻辑推理范式不适合处理社交情境，因为社交情境涉及模糊线索的解读过程，很少产生明确结果，需要模拟人类的社会认知过程。

Method: 1. 提出认知推理范式，将解读过程构建为相互连接的认知单元流；2. 通过树状规划模拟人类思维的联想性和渐进性，生成认知流数据集；3. 使用监督微调建立基础认知推理能力；4. 采用强化学习通过试错优化模型，使用多目标奖励函数同时优化认知流和响应质量。

Result: 实验表明CogFlow有效增强了LLM的社会认知能力，甚至对人类也有帮助，能够实现更有效的社交决策。

Conclusion: CogFlow框架成功地将人类的社会认知过程建模到LLM中，为处理社交情境提供了一种有效的认知推理范式。

Abstract: LLMs trained for logical reasoning excel at step-by-step deduction to reach
verifiable answers. However, this paradigm is ill-suited for navigating social
situations, which induce an interpretive process of analyzing ambiguous cues
that rarely yield a definitive outcome. To bridge this gap, we introduce
Cognitive Reasoning, a paradigm modeled on human social cognition. It
formulates the interpretive process into a structured cognitive flow of
interconnected cognitive units (e.g., observation or attribution), which
combine adaptively to enable effective social thinking and responses. We then
propose CogFlow, a complete framework that instills this capability in LLMs.
CogFlow first curates a dataset of cognitive flows by simulating the
associative and progressive nature of human thought via tree-structured
planning. After instilling the basic cognitive reasoning capability via
supervised fine-tuning, CogFlow adopts reinforcement learning to enable the
model to improve itself via trial and error, guided by a multi-objective reward
that optimizes both cognitive flow and response quality. Extensive experiments
show that CogFlow effectively enhances the social cognitive capabilities of
LLMs, and even humans, leading to more effective social decision-making.

</details>


### [104] [VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](https://arxiv.org/abs/2509.22651)
*Ke Wang,Houxing Ren,Zimu Lu,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 提出了VoiceAssistant-Eval基准测试，用于全面评估AI语音助手在听、说、看三个方面的能力，包含10,497个测试用例和13个任务类别。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法充分评估语音优先AI助手的完整能力范围，需要更全面的评估框架。

Method: 构建包含10,497个精心策划示例的基准测试，涵盖13个任务类别，包括自然声音、音乐、口语对话的听力测试，多轮对话、角色扮演模仿的说话测试，以及多样化图像的视觉测试。

Result: 评估了21个开源模型和GPT-4o-Audio，发现专有模型并非普遍优于开源模型；大多数模型擅长说话任务但在音频理解方面落后；设计良好的小型模型可与大型模型媲美。

Conclusion: VoiceAssistant-Eval识别了当前模型的差距，为下一代AI助手的开发提供了严格的评估框架。

Abstract: The growing capabilities of large language models and multimodal systems have
spurred interest in voice-first AI assistants, yet existing benchmarks are
inadequate for evaluating the full range of these systems' capabilities. We
introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI
assistants across listening, speaking, and viewing. VoiceAssistant-Eval
comprises 10,497 curated examples spanning 13 task categories. These tasks
include natural sounds, music, and spoken dialogue for listening; multi-turn
dialogue, role-play imitation, and various scenarios for speaking; and highly
heterogeneous images for viewing. To demonstrate its utility, we evaluate 21
open-source models and GPT-4o-Audio, measuring the quality of the response
content and speech, as well as their consistency. The results reveal three key
findings: (1) proprietary models do not universally outperform open-source
models; (2) most models excel at speaking tasks but lag in audio understanding;
and (3) well-designed smaller models can rival much larger ones. Notably, the
mid-sized Step-Audio-2-mini (7B) achieves more than double the listening
accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal
(audio plus visual) input and role-play voice imitation tasks are difficult for
current models, and significant gaps persist in robustness and safety
alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous
framework for evaluating and guiding the development of next-generation AI
assistants. Code and data will be released at
https://mathllm.github.io/VoiceAssistantEval/ .

</details>


### [105] [Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs](https://arxiv.org/abs/2509.22582)
*Yehonatan Pesiakhovsky,Zorik Gekhman,Yosi Mass,Liat Ein-Dor,Roi Reichart*

Main category: cs.CL

TL;DR: 研究LLMs在定位上下文幻觉方面的应用，构建了专门的基准测试，提出基于自由文本描述的新表示方法，评估了四种大型LLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的复杂评估流程不实用，需要更实用的替代方案来定位模型输出中无法通过源文本验证的信息（上下文幻觉）。

Method: 构建了针对LLMs的基准测试，包含1000多个示例的人工标注；提出基于自由文本描述的幻觉表示方法；评估了四种大型LLMs；分析了最优提示策略。

Result: 基准测试具有挑战性，最佳模型的F1分数仅为0.67；发现LLMs存在两个主要问题：错误地将缺失细节标记为不一致，以及难以处理包含模型参数知识但源文本中不存在的事实。

Conclusion: LLMs在定位上下文幻觉方面仍有挑战，需要改进提示策略和处理与参数知识对齐但源文本中不存在的信息的能力。

Abstract: Context-grounded hallucinations are cases where model outputs contain
information not verifiable against the source text. We study the applicability
of LLMs for localizing such hallucinations, as a more practical alternative to
existing complex evaluation pipelines. In the absence of established benchmarks
for meta-evaluation of hallucinations localization, we construct one tailored
to LLMs, involving a challenging human annotation of over 1,000 examples. We
complement the benchmark with an LLM-based evaluation protocol, verifying its
quality in a human evaluation. Since existing representations of hallucinations
limit the types of errors that can be expressed, we propose a new
representation based on free-form textual descriptions, capturing the full
range of possible errors. We conduct a comprehensive study, evaluating four
large-scale LLMs, which highlights the benchmark's difficulty, as the best
model achieves an F1 score of only 0.67. Through careful analysis, we offer
insights into optimal prompting strategies for the task and identify the main
factors that make it challenging for LLMs: (1) a tendency to incorrectly flag
missing details as inconsistent, despite being instructed to check only facts
in the output; and (2) difficulty with outputs containing factually correct
information absent from the source - and thus not verifiable - due to alignment
with the model's parametric knowledge.

</details>


### [106] [ArabJobs: A Multinational Corpus of Arabic Job Ads](https://arxiv.org/abs/2509.22589)
*Mo El-Haj*

Main category: cs.CL

TL;DR: ArabJobs是一个包含8500多个阿拉伯语招聘广告的公开语料库，涵盖埃及、约旦、沙特和阿联酋，用于研究阿拉伯劳动力市场的语言、地区和社会经济差异。


<details>
  <summary>Details</summary>
Motivation: 收集阿拉伯语招聘广告数据，以捕捉阿拉伯劳动力市场中的语言、地区和社会经济变化，支持公平性阿拉伯NLP和劳动力市场研究。

Method: 构建包含8500多个招聘广告、55万词的阿拉伯语语料库，分析性别代表性、职业结构、方言变化，并应用大语言模型进行薪资估计和职位分类标准化。

Result: 数据集展示了阿拉伯劳动力市场中的性别代表性和职业结构差异，揭示了不同地区间的方言变化，并成功应用于薪资估计、职位分类等任务。

Conclusion: ArabJobs语料库为公平性阿拉伯NLP和劳动力市场研究提供了有价值的资源，支持性别偏见检测、职业分类等基准任务，数据集已在GitHub公开。

Abstract: ArabJobs is a publicly available corpus of Arabic job advertisements
collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates.
Comprising over 8,500 postings and more than 550,000 words, the dataset
captures linguistic, regional, and socio-economic variation in the Arab labour
market. We present analyses of gender representation and occupational
structure, and highlight dialectal variation across ads, which offers
opportunities for future research. We also demonstrate applications such as
salary estimation and job category normalisation using large language models,
alongside benchmark tasks for gender bias detection and profession
classification. The findings show the utility of ArabJobs for fairness-aware
Arabic NLP and labour market research. The dataset is publicly available on
GitHub: https://github.com/drelhaj/ArabJobs.

</details>


### [107] [From Formal Language Theory to Statistical Learning: Finite Observability of Subregular Languages](https://arxiv.org/abs/2509.22598)
*Katsuhiko Hayashi,Hidetaka Kamigaito*

Main category: cs.CL

TL;DR: 论文证明所有标准次正则语言类在通过其判定谓词表示时都是线性可分的，这确立了有限可观测性并保证使用简单线性模型的可学习性。


<details>
  <summary>Details</summary>
Motivation: 为自然语言结构建模提供一个严格且可解释的基础，验证次正则层级在语言学习中的适用性。

Method: 通过理论证明和实验验证：理论证明次正则语言类的线性可分性，合成实验在无噪声条件下验证完美可分性，真实数据实验在英语形态学上测试。

Result: 合成实验确认无噪声条件下的完美可分性，真实数据实验显示学习到的特征与已知语言约束一致。

Conclusion: 次正则层级为自然语言结构建模提供了严格且可解释的基础，证明了其在语言学习中的有效性。

Abstract: We prove that all standard subregular language classes are linearly separable
when represented by their deciding predicates. This establishes finite
observability and guarantees learnability with simple linear models. Synthetic
experiments confirm perfect separability under noise-free conditions, while
real-data experiments on English morphology show that learned features align
with well-known linguistic constraints. These results demonstrate that the
subregular hierarchy provides a rigorous and interpretable foundation for
modeling natural language structure. Our code used in real-data experiments is
available at https://github.com/UTokyo-HayashiLab/subregular.

</details>


### [108] [Capturing Opinion Shifts in Deliberative Discourse through Frequency-based Quantum deep learning methods](https://arxiv.org/abs/2509.22603)
*Rakesh Thakur,Harsh Chaturvedi,Ruqayya Shah,Janvi Chauhan,Ayush Sharma*

Main category: cs.CL

TL;DR: 比较分析多种NLP技术在模拟审议过程中的表现，重点关注意见转变预测和结果分析，提出了两种优于现有方法的模型。


<details>
  <summary>Details</summary>
Motivation: 审议在决策过程中至关重要，通过权衡不同观点来塑造结果。随着NLP技术的发展，现在可以计算建模审议过程，分析意见转变并预测不同场景下的潜在结果。

Method: 收集来自不同背景个体的意见构建自建数据集，使用包含显著事实的产品演示来模拟审议过程。比较分析两种模型：基于频率的话语调制和量子审议框架。

Result: 提出的两种模型在解释审议性话语和产生有意义的见解方面优于现有最先进模型，能够有效捕捉观众意见的可测量转变。

Conclusion: 研究结果在公共政策制定、辩论评估、决策支持框架和大规模社交媒体意见挖掘方面具有实际应用价值。

Abstract: Deliberation plays a crucial role in shaping outcomes by weighing diverse
perspectives before reaching decisions. With recent advancements in Natural
Language Processing, it has become possible to computationally model
deliberation by analyzing opinion shifts and predicting potential outcomes
under varying scenarios. In this study, we present a comparative analysis of
multiple NLP techniques to evaluate how effectively models interpret
deliberative discourse and produce meaningful insights. Opinions from
individuals of varied backgrounds were collected to construct a self-sourced
dataset that reflects diverse viewpoints. Deliberation was simulated using
product presentations enriched with striking facts, which often prompted
measurable shifts in audience opinions. We have given comparative analysis
between two models namely Frequency-Based Discourse Modulation and
Quantum-Deliberation Framework which outperform the existing state of art
models. The findings highlight practical applications in public policy-making,
debate evaluation, decision-support frameworks, and large-scale social media
opinion mining.

</details>


### [109] [From tests to effect sizes: Quantifying uncertainty and statistical variability in multilingual and multitask NLP evaluation benchmarks](https://arxiv.org/abs/2509.22612)
*Jonne Sälevä,Duygu Ataman,Constantine Lignos*

Main category: cs.CL

TL;DR: 提出了基于重采样的方法来量化多语言/多任务NLP基准测试中评估指标的不确定性和统计精度，考虑了模型和数据相关的性能变化源。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往低估了评估指标的整体变异性，因为未能同时考虑模型和数据相关的性能变化源。

Method: 使用重采样方法，同时考虑模型和数据不确定性，计算各种排行榜指标（如平均值/中位数、模型间差异、排名）的抽样分布。

Result: 在多语言问答、机器翻译和命名实体识别任务中验证了该方法，证明同时考虑两种变化源对于避免显著低估整体变异性是必要的。

Conclusion: 重采样方法为NLP基准测试提供了更准确的统计精度评估，有助于更可靠地比较模型性能。

Abstract: In this paper, we introduce a set of resampling-based methods for quantifying
uncertainty and statistical precision of evaluation metrics in multilingual
and/or multitask NLP benchmarks. We show how experimental variation in
performance scores arises from both model- and data-related sources, and that
accounting for both of them is necessary to avoid substantially underestimating
the overall variability over hypothetical replications. Using multilingual
question answering, machine translation, and named entity recognition as
example tasks, we also demonstrate how resampling methods are useful for
computing sampling distributions for various quantities used in leaderboards
such as the average/median, pairwise differences between models, and rankings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [110] [Towards mitigating information leakage when evaluating safety monitors](https://arxiv.org/abs/2509.21344)
*Gerard Boxo,Aman Neelappa,Shivam Raval*

Main category: cs.AI

TL;DR: 本文提出了一个系统框架来评估白盒监控器的性能，重点关注其检测真实模型行为而非表面诱导伪影的能力。通过欺骗检测案例研究，识别了两种导致监控器性能虚高的泄漏形式，并提出了三种缓解策略。


<details>
  <summary>Details</summary>
Motivation: 训练和评估白盒监控器需要展示目标行为的响应样本，但这些样本通常通过提示或微调获得，导致诱导信息不可避免地泄漏到监控器数据中，从而虚高其有效性。

Method: 提出了三种评估策略：内容过滤（从输入中移除欺骗相关文本）、分数过滤（仅聚合任务相关标记的分数）、以及提示蒸馏微调模型生物（训练模型在没有明确提示的情况下表现出欺骗行为）。

Result: 实验发现：内容过滤能平滑移除诱导信号，使探针AUROC降低30%；分数过滤使AUROC降低15%；微调模型生物改进了监控器评估但使其性能降低高达40%。

Conclusion: 白盒监控器的评估需要仔细考虑诱导泄漏问题，提出的缓解策略能更准确地衡量监控器检测真实模型行为的能力，避免因表面伪影而虚高性能。

Abstract: White box monitors that analyze model internals offer promising advantages
for detecting potentially harmful behaviors in large language models, including
lower computational costs and integration into layered defense systems.However,
training and evaluating these monitors requires response exemplars that exhibit
the target behaviors, typically elicited through prompting or fine-tuning. This
presents a challenge when the information used to elicit behaviors inevitably
leaks into the data that monitors ingest, inflating their effectiveness. We
present a systematic framework for evaluating a monitor's performance in terms
of its ability to detect genuine model behavior rather than superficial
elicitation artifacts. Furthermore, we propose three novel strategies to
evaluate the monitor: content filtering (removing deception-related text from
inputs), score filtering (aggregating only over task-relevant tokens), and
prompt distilled fine-tuned model organisms (models trained to exhibit
deceptive behavior without explicit prompting). Using deception detection as a
representative case study, we identify two forms of leakage that inflate
monitor performance: elicitation leakage from prompts that explicitly request
harmful behavior, and reasoning leakage from models that verbalize their
deceptive actions. Through experiments on multiple deception benchmarks, we
apply our proposed mitigation strategies and measure performance retention. Our
evaluation of the monitors reveal three crucial findings: (1) Content filtering
is a good mitigation strategy that allows for a smooth removal of elicitation
signal and can decrease probe AUROC by 30\% (2) Score filtering was found to
reduce AUROC by 15\% but is not as straightforward to attribute to (3) A
finetuned model organism improves monitor evaluations but reduces their
performance by upto 40\%, even when re-trained.

</details>


### [111] [Correct Reasoning Paths Visit Shared Decision Pivots](https://arxiv.org/abs/2509.21549)
*Dongkyu Cho,Amy B. Z. Zhang,Bilel Fehri,Sheng Wang,Rumi Chunara,Rui Song,Hengrui Cai*

Main category: cs.AI

TL;DR: 提出决策支点概念，通过自训练方法验证和压缩思维链推理路径，无需真实推理数据或外部指标即可对齐模型推理过程。


<details>
  <summary>Details</summary>
Motivation: 思维链推理暴露了大型语言模型的中间思考过程，但大规模验证这些推理轨迹仍然是一个未解决的问题。

Method: 引入决策支点作为可验证的检查点，提出自训练流程：采样多样推理路径挖掘共享支点，用辅助验证器压缩轨迹为支点聚焦的短路径推理，使用自生成输出进行后训练。

Result: 在LogiQA、MedQA和MATH500等标准基准测试中验证了方法的有效性。

Conclusion: 该方法能够在不依赖真实推理数据或外部指标的情况下有效对齐模型推理过程。

Abstract: Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of
large language models (LLMs), yet verifying those traces at scale remains
unsolved. In response, we introduce the idea of decision pivots-minimal,
verifiable checkpoints that any correct reasoning path must visit. We
hypothesize that correct reasoning, though stylistically diverse, converge on
the same pivot set, while incorrect ones violate at least one pivot. Leveraging
this property, we propose a self-training pipeline that (i) samples diverse
reasoning paths and mines shared decision pivots, (ii) compresses each trace
into pivot-focused short-path reasoning using an auxiliary verifier, and (iii)
post-trains the model using its self-generated outputs. The proposed method
aligns reasoning without ground truth reasoning data or external metrics.
Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the
effectiveness of our method.

</details>


### [112] [AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need](https://arxiv.org/abs/2509.21553)
*Ahmed Jaber,Wangshu Zhu,Karthick Jayavelu,Justin Downes,Sameer Mohamed,Candace Agonafir,Linnia Hawkins,Tian Zheng*

Main category: cs.AI

TL;DR: 提出了一种结合知识图谱和AI代理的云原生科学工作流系统，旨在降低气候数据科学的技术门槛，实现非专业用户对相关数据集的识别和分析。


<details>
  <summary>Details</summary>
Motivation: 气候数据科学面临数据源分散、格式异构和技术门槛高等挑战，限制了参与度、减缓了发现速度并降低了科学工作流的可重复性。

Method: 通过整合精心策划的知识图谱与AI代理，知识图谱提供统一的数据集、工具和工作流组织层，AI代理基于生成式AI服务实现自然语言交互、自动化数据访问和简化分析。

Result: 该系统显著降低了参与气候数据科学的技术门槛，使非专业用户能够识别和分析相关数据集，展示了可扩展和自主的科学工作流。

Conclusion: 该系统为民主化气候数据访问和建立可重复、可扩展的人类-AI协作科研框架开辟了道路。

Abstract: Climate data science faces persistent barriers stemming from the fragmented
nature of data sources, heterogeneous formats, and the steep technical
expertise required to identify, acquire, and process datasets. These challenges
limit participation, slow discovery, and reduce the reproducibility of
scientific workflows. In this paper, we present a proof of concept for
addressing these barriers through the integration of a curated knowledge graph
(KG) with AI agents designed for cloud-native scientific workflows. The KG
provides a unifying layer that organizes datasets, tools, and workflows, while
AI agents -- powered by generative AI services -- enable natural language
interaction, automated data access, and streamlined analysis. Together, these
components drastically lower the technical threshold for engaging in climate
data science, enabling non-specialist users to identify and analyze relevant
datasets. By leveraging existing cloud-ready API data portals, we demonstrate
that "a knowledge graph is all you need" to unlock scalable and agentic
workflows for scientific inquiry. The open-source design of our system further
supports community contributions, ensuring that the KG and associated tools can
evolve as a shared commons. Our results illustrate a pathway toward
democratizing access to climate data and establishing a reproducible,
extensible framework for human--AI collaboration in scientific research.

</details>


### [113] [EEG-Based Consumer Behaviour Prediction: An Exploration from Classical Machine Learning to Graph Neural Networks](https://arxiv.org/abs/2509.21567)
*Mohammad Parsa Afshar,Aryan Azimi*

Main category: cs.AI

TL;DR: 使用EEG数据和机器学习模型预测消费者行为，比较了传统机器学习模型和图神经网络模型的表现。


<details>
  <summary>Details</summary>
Motivation: 预测消费者行为在营销、认知神经科学和人机交互中很重要，EEG数据可以提供大脑神经活动的详细信息来分析决策过程。

Method: 从NeuMa数据集中提取和清理EEG特征，为GNN模型创建大脑连接特征，使用多种机器学习模型（包括传统模型和GNN）进行比较。

Result: 整体结果没有显著差异，但GNN模型在某些基本标准上表现更好，特别是在传统模型表现不佳的情况下。

Conclusion: EEG信号分析与机器学习模型结合可以更深入理解消费者行为，并提供了传统模型（如SVM）与新兴模型（如GNN）在基于EEG的神经营销中的全面比较。

Abstract: Prediction of consumer behavior is one of the important purposes in
marketing, cognitive neuroscience, and human-computer interaction. The
electroencephalography (EEG) data can help analyze the decision process by
providing detailed information about the brain's neural activity. In this
research, a comparative approach is utilized for predicting consumer behavior
by EEG data. In the first step, the features of the EEG data from the NeuMa
dataset were extracted and cleaned. For the Graph Neural Network (GNN) models,
the brain connectivity features were created. Different machine learning
models, such as classical models and Graph Neural Networks, are used and
compared. The GNN models with different architectures are implemented to have a
comprehensive comparison; furthermore, a wide range of classical models, such
as ensemble models, are applied, which can be very helpful to show the
difference and performance of each model on the dataset. Although the results
did not show a significant difference overall, the GNN models generally
performed better in some basic criteria where classical models were not
satisfactory. This study not only shows that combining EEG signal analysis and
machine learning models can provide an approach to deeper understanding of
consumer behavior, but also provides a comprehensive comparison between the
machine learning models that have been widely used in previous studies in the
EEG-based neuromarketing such as Support Vector Machine (SVM), and the models
which are not used or rarely used in the field, like Graph Neural Networks.

</details>


### [114] [GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models](https://arxiv.org/abs/2509.21593)
*Peng Luo,Xiayin Lou,Yu Zheng,Zhuo Zheng,Stefano Ermon*

Main category: cs.AI

TL;DR: GeoEvolve是一个多智能体LLM框架，通过结合进化搜索和地理空间领域知识来自动设计和优化地理空间算法，在空间插值和不确定性量化任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的算法发现框架缺乏地理空间领域知识和多步推理能力，难以解决复杂的地理空间问题。

Method: 采用双循环结构：内循环使用代码进化器生成和变异候选解，外循环通过智能控制器评估全局精英，并查询GeoKnowRAG模块注入地理学理论先验知识。

Result: 在空间插值任务中减少RMSE误差13-21%，在不确定性估计中提升性能17%。消融研究证实领域知识引导的检索对稳定高质量进化至关重要。

Conclusion: GeoEvolve为自动化、知识驱动的地理空间建模提供了可扩展路径，为可信赖和高效的AI-for-Science发现开辟了新机遇。

Abstract: Geospatial modeling provides critical solutions for pressing global
challenges such as sustainability and climate change. Existing large language
model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at
evolving generic code but lack the domain knowledge and multi-step reasoning
required for complex geospatial problems. We introduce GeoEvolve, a multi-agent
LLM framework that couples evolutionary search with geospatial domain knowledge
to automatically design and refine geospatial algorithms. GeoEvolve operates in
two nested loops: an inner loop leverages a code evolver to generate and mutate
candidate solutions, while an outer agentic controller evaluates global elites
and queries a GeoKnowRAG module -- a structured geospatial knowledge base that
injects theoretical priors from geography. This knowledge-guided evolution
steers the search toward theoretically meaningful and computationally efficient
algorithms. We evaluate GeoEvolve on two fundamental and classical tasks:
spatial interpolation (kriging) and spatial uncertainty quantification
(geospatial conformal prediction). Across these benchmarks, GeoEvolve
automatically improves and discovers new algorithms, incorporating geospatial
theory on top of classical models. It reduces spatial interpolation error
(RMSE) by 13-21% and enhances uncertainty estimation performance by 17\%.
Ablation studies confirm that domain-guided retrieval is essential for stable,
high-quality evolution. These results demonstrate that GeoEvolve provides a
scalable path toward automated, knowledge-driven geospatial modeling, opening
new opportunities for trustworthy and efficient AI-for-Science discovery.

</details>


### [115] [Automated and Interpretable Survival Analysis from Multimodal Data](https://arxiv.org/abs/2509.21600)
*Mafalda Malafaia,Peter A. N. Bosman,Coen Rasch,Tanja Alderliesten*

Main category: cs.AI

TL;DR: 提出了一个可解释的多模态AI框架MultiFIX，整合临床变量和CT影像进行生存分析，在头颈癌数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着多模态数据的增长和临床对透明模型的需求增加，需要开发准确且可解释的生存分析方法来支持验证和信任。

Method: 使用深度学习推断生存相关特征，通过Grad-CAM解释影像特征，通过遗传编程建模临床变量为符号表达式，采用透明的Cox回归进行风险估计和分层。

Result: 在头颈癌RADCURE数据集上，MultiFIX的C指数达到0.838（预测）和0.826（分层），优于临床和学术基线方法。

Conclusion: MultiFIX展示了可解释多模态AI在精准肿瘤学中的潜力，能够实现透明且准确的生存分析。

Abstract: Accurate and interpretable survival analysis remains a core challenge in
oncology. With growing multimodal data and the clinical need for transparent
models to support validation and trust, this challenge increases in complexity.
We propose an interpretable multimodal AI framework to automate survival
analysis by integrating clinical variables and computed tomography imaging. Our
MultiFIX-based framework uses deep learning to infer survival-relevant features
that are further explained: imaging features are interpreted via Grad-CAM,
while clinical variables are modeled as symbolic expressions through genetic
programming. Risk estimation employs a transparent Cox regression, enabling
stratification into groups with distinct survival outcomes. Using the
open-source RADCURE dataset for head and neck cancer, MultiFIX achieves a
C-index of 0.838 (prediction) and 0.826 (stratification), outperforming the
clinical and academic baseline approaches and aligning with known prognostic
markers. These results highlight the promise of interpretable multimodal AI for
precision oncology with MultiFIX.

</details>


### [116] [Semantic F1 Scores: Fair Evaluation Under Fuzzy Class Boundaries](https://arxiv.org/abs/2509.21633)
*Georgios Chochlakis,Jackson Trager,Vedant Jhaveri,Nikhil Ravichandran,Alexandros Potamianos,Shrikanth Narayanan*

Main category: cs.AI

TL;DR: 提出了语义F1分数，这是一种用于主观或多标签分类的新评估指标，通过量化预测标签与真实标签之间的语义相关性来改进传统F1指标。


<details>
  <summary>Details</summary>
Motivation: 传统F1指标将语义相关的预测视为完全失败，无法反映类别边界模糊或人类标注者存在分歧的现实情况。需要一种能够给予语义相关但不完全相同标签部分分数的评估方法。

Method: 使用标签相似度矩阵计算软精度和软召回分数，通过新颖的两步精度-召回公式推导出语义F1分数，能够处理任意大小的标签集而不丢弃标签或强制匹配不相似的标签。

Result: 通过理论论证和在合成及真实数据上的广泛实证验证，语义F1分数显示出更好的可解释性和生态效度。该方法对相似度矩阵的误设具有鲁棒性。

Conclusion: 语义F1分数提供了更公平的评估，认识到类别重叠、标注者分歧以及基于相似预测的下游决策会产生相似结果的事实，适用于跨任务和模态的应用。

Abstract: We propose Semantic F1 Scores, novel evaluation metrics for subjective or
fuzzy multi-label classification that quantify semantic relatedness between
predicted and gold labels. Unlike the conventional F1 metrics that treat
semantically related predictions as complete failures, Semantic F1 incorporates
a label similarity matrix to compute soft precision-like and recall-like
scores, from which the Semantic F1 scores are derived. Unlike existing
similarity-based metrics, our novel two-step precision-recall formulation
enables the comparison of label sets of arbitrary sizes without discarding
labels or forcing matches between dissimilar labels. By granting partial credit
for semantically related but nonidentical labels, Semantic F1 better reflects
the realities of domains marked by human disagreement or fuzzy category
boundaries. In this way, it provides fairer evaluations: it recognizes that
categories overlap, that annotators disagree, and that downstream decisions
based on similar predictions lead to similar outcomes. Through theoretical
justification and extensive empirical validation on synthetic and real data, we
show that Semantic F1 demonstrates greater interpretability and ecological
validity. Because it requires only a domain-appropriate similarity matrix,
which is robust to misspecification, and not a rigid ontology, it is applicable
across tasks and modalities.

</details>


### [117] [Can AI Perceive Physical Danger and Intervene?](https://arxiv.org/abs/2509.21651)
*Abhishek Jindal,Dmitry Kalashnikov,Oscar Chang,Divya Garikapati,Anirudha Majumdar,Pierre Sermanet,Vikas Sindhwani*

Main category: cs.AI

TL;DR: 该论文提出了一个可扩展的物理安全基准测试方法，用于评估具身AI系统对物理安全的理解能力，并开发了后训练范式来提升模型的安全推理能力。


<details>
  <summary>Details</summary>
Motivation: 当AI与物理世界交互时，存在直接的物理伤害风险，需要评估现有基础模型对物理安全常识的理解程度。

Method: 基于真实世界伤害叙事构建逼真图像和视频，分析主流基础模型的风险感知能力，并开发后训练范式来教授模型显式推理具身安全约束。

Result: 通过基准测试发现模型在安全关键应用中的部署准备度存在多方面问题，后训练方法在约束满足评估中达到了最先进性能。

Conclusion: 该研究为具身AI系统的物理安全评估提供了有效工具，并展示了通过显式安全推理提升模型安全性的可行性。

Abstract: When AI interacts with the physical world -- as a robot or an assistive agent
-- new safety challenges emerge beyond those of purely ``digital AI". In such
interactions, the potential for physical harm is direct and immediate. How well
do state-of-the-art foundation models understand common-sense facts about
physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of
coffee should not be handed to a child? In this paper, our contributions are
three-fold: first, we develop a highly scalable approach to continuous physical
safety benchmarking of Embodied AI systems, grounded in real-world injury
narratives and operational safety constraints. To probe multi-modal safety
understanding, we turn these narratives and constraints into photorealistic
images and videos capturing transitions from safe to unsafe states, using
advanced generative models. Secondly, we comprehensively analyze the ability of
major foundation models to perceive risks, reason about safety, and trigger
interventions; this yields multi-faceted insights into their deployment
readiness for safety-critical agentic applications. Finally, we develop a
post-training paradigm to teach models to explicitly reason about
embodiment-specific safety constraints provided through system instructions.
The resulting models generate thinking traces that make safety reasoning
interpretable and transparent, achieving state of the art performance in
constraint satisfaction evaluations. The benchmark will be released at
https://asimov-benchmark.github.io/v2

</details>


### [118] [Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization](https://arxiv.org/abs/2509.21718)
*Shehzeen Hussain,Paarth Neekhara,Xuesong Yang,Edresson Casanova,Subhankar Ghosh,Roy Fejgin,Ryan Langman,Mikyas Desta,Leili Tavabi,Jason Li*

Main category: cs.AI

TL;DR: 提出基于GRPO的框架，利用预训练的ASR模型来优化低资源语言的TTS系统，仅需少量配对数据和大量非配对文本即可生成高质量的语音。


<details>
  <summary>Details</summary>
Motivation: 低资源语言缺乏配对的文本-语音数据，但ASR模型相对容易获得，因此利用ASR来指导TTS模型训练。

Method: 首先用IPA训练多语言TTS基础模型，然后用少量配对数据微调目标语言的韵律特征，最后用GRPO和ASR、说话人验证、音频质量评估模型的多目标奖励进行优化。

Result: 在低资源语言上生成可理解且说话人一致的语音，性能显著优于仅微调的方法，在高资源语言上也优于DPO等离线对齐方法。

Conclusion: GRPO框架能有效利用ASR等预训练模型提升低资源语言的TTS性能，且在高资源语言上也有优势。

Abstract: Developing high-quality text-to-speech (TTS) systems for low-resource
languages is challenging due to the scarcity of paired text and speech data. In
contrast, automatic speech recognition (ASR) models for such languages are
often more accessible, owing to large-scale multilingual pre-training efforts.
We propose a framework based on Group Relative Policy Optimization (GRPO) to
adapt an autoregressive, multilingual TTS model to new languages. Our method
first establishes a language-agnostic foundation for TTS synthesis by training
a multilingual baseline with International Phonetic Alphabet (IPA) tokens.
Next, we fine-tune this model on limited paired data of the new languages to
capture the target language's prosodic features. Finally, we apply GRPO to
optimize the model using only unpaired text and speaker prompts, guided by a
multi-objective reward from pretrained ASR, speaker verification, and audio
quality estimation models. Experiments demonstrate that this pipeline produces
intelligible and speaker-consistent speech in low-resource languages,
substantially outperforming fine-tuning alone. Furthermore, our GRPO-based
framework also improves TTS performance in high-resource languages, surpassing
offline alignment methods such as Direct Preference Optimization (DPO) yielding
superior intelligibility, speaker similarity, and audio quality.

</details>


### [119] [Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts](https://arxiv.org/abs/2509.21743)
*Ammar Ahmed,Azal Ahmad Khan,Ayaan Ahmad,Sheng Di,Zirui Liu,Ali Anwar*

Main category: cs.AI

TL;DR: RoT通过检索和重用先前的推理步骤作为可组合的"思维"步骤来指导新问题，显著减少推理延迟和成本，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过生成长推理轨迹来提高准确性，但这会增加延迟和成本，因此需要在推理时提高效率。

Method: 提出检索思维（RoT）方法，将推理步骤组织成具有顺序和语义边的思维图，通过检索查询相关节点和应用奖励引导的遍历来组装问题特定模板。

Result: RoT在多个模型上的推理基准测试中，输出标记减少高达40%，推理延迟降低82%，成本降低59%，同时保持准确性。

Conclusion: RoT通过检索动态构建模板，为高效大型推理模型推理建立了可扩展的范式。

Abstract: Large reasoning models improve accuracy by producing long reasoning traces,
but this inflates latency and cost, motivating inference-time efficiency. We
propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable
``thought" steps to guide new problems. RoT organizes steps into a thought
graph with sequential and semantic edges to enable fast retrieval and flexible
recombination. At inference, RoT retrieves query-relevant nodes and applies
reward-guided traversal to assemble a problem-specific template that guides
generation. This dynamic template reuse reduces redundant exploration and,
therefore, reduces output tokens while preserving accuracy. We evaluate RoT on
reasoning benchmarks with multiple models, measuring accuracy, token usage,
latency, and memory overhead. Findings show small prompt growth but substantial
efficiency gains, with RoT reducing output tokens by up to 40%, inference
latency by 82%, and cost by 59% while maintaining accuracy. RoT establishes a
scalable paradigm for efficient LRM reasoning via dynamic template construction
through retrieval.

</details>


### [120] [Lifelong Learning with Behavior Consolidation for Vehicle Routing](https://arxiv.org/abs/2509.21765)
*Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang,Xin Yao*

Main category: cs.AI

TL;DR: 提出了一个终身学习框架LLR-BC，用于神经VRP求解器在多任务环境下的持续学习，通过行为整合方法解决灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经求解器在遇到新任务时，要么依赖零样本泛化（可能因任务差异而效果不佳），要么通过微调导致灾难性遗忘。需要一种能够持续学习新任务同时保持旧任务性能的方法。

Method: 提出了LLR-BC框架，通过决策导向的行为整合方法，将新任务训练的解算器行为与缓冲的先前知识进行对齐，并为低置信度决策分配更大的整合权重。

Result: 在容量约束车辆路径问题和旅行商问题上的广泛实验表明，LLR-BC在终身学习设置下能够训练高性能神经求解器，有效解决灾难性遗忘问题，保持模型可塑性并提升零样本泛化能力。

Conclusion: LLR-BC为神经VRP求解器提供了一种有效的终身学习解决方案，能够在多任务环境中持续学习而不遗忘先前知识。

Abstract: Recent neural solvers have demonstrated promising performance in learning to
solve routing problems. However, existing studies are primarily based on
one-off training on one or a set of predefined problem distributions and
scales, i.e., tasks. When a new task arises, they typically rely on either
zero-shot generalization, which may be poor due to the discrepancies between
the new task and the training task(s), or fine-tuning the pretrained solver on
the new task, which possibly leads to catastrophic forgetting of knowledge
acquired from previous tasks. This paper explores a novel lifelong learning
paradigm for neural VRP solvers, where multiple tasks with diverse
distributions and scales arise sequentially over time. Solvers are required to
effectively and efficiently learn to solve new tasks while maintaining their
performance on previously learned tasks. Consequently, a novel framework called
Lifelong Learning Router with Behavior Consolidation (LLR-BC) is proposed.
LLR-BC consolidates prior knowledge effectively by aligning behaviors of the
solver trained on a new task with the buffered ones in a decision-seeking way.
To encourage more focus on crucial experiences, LLR-BC assigns greater
consolidated weights to decisions with lower confidence. Extensive experiments
on capacitated vehicle routing problems and traveling salesman problems
demonstrate LLR-BC's effectiveness in training high-performance neural solvers
in a lifelong learning setting, addressing the catastrophic forgetting issue,
maintaining their plasticity, and improving zero-shot generalization ability.

</details>


### [121] [UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios](https://arxiv.org/abs/2509.21766)
*Haotian Luo,Huaisong Zhang,Xuelin Zhang,Haoyu Wang,Zeyu Qin,Wenjie Lu,Guozheng Ma,Haiying He,Yingsha Xie,Qiyang Zhou,Zixuan Hu,Hongze Mi,Yibo Wang,Naiqiang Tan,Hong Chen,Yi R. Fung,Chun Yuan,Li Shen*

Main category: cs.AI

TL;DR: 提出了UltraHorizon基准测试，用于评估智能体在长时域、部分可观测场景下的持续推理、规划、记忆管理和工具使用等核心能力，发现现有LLM智能体在这些复杂任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注短时域、完全可观测任务，而现实世界中的关键任务（如软件开发、投资决策、科学发现）往往涉及长时域和部分可观测场景，需要持续的核心能力支持。

Method: 通过探索作为统一任务，在三个不同环境中设计长时域发现任务，智能体需要通过持续推理、规划、记忆和工具管理来迭代发现隐藏规则。

Result: 在最重规模设置下，轨迹平均超过20万token和400+工具调用，标准配置下也超过35k token和60+工具调用。实验显示LLM智能体在这些场景下表现不佳，而人类参与者得分更高。

Conclusion: 智能体在长时域任务中存在显著能力差距，简单扩展方法无效，主要失败原因包括上下文锁定和功能性基础能力差距。

Abstract: Autonomous agents have recently achieved remarkable progress across diverse
domains, yet most evaluations focus on short-horizon, fully observable tasks.
In contrast, many critical real-world tasks, such as large-scale software
development, commercial investment, and scientific discovery, unfold in
long-horizon and partially observable scenarios where success hinges on
sustained reasoning, planning, memory management, and tool use. Existing
benchmarks rarely capture these long-horizon challenges, leaving a gap in
systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a
novel benchmark that measures the foundational capabilities essential for
complex real-world challenges. We use exploration as a unifying task across
three distinct environments to validate these core competencies. Agents are
designed in long-horizon discovery tasks where they must iteratively uncover
hidden rules through sustained reasoning, planning, memory and tools
management, and interaction with environments. Under the heaviest scale
setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool
calls, whereas in standard configurations they still exceed \textbf{35k} tokens
and involve more than \textbf{60} tool calls on average. Our extensive
experiments reveal that LLM-agents consistently underperform in these settings,
whereas human participants achieve higher scores, underscoring a persistent gap
in agents' long-horizon abilities. We also observe that simple scaling fails in
our task. To better illustrate the failure of agents, we conduct an in-depth
analysis of collected trajectories. We identify eight types of errors and
attribute them to two primary causes: in-context locking and functional
fundamental capability gaps.
\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available
here.}

</details>


### [122] [Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety](https://arxiv.org/abs/2509.21782)
*Junliang Liu,Jingyu Xiao,Wenxin Tang,Wenxuan Wang,Zhixian Wang,Minrui Zhang,Shuanghe Yu*

Main category: cs.AI

TL;DR: 提出了WebRSSBench基准，用于全面评估多模态大语言模型在网页理解中的推理、鲁棒性和安全性能力，包含8个任务和3799个问答对。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注视觉感知或UI代码生成，缺乏对端到端网页应用所需的推理、鲁棒性和安全性能力的评估。

Method: 从729个网站构建包含3799个问答对的基准，涵盖位置关系推理、颜色鲁棒性、安全关键检测等8个任务，采用标准化提示、确定性评估脚本和多阶段质量控制。

Result: 评估12个MLLM发现显著差距：模型在真实布局的组合和跨元素推理上表现不佳，面对UI和内容扰动时鲁棒性有限，在识别和避免安全关键操作方面过于保守。

Conclusion: WebRSSBench揭示了当前MLLM在网页理解能力上的不足，为开发更可靠的AI网页协作系统提供了重要基准。

Abstract: Multimodal large language models (MLLMs) are increasingly positioned as AI
collaborators for building complex web-related applications like GUI agents and
front-end code generation. However, existing benchmarks largely emphasize
visual perception or UI code generation, showing insufficient evaluation on the
reasoning, robustness and safety capability required for end-to-end web
applications. To bridge the gap, we introduce a comprehensive web understanding
benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and
Safety across eight tasks, such as position relationship reasoning, color
robustness, and safety critical detection, etc. The benchmark is constructed
from 729 websites and contains 3799 question answer pairs that probe multi-step
inference over page structure, text, widgets, and safety-critical interactions.
To ensure reliable measurement, we adopt standardized prompts, deterministic
evaluation scripts, and multi-stage quality control combining automatic checks
with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The
results reveal significant gaps, models still struggle with compositional and
cross-element reasoning over realistic layouts, show limited robustness when
facing perturbations in user interfaces and content such as layout
rearrangements or visual style shifts, and are rather conservative in
recognizing and avoiding safety critical or irreversible actions. Our code is
available at https://github.com/jinliang-byte/webssrbench.

</details>


### [123] [D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents](https://arxiv.org/abs/2509.21799)
*Hongze Mi,Yibo Feng,Wenjie Lu,Yuqi Wang,Jinyuan Li,Song Cao,He Cui,Tengfei Tian,Xuelin Zhang,Haotian Luo,Di Sun,Naiqiang Tan,Gang Pan*

Main category: cs.AI

TL;DR: D-Artemis是一个基于人类认知循环（思考、对齐、反思）的GUI代理框架，通过应用特定提示检索、预执行对齐和状态反思机制，显著提升了多模态大语言模型在GUI任务中的性能，无需复杂轨迹数据集训练即可实现强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理方法面临数据瓶颈、延迟错误检测成本高和矛盾指导风险等关键挑战，需要更高效的认知框架来模拟人类交互过程。

Method: 提出D-Artemis框架，包含：1）应用特定提示检索机制；2）预执行对齐阶段，包含思想-行动一致性检查和行动校正代理；3）后执行状态反思代理完成认知循环。

Result: 在AndroidWorld上达到75.8%的成功率，在ScreenSpot-V2上达到96.8%的成功率，创下新的SOTA结果。消融研究证实各组件对框架性能均有显著贡献。

Conclusion: D-Artemis通过模拟人类认知循环，有效提升了通用多模态大语言模型在GUI任务中的能力，无需复杂数据集训练即可实现优异性能，为GUI代理发展提供了新思路。

Abstract: Graphical User Interface (GUI) agents aim to automate a wide spectrum of
human tasks by emulating user interaction. Despite rapid advancements, current
approaches are hindered by several critical challenges: data bottleneck in
end-to-end training, high cost of delayed error detection, and risk of
contradictory guidance. Inspired by the human cognitive loop of Thinking,
Alignment, and Reflection, we present D-Artemis -- a novel deliberative
framework in this paper. D-Artemis leverages a fine-grained, app-specific tip
retrieval mechanism to inform its decision-making process. It also employs a
proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC)
Check module and Action Correction Agent (ACA) work in concert to mitigate the
risk of execution failures. A post-execution Status Reflection Agent (SRA)
completes the cognitive loop, enabling strategic learning from experience.
Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal
large language models (MLLMs) for GUI tasks without the need for training on
complex trajectory datasets, demonstrating strong generalization. D-Artemis
establishes new state-of-the-art (SOTA) results across both major benchmarks,
achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.
Extensive ablation studies further demonstrate the significant contribution of
each component to the framework.

</details>


### [124] [ProRe: A Proactive Reward System for GUI Agents via Reasoner-Actor Collaboration](https://arxiv.org/abs/2509.21823)
*Gaole Dai,Shiqi Jiang,Ting Cao,Yuqing Yang,Yuanchun Li,Rui Tan,Mo Li,Lili Qiu*

Main category: cs.AI

TL;DR: ProRe是一个主动奖励系统，通过通用推理器和领域特定评估器主动与环境交互来收集额外观察，从而为GUI代理提供更准确可验证的奖励。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则或模型的奖励方法难以泛化到GUI代理场景，因为无法获取真实轨迹或应用数据库，而基于静态轨迹的LLM-as-a-Judge方法准确率有限。

Method: 使用通用推理器调度有针对性的状态探测任务，领域特定评估器执行这些任务，通过主动与环境交互收集额外观察，使推理器能够分配更准确可验证的奖励。

Result: 在超过3000条轨迹上的实验表明，ProRe将奖励准确率和F1分数分别提高了5.3%和19.4%，与最先进策略代理集成后成功率提高了22.4%。

Conclusion: ProRe通过主动环境交互显著提升了GUI代理的奖励评估准确性，为训练和评估大型语言模型提供了更有效的奖励机制。

Abstract: Reward is critical to the evaluation and training of large language models
(LLMs). However, existing rule-based or model-based reward methods struggle to
generalize to GUI agents, where access to ground-truth trajectories or
application databases is often unavailable, and static trajectory-based
LLM-as-a-Judge approaches suffer from limited accuracy. To address these
challenges, we propose ProRe, a proactive reward system that leverages a
general-purpose reasoner and domain-specific evaluator agents (actors). The
reasoner schedules targeted state probing tasks, which the evaluator agents
then execute by actively interacting with the environment to collect additional
observations. This enables the reasoner to assign more accurate and verifiable
rewards to GUI agents. Empirical results on over 3K trajectories demonstrate
that ProRe improves reward accuracy and F1 score by up to 5.3% and 19.4%,
respectively. Furthermore, integrating ProRe with state-of-the-art policy
agents yields a success rate improvement of up to 22.4%.

</details>


### [125] [DS-STAR: Data Science Agent via Iterative Planning and Verification](https://arxiv.org/abs/2509.21825)
*Jaehyun Nam,Jinsung Yoon,Jiefeng Chen,Jinwoo Shin,Tomas Pfister*

Main category: cs.AI

TL;DR: DS-STAR是一个新颖的数据科学智能体，通过自动探索异构数据格式、LLM验证分析计划充分性以及迭代式顺序规划机制，显著提升了复杂数据分析任务的自动化能力。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在处理异构数据格式时表现不佳，难以生成最优分析计划，且由于开放性问题缺乏真实标签，验证计划充分性具有挑战性。

Method: DS-STAR包含三个核心组件：数据文件分析模块自动探索和提取多种数据格式的上下文；LLM验证步骤评估各阶段分析计划的充分性；顺序规划机制从简单可执行计划开始，基于反馈迭代优化直至验证通过。

Result: 在DABStep、KramaBench和DA-Code三个基准测试中达到最先进性能，特别是在需要处理多个异构数据文件的困难任务上显著优于基线方法。

Conclusion: DS-STAR通过其创新的架构设计，能够可靠地导航涉及多样化数据源的复杂分析，为自动化数据科学任务提供了有效的解决方案。

Abstract: Data science, which transforms raw data into actionable insights, is critical
for data-driven decision-making. However, these tasks are often complex,
involving steps for exploring multiple data sources and synthesizing findings
to deliver insightful answers. While large language models (LLMs) show
significant promise in automating this process, they often struggle with
heterogeneous data formats and generate sub-optimal analysis plans, as
verifying plan sufficiency is inherently difficult without ground-truth labels
for such open-ended tasks. To overcome these limitations, we introduce DS-STAR,
a novel data science agent. Specifically, DS-STAR makes three key
contributions: (1) a data file analysis module that automatically explores and
extracts context from diverse data formats, including unstructured types; (2) a
verification step where an LLM-based judge evaluates the sufficiency of the
analysis plan at each stage; and (3) a sequential planning mechanism that
starts with a simple, executable plan and iteratively refines it based on the
DS-STAR's feedback until its sufficiency is verified. This iterative refinement
allows DS-STAR to reliably navigate complex analyses involving diverse data
sources. Our experiments show that DS-STAR achieves state-of-the-art
performance across three challenging benchmarks: DABStep, KramaBench, and
DA-Code. Moreover, DS-STAR particularly outperforms baselines on hard tasks
that require processing multiple data files with heterogeneous formats.

</details>


### [126] [Axiomatic Choice and the Decision-Evaluation Paradox](https://arxiv.org/abs/2509.21836)
*Ben Abramowitz,Nicholas Mattei*

Main category: cs.AI

TL;DR: 提出了一个基于公理的决策建模框架，揭示了决策-评估悖论，并指出在训练决策模型和应用公理时需要格外谨慎。


<details>
  <summary>Details</summary>
Motivation: 研究如何用公理（如伦理约束）来建模决策，探索决策公理的结构特性及其在决策制定与评估中的潜在冲突。

Method: 建立了一个决策公理框架，基于结构特性对公理进行分类，并分析决策制定与评估之间的张力关系。

Result: 发现了决策-评估悖论，该悖论在现实的公理结构中普遍存在，揭示了决策制定与评估之间的内在矛盾。

Conclusion: 在训练决策模型和应用公理进行决策制定与评估时，必须格外谨慎，以避免决策-评估悖论带来的问题。

Abstract: We introduce a framework for modeling decisions with axioms that are
statements about decisions, e.g., ethical constraints. Using our framework we
define a taxonomy of decision axioms based on their structural properties and
demonstrate a tension between the use of axioms to make decisions and the use
of axioms to evaluate decisions which we call the Decision-Evaluation Paradox.
We argue that the Decision-Evaluation Paradox arises with realistic axiom
structures, and the paradox illuminates why one must be exceptionally careful
when training models on decision data or applying axioms to make and evaluate
decisions.

</details>


### [127] [DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents](https://arxiv.org/abs/2509.21842)
*Yansong Ning,Rui Liu,Jun Wang,Kai Chen,Wei Li,Jun Fang,Kan Zheng,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: DeepTravel是一个端到端的强化学习框架，用于构建自主旅行规划代理，能够自主规划、执行工具并反思工具响应，在多步推理中探索、验证和优化中间行动。


<details>
  <summary>Details</summary>
Motivation: 现有旅行规划代理依赖手工制作的提示和固定的代理工作流程，限制了代理的灵活性和自主性。

Method: 构建沙盒环境缓存交通、住宿和POI数据；开发分层奖励建模系统，包括轨迹级验证器和回合级验证器；提出回复增强的强化学习方法，从失败经验缓冲区定期重放。

Result: 在滴滴企业解决方案App上部署的DeepTravel使小型LLM（如Qwen3 32B）在旅行规划任务中显著优于OpenAI o1、o3和DeepSeek R1等前沿LLM。

Conclusion: DeepTravel框架成功构建了自主旅行规划代理，通过强化学习和分层奖励系统显著提升了代理性能。

Abstract: Travel planning (TP) agent has recently worked as an emerging building block
to interact with external tools and resources for travel itinerary generation,
ensuring enjoyable user experience. Despite its benefits, existing studies rely
on hand craft prompt and fixed agent workflow, hindering more flexible and
autonomous TP agent. This paper proposes DeepTravel, an end to end agentic
reinforcement learning framework for building autonomous travel planning agent,
capable of autonomously planning, executing tools, and reflecting on tool
responses to explore, verify, and refine intermediate actions in multi step
reasoning. To achieve this, we first construct a robust sandbox environment by
caching transportation, accommodation and POI data, facilitating TP agent
training without being constrained by real world APIs limitations (e.g.,
inconsistent outputs). Moreover, we develop a hierarchical reward modeling
system, where a trajectory level verifier first checks spatiotemporal
feasibility and filters unsatisfied travel itinerary, and then the turn level
verifier further validate itinerary detail consistency with tool responses,
enabling efficient and precise reward service. Finally, we propose the reply
augmented reinforcement learning method that enables TP agent to periodically
replay from a failures experience buffer, emerging notable agentic capacity. We
deploy trained TP agent on DiDi Enterprise Solutions App and conduct
comprehensive online and offline evaluations, demonstrating that DeepTravel
enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing
frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.

</details>


### [128] [Reimagining Agent-based Modeling with Large Language Model Agents via Shachi](https://arxiv.org/abs/2509.21862)
*So Kuroki,Yingtao Tian,Kou Misaki,Takashi Ikegami,Takuya Akiba,Yujin Tang*

Main category: cs.AI

TL;DR: Shachi是一个用于研究LLM驱动多智能体系统涌现行为的模块化框架，将智能体策略分解为配置、记忆和工具三个核心认知组件，通过系统化实验方法分析架构选择对集体行为的影响。


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体系统的研究缺乏受控实验的原则性方法，限制了涌现行为研究的进展。

Method: 提出Shachi框架，将智能体策略分解为配置（内在特征）、记忆（上下文持久性）和工具（扩展能力）三个组件，由LLM推理引擎协调。

Result: 在10个任务的基准测试中验证了方法的有效性，通过模拟美国关税冲击实验证明，只有当智能体认知架构正确配置记忆和工具时，其行为才能与真实市场反应一致。

Conclusion: 该工作为构建和评估LLM智能体提供了严谨的开源基础，旨在促进更具累积性和科学依据的研究。

Abstract: The study of emergent behaviors in large language model (LLM)-driven
multi-agent systems is a critical research challenge, yet progress is limited
by a lack of principled methodologies for controlled experimentation. To
address this, we introduce Shachi, a formal methodology and modular framework
that decomposes an agent's policy into core cognitive components: Configuration
for intrinsic traits, Memory for contextual persistence, and Tools for expanded
capabilities, all orchestrated by an LLM reasoning engine. This principled
architecture moves beyond brittle, ad-hoc agent designs and enables the
systematic analysis of how specific architectural choices influence collective
behavior. We validate our methodology on a comprehensive 10-task benchmark and
demonstrate its power through novel scientific inquiries. Critically, we
establish the external validity of our approach by modeling a real-world U.S.
tariff shock, showing that agent behaviors align with observed market reactions
only when their cognitive architecture is appropriately configured with memory
and tools. Our work provides a rigorous, open-source foundation for building
and evaluating LLM agents, aimed at fostering more cumulative and
scientifically grounded research.

</details>


### [129] [TRACE: Learning to Compute on Graphs](https://arxiv.org/abs/2509.21886)
*Ziyang Zheng,Jiaying Zhu,Jingyi Zhou,Qiang Xu*

Main category: cs.AI

TL;DR: TRACE是一个新的图表示学习范式，通过层次化Transformer架构和函数偏移学习目标，解决了传统方法在计算图建模中的架构不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 传统消息传递神经网络(MPNNs)和Transformer在图计算任务中存在架构不匹配，无法捕捉计算的位置感知和层次化特性。

Method: 使用层次化Transformer模拟计算流程，并引入函数偏移学习目标，将复杂全局函数预测分解为预测真实函数与局部近似之间的差异。

Result: 在电子电路等复杂计算图上，TRACE在全面基准测试中显著优于所有现有架构。

Conclusion: 架构对齐的主干网络和解耦学习目标构成了更稳健的图计算学习范式。

Abstract: Learning to compute, the ability to model the functional behavior of a
computational graph, is a fundamental challenge for graph representation
learning. Yet, the dominant paradigm is architecturally mismatched for this
task. This flawed assumption, central to mainstream message passing neural
networks (MPNNs) and their conventional Transformer-based counterparts,
prevents models from capturing the position-aware, hierarchical nature of
computation. To resolve this, we introduce \textbf{TRACE}, a new paradigm built
on an architecturally sound backbone and a principled learning objective.
First, TRACE employs a Hierarchical Transformer that mirrors the step-by-step
flow of computation, providing a faithful architectural backbone that replaces
the flawed permutation-invariant aggregation. Second, we introduce
\textbf{function shift learning}, a novel objective that decouples the learning
problem. Instead of predicting the complex global function directly, our model
is trained to predict only the \textit{function shift}, the discrepancy between
the true global function and a simple local approximation that assumes input
independence. We validate this paradigm on electronic circuits, one of the most
complex and economically critical classes of computational graphs. Across a
comprehensive suite of benchmarks, TRACE substantially outperforms all prior
architectures. These results demonstrate that our architecturally-aligned
backbone and decoupled learning objective form a more robust paradigm for the
fundamental challenge of learning to compute on graphs.

</details>


### [130] [GenesisGeo: Technical Report](https://arxiv.org/abs/2509.21896)
*Minfeng Zhu,Zi Wang,Sizhe Ji,Zhengtong Du,Junming Ke,Xiao Deng,Zanlang Yin,Xiuqi Huang,Heyu Wang,Wei Chen*

Main category: cs.AI

TL;DR: GenesisGeo是一个欧几里得几何自动定理证明器，通过120倍加速的符号推理引擎和神经符号方法，在IMO-AG-30基准测试中达到了IMO金牌水平。


<details>
  <summary>Details</summary>
Motivation: 开发高效的几何定理自动证明系统，解决大规模几何问题，特别是包含辅助构造的复杂问题。

Method: 结合定理匹配技术将符号推理引擎DDARN加速120倍，并用C++实现核心组件；基于Qwen3-0.6B-Base构建神经符号证明器GenesisGeo。

Result: 开源了2180万几何问题数据集，其中300多万包含辅助构造；单模型在IMO-AG-30基准中解决24/30问题（IMO银牌水平），双模型集成达到26/30问题（IMO金牌水平）。

Conclusion: GenesisGeo通过神经符号方法和优化的符号推理引擎，在几何定理证明领域达到了最先进的性能水平。

Abstract: We present GenesisGeo, an automated theorem prover in Euclidean geometry. We
have open-sourced a large-scale geometry dataset of 21.8 million geometric
problems, over 3 million of which contain auxiliary constructions. Specially,
we significantly accelerate the symbolic deduction engine DDARN by 120x through
theorem matching, combined with a C++ implementation of its core components.
Furthermore, we build our neuro-symbolic prover, GenesisGeo, upon
Qwen3-0.6B-Base, which solves 24 of 30 problems (IMO silver medal level) in the
IMO-AG-30 benchmark using a single model, and achieves 26 problems (IMO gold
medal level) with a dual-model ensemble.

</details>


### [131] [DyRo-MCTS: A Robust Monte Carlo Tree Search Approach to Dynamic Job Shop Scheduling](https://arxiv.org/abs/2509.21902)
*Ruiqi Chen,Yi Mei,Fangfang Zhang,Mengjie Zhang*

Main category: cs.AI

TL;DR: 提出了DyRo-MCTS方法，将动作鲁棒性估计集成到MCTS中，用于动态作业车间调度问题，在保证调度性能的同时提高对新作业到达的适应性。


<details>
  <summary>Details</summary>
Motivation: 动态作业车间调度面临新作业频繁到达的干扰，现有离线学习策略不完善，而在线规划又因问题信息不完整容易受到扰动影响。

Method: DyRo-MCTS方法在MCTS中集成动作鲁棒性估计，引导生产环境朝向既能有良好调度结果又易于适应未来作业到达的状态。

Result: 实验表明DyRo-MCTS显著提升离线学习策略性能，且在线规划时间增加可忽略；在各种调度场景下始终优于传统MCTS。

Conclusion: DyRo-MCTS通过做出鲁棒调度决策，在扰动下实现长期可持续的性能提升。

Abstract: Dynamic job shop scheduling, a fundamental combinatorial optimisation problem
in various industrial sectors, poses substantial challenges for effective
scheduling due to frequent disruptions caused by the arrival of new jobs.
State-of-the-art methods employ machine learning to learn scheduling policies
offline, enabling rapid responses to dynamic events. However, these offline
policies are often imperfect, necessitating the use of planning techniques such
as Monte Carlo Tree Search (MCTS) to improve performance at online decision
time. The unpredictability of new job arrivals complicates online planning, as
decisions based on incomplete problem information are vulnerable to
disturbances. To address this issue, we propose the Dynamic Robust MCTS
(DyRo-MCTS) approach, which integrates action robustness estimation into MCTS.
DyRo-MCTS guides the production environment toward states that not only yield
good scheduling outcomes but are also easily adaptable to future job arrivals.
Extensive experiments show that DyRo-MCTS significantly improves the
performance of offline-learned policies with negligible additional online
planning time. Moreover, DyRo-MCTS consistently outperforms vanilla MCTS across
various scheduling scenarios. Further analysis reveals that its ability to make
robust scheduling decisions leads to long-term, sustainable performance gains
under disturbances.

</details>


### [132] [Outlier Detection in Plantar Pressure: Human-Centered Comparison of Statistical Parametric Mapping and Explainable Machine Learning](https://arxiv.org/abs/2509.21943)
*Carlo Dindorf,Jonas Dully,Steven Simon,Dennis Perchthaler,Stephan Becker,Hannah Ehmann,Kjell Heitmann,Bernd Stetter,Christian Diers,Michael Fröhlich*

Main category: cs.AI

TL;DR: 本研究比较了统计参数映射(SPM)和可解释机器学习方法在足底压力数据异常检测中的表现，发现ML模型在准确性上优于SPM，且两种方法的解释性都被专家认为是清晰有用的。


<details>
  <summary>Details</summary>
Motivation: 足底压力映射在临床诊断和运动科学中很重要，但大型异构数据集常包含技术错误或程序不一致导致的异常值。SPM提供可解释分析但对对齐敏感，其异常检测能力尚不明确。

Method: 比较了两种方法：(1)非参数、依赖配准的SPM方法；(2)使用SHAP解释的卷积神经网络(CNN)。通过嵌套交叉验证评估性能，通过语义差异调查评估解释质量。

Result: ML模型达到高准确率并优于SPM，SPM误分类了临床有意义的变异并遗漏了真实异常值。专家认为SPM和SHAP解释都清晰、有用且可信，但SPM被认为复杂度较低。

Conclusion: SPM和可解释ML在足底压力数据自动异常检测中具有互补潜力，可解释性在将复杂模型输出转化为可解释见解以有效支持决策方面至关重要。

Abstract: Plantar pressure mapping is essential in clinical diagnostics and sports
science, yet large heterogeneous datasets often contain outliers from technical
errors or procedural inconsistencies. Statistical Parametric Mapping (SPM)
provides interpretable analyses but is sensitive to alignment and its capacity
for robust outlier detection remains unclear. This study compares an SPM
approach with an explainable machine learning (ML) approach to establish
transparent quality-control pipelines for plantar pressure datasets. Data from
multiple centers were annotated by expert consensus and enriched with synthetic
anomalies resulting in 798 valid samples and 2000 outliers. We evaluated (i) a
non-parametric, registration-dependent SPM approach and (ii) a convolutional
neural network (CNN), explained using SHapley Additive exPlanations (SHAP).
Performance was assessed via nested cross-validation; explanation quality via a
semantic differential survey with domain experts. The ML model reached high
accuracy and outperformed SPM, which misclassified clinically meaningful
variations and missed true outliers. Experts perceived both SPM and SHAP
explanations as clear, useful, and trustworthy, though SPM was assessed less
complex. These findings highlight the complementary potential of SPM and
explainable ML as approaches for automated outlier detection in plantar
pressure data, and underscore the importance of explainability in translating
complex model outputs into interpretable insights that can effectively inform
decision-making.

</details>


### [133] [CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration](https://arxiv.org/abs/2509.21981)
*Zhimin Wang,Shaokang He,Duo Wu,Jinghe Wang,Linjia Kang,Jing Yu,Zhi Wang*

Main category: cs.AI

TL;DR: CoBel-World是一个为LLM智能体设计的协作信念世界框架，通过建模物理环境和协作伙伴的心理状态，实现了零样本贝叶斯式信念更新，显著减少了通信成本并提高了任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM协作框架忽视了动态意图推理能力，导致计划不一致和冗余通信，降低了协作效率。需要让LLM智能体能够推理协作伙伴的意图以避免协调失误。

Method: 提出CoBel-World框架，使用符号信念语言将开放世界任务知识解析为结构化信念，通过LLM推理进行零样本贝叶斯式信念更新，主动检测潜在协调冲突并自适应通信。

Result: 在TDW-MAT和C-WAH基准测试中，相比最强基线，通信成本减少22-60%，任务完成效率提高4-28%。

Conclusion: 显式的、意图感知的信念建模对于基于LLM的多智能体系统中实现高效、类人协作至关重要。

Abstract: Effective real-world multi-agent collaboration requires not only accurate
planning but also the ability to reason about collaborators' intents -- a
crucial capability for avoiding miscoordination and redundant communication
under partial observable environments. Due to their strong planning and
reasoning capabilities, large language models (LLMs) have emerged as promising
autonomous agents for collaborative task solving. However, existing
collaboration frameworks for LLMs overlook their reasoning potential for
dynamic intent inference, and thus produce inconsistent plans and redundant
communication, reducing collaboration efficiency. To bridge this gap, we
propose CoBel-World, a novel framework that equips LLM agents with a
collaborative belief world -- an internal representation jointly modeling the
physical environment and collaborators' mental states. CoBel-World enables
agents to parse open-world task knowledge into structured beliefs via a
symbolic belief language, and perform zero-shot Bayesian-style belief updates
through LLM reasoning. This allows agents to proactively detect potential
miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated
on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World
significantly reduces communication costs by 22-60% and improves task
completion efficiency by 4-28% compared to the strongest baseline. Our results
show that explicit, intent-aware belief modeling is essential for efficient and
human-like collaboration in LLM-based multi-agent systems.

</details>


### [134] [RISK: A Framework for GUI Agents in E-commerce Risk Management](https://arxiv.org/abs/2509.21982)
*Renqi Chen,Zeyin Tao,Jianming Guo,Jingzhe Zhu,Yiheng Peng,Qingqing Sun,Tianyi Zhang,Shuai Chen*

Main category: cs.AI

TL;DR: RISK框架为电商风险管理构建GUI代理，包含数据集、基准测试和强化微调框架，在单步和多步任务上分别提升6.8%和8.8%，在线评估任务成功率70.5%。


<details>
  <summary>Details</summary>
Motivation: 传统爬虫方法和现有GUI代理无法处理电商风险管理需要的多步骤、有状态交互，这些代理通常局限于单步任务且缺乏处理动态交互内容的能力。

Method: RISK框架包含三个组件：RISK-Data数据集（8,492单步和2,386多步轨迹）、RISK-Bench基准测试（802单步和320多步轨迹）、RISK-R1强化微调框架（考虑输出格式、单步级、多步级和任务级四个方面的奖励机制）。

Result: RISK-R1在离线单步任务上提升6.8%，离线多步任务上提升8.8%，在线评估任务成功率70.5%，优于现有基线方法。

Conclusion: RISK为自动化复杂网络交互提供了可扩展的领域特定解决方案，推动了电商风险管理的技术进步。

Abstract: E-commerce risk management requires aggregating diverse, deeply embedded web
data through multi-step, stateful interactions, which traditional scraping
methods and most existing Graphical User Interface (GUI) agents cannot handle.
These agents are typically limited to single-step tasks and lack the ability to
manage dynamic, interactive content critical for effective risk assessment. To
address this challenge, we introduce RISK, a novel framework designed to build
and deploy GUI agents for this domain. RISK integrates three components: (1)
RISK-Data, a dataset of 8,492 single-step and 2,386 multi-step interaction
trajectories, collected through a high-fidelity browser framework and a
meticulous data curation process; (2) RISK-Bench, a benchmark with 802
single-step and 320 multi-step trajectories across three difficulty levels for
standardized evaluation; and (3) RISK-R1, a R1-style reinforcement fine-tuning
framework considering four aspects: (i) Output Format: Updated format reward to
enhance output syntactic correctness and task comprehension, (ii) Single-step
Level: Stepwise accuracy reward to provide granular feedback during early
training stages, (iii) Multi-step Level: Process reweight to emphasize critical
later steps in interaction sequences, and (iv) Task Level: Level reweight to
focus on tasks of varying difficulty. Experiments show that RISK-R1 outperforms
existing baselines, achieving a 6.8% improvement in offline single-step and an
8.8% improvement in offline multi-step. Moreover, it attains a top task success
rate of 70.5% in online evaluation. RISK provides a scalable, domain-specific
solution for automating complex web interactions, advancing the state of the
art in e-commerce risk management.

</details>


### [135] [Bilinear relational structure fixes reversal curse and enables consistent model editing](https://arxiv.org/abs/2509.21993)
*Dong-Kyum Kim,Minsung Kim,Jea Kwon,Nakyeong Yang,Meeyoung Cha*

Main category: cs.AI

TL;DR: 该论文证明反转诅咒不是语言模型的固有局限，而是知识编码方式的产物。通过在关系知识图上训练模型，可以诱导出双线性关系结构，从而缓解反转诅咒并实现一致的模型编辑。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型的反转诅咒问题——即模型无法从已知事实"A是B"推断出反向事实"B是A"，并探索模型编辑的一致性问题。

Method: 在合成的关系知识图数据集上从头训练语言模型，分析隐藏表示中出现的双线性关系结构。

Result: 具有双线性结构的模型能够推断未见过的反向事实，并且在模型编辑时能够正确传播到反向事实和其他逻辑依赖事实。

Conclusion: 模型编辑的成功不仅取决于编辑算法，更关键的是被修改知识的底层表示几何结构。双线性内部表示使语言模型在编辑后能够保持逻辑一致性。

Abstract: The reversal curse -- a language model's (LM) inability to infer an unseen
fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a
fundamental limitation. We show that this is not an inherent failure but an
artifact of how models encode knowledge. By training LMs from scratch on a
synthetic dataset of relational knowledge graphs, we demonstrate that bilinear
relational structure emerges in their hidden representations. This structure
substantially alleviates the reversal curse, enabling LMs to infer unseen
reverse facts. Crucially, we also find that this bilinear structure plays a key
role in consistent model editing. When a fact is updated in a LM with this
structure, the edit correctly propagates to its reverse and other logically
dependent facts. In contrast, models lacking this representation not only
suffer from the reversal curse but also fail to generalize edits, further
introducing logical inconsistencies. Our results establish that training on a
relational knowledge dataset induces the emergence of bilinear internal
representations, which in turn enable LMs to behave in a logically consistent
manner after editing. This implies that the success of model editing depends
critically not just on editing algorithms but on the underlying
representational geometry of the knowledge being modified.

</details>


### [136] [GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments](https://arxiv.org/abs/2509.21998)
*Hanlin Zhu,Tianyu Guo,Song Mei,Stuart Russell,Nikhil Ghosh,Alberto Bietti,Jiantao Jiao*

Main category: cs.AI

TL;DR: 提出了GSM-Agent基准测试，评估LLM在需要主动使用工具收集信息来解决小学数学问题时的代理推理能力，发现即使是前沿模型准确率也仅67%，并提出了代理推理图和工具增强测试时缩放方法来改进性能。


<details>
  <summary>Details</summary>
Motivation: 当前代理基准测试往往将代理推理与复杂数学推理、专家级知识等能力混在一起，难以单独评估代理推理能力。需要构建专门评估代理推理的基准测试。

Method: 构建GSM-Agent基准测试，要求LLM代理解决小学数学问题但只提供问题而不提供前提信息，需要主动使用工具收集信息。提出代理推理图概念来分析和可视化推理模式。

Result: 即使是GPT-5等前沿模型在GSM-Agent上准确率也仅67%。发现许多模型在代理推理中缺乏重新访问先前访问节点的能力，这是静态推理中的关键模式。

Conclusion: GSM-Agent基准测试和代理推理框架有助于未来理解和推进代理推理边界的研究。提出的工具增强测试时缩放方法可以改进LLM的代理推理性能。

Abstract: As LLMs are increasingly deployed as agents, agentic reasoning - the ability
to combine tool use, especially search, and reasoning - becomes a critical
skill. However, it is hard to disentangle agentic reasoning when evaluated in
complex environments and tasks. Current agent benchmarks often mix agentic
reasoning with challenging math reasoning, expert-level knowledge, and other
advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent,
where an LLM agent is required to solve grade-school-level reasoning problems,
but is only presented with the question in the prompt without the premises that
contain the necessary information to solve the task, and needs to proactively
collect that information using tools. Although the original tasks are
grade-school math problems, we observe that even frontier models like GPT-5
only achieve 67% accuracy. To understand and analyze the agentic reasoning
patterns, we propose the concept of agentic reasoning graph: cluster the
environment's document embeddings into nodes, and map each tool call to its
nearest node to build a reasoning path. Surprisingly, we identify that the
ability to revisit a previously visited node, widely taken as a crucial pattern
in static reasoning, is often missing for agentic reasoning for many models.
Based on the insight, we propose a tool-augmented test-time scaling method to
improve LLM's agentic reasoning performance by adding tools to encourage models
to revisit. We expect our benchmark and the agentic reasoning framework to aid
future studies of understanding and pushing the boundaries of agentic
reasoning.

</details>


### [137] [The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs through Model Merging](https://arxiv.org/abs/2509.22034)
*Xiaochong Lan,Yu Zheng,Shiteng Cao,Yong Li*

Main category: cs.AI

TL;DR: 该论文通过大规模实证研究评估了模型融合技术在语言模型推理能力调控方面的潜力，发现模型融合能够有效平衡推理准确性和计算成本，甚至在某些情况下实现帕累托改进。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在真实应用中的需求增长，需要能够高效生成具有可调推理能力的模型谱系，平衡推理深度和计算成本。

Method: 进行大规模实证研究，评估多种模型融合技术，通过系统变化融合强度构建准确率-效率曲线，分析可调性能空间。

Result: 模型融合提供了有效且可控的方法来校准推理准确性和标记效率之间的权衡，即使父模型具有高度不同的权重空间。发现了帕累托改进实例，即融合模型在准确性和标记消耗方面都优于其中一个父模型。

Conclusion: 该研究首次全面分析了模型融合的可调空间，为创建具有特定推理配置的LLM以满足多样化应用需求提供了实用指南。

Abstract: The growing demand for large language models (LLMs) with tunable reasoning
capabilities in many real-world applications highlights a critical need for
methods that can efficiently produce a spectrum of models balancing reasoning
depth and computational cost. Model merging has emerged as a promising,
training-free technique to address this challenge by arithmetically combining
the weights of a general-purpose model with a specialized reasoning model.
While various merging techniques exist, their potential to create a spectrum of
models with fine-grained control over reasoning abilities remains largely
unexplored. This work presents a large-scale empirical study evaluating a range
of model merging techniques across multiple reasoning benchmarks. We
systematically vary merging strengths to construct accuracy-efficiency curves,
providing the first comprehensive view of the tunable performance landscape.
Our findings reveal that model merging offers an effective and controllable
method for calibrating the trade-off between reasoning accuracy and token
efficiency, even when parent models have highly divergent weight spaces.
Crucially, we identify instances of Pareto Improvement, where a merged model
achieves both higher accuracy and lower token consumption than one of its
parents. Our study provides the first comprehensive analysis of this tunable
space, offering practical guidelines for creating LLMs with specific reasoning
profiles to meet diverse application demands.

</details>


### [138] [A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning](https://arxiv.org/abs/2509.22044)
*Ziqi Wang,Boye Niu,Zhongli Li,Linghui Meng,Jing Liu,Zhi Zheng,Tong Xu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.AI

TL;DR: A2R是一个非对称两阶段推理框架，通过探索器并行生成多个解决方案，再由合成器整合这些参考进行精炼推理，显著提升模型在复杂任务上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在单次尝试中的表现与其潜在能力之间的差距，这种差距通常只有在多个解决方案路径中才能显现。

Method: 采用两阶段推理框架：探索器模型通过重复采样并行生成潜在解决方案，合成器模型整合这些参考进行第二阶段的精炼推理。

Result: Qwen3-8B-distill模型性能提升75%；A2R-Efficient变体（Qwen3-4B探索器+Qwen3-8B合成器）超越单一Qwen3-32B模型性能，成本降低30%。

Conclusion: A2R不仅是性能提升框架，更是现实应用中高效实用的解决方案，通过非对称扩展范式实现计算资源的优化利用。

Abstract: Recent Large Reasoning Models have achieved significant improvements in
complex task-solving capabilities by allocating more computation at the
inference stage with a "thinking longer" paradigm. Even as the foundational
reasoning capabilities of models advance rapidly, the persistent gap between a
model's performance in a single attempt and its latent potential, often
revealed only across multiple solution paths, starkly highlights the disparity
between its realized and inherent capabilities. To address this, we present
A2R, an Asymmetric Two-Stage Reasoning framework designed to explicitly bridge
the gap between a model's potential and its actual performance. In this
framework, an "explorer" model first generates potential solutions in parallel
through repeated sampling. Subsequently,a "synthesizer" model integrates these
references for a more refined, second stage of reasoning. This two-stage
process allows computation to be scaled orthogonally to existing sequential
methods. Our work makes two key innovations: First, we present A2R as a
plug-and-play parallel reasoning framework that explicitly enhances a model's
capabilities on complex questions. For example, using our framework, the
Qwen3-8B-distill model achieves a 75% performance improvement compared to its
self-consistency baseline. Second, through a systematic analysis of the
explorer and synthesizer roles, we identify an effective asymmetric scaling
paradigm. This insight leads to A2R-Efficient, a "small-to-big" variant that
combines a Qwen3-4B explorer with a Qwen3-8B synthesizer. This configuration
surpasses the average performance of a monolithic Qwen3-32B model at a nearly
30% lower cost. Collectively, these results show that A2R is not only a
performance-boosting framework but also an efficient and practical solution for
real-world applications.

</details>


### [139] [Generalizing Multi-Objective Search via Objective-Aggregation Functions](https://arxiv.org/abs/2509.22085)
*Hadar Peer,Eyal Weiss,Ron Alterovitz,Oren Salzman*

Main category: cs.AI

TL;DR: 提出了一种多目标搜索的通用问题表述，通过隐藏目标的聚合函数来优化解决方案目标，支持标准MOS算法的应用，在多个机器人规划问题中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现实机器人系统需要同时平衡多个冲突目标，但现有复杂交互问题无法直接使用现成的先进MOS算法，需要更通用的问题表述。

Method: 提出广义问题表述，通过聚合函数优化隐藏目标，只需适当扩展核心操作即可应用标准MOS算法。

Result: 在导航、操作、医疗系统规划、检查规划和路线规划等多个机器人问题中，扩展后的算法比原始版本性能提升数个数量级。

Conclusion: 该通用表述支持标准MOS算法应用，通过扩展核心操作即可处理复杂目标聚合问题，显著提升算法性能。

Abstract: Multi-objective search (MOS) has become essential in robotics, as real-world
robotic systems need to simultaneously balance multiple, often conflicting
objectives. Recent works explore complex interactions between objectives,
leading to problem formulations that do not allow the usage of out-of-the-box
state-of-the-art MOS algorithms. In this paper, we suggest a generalized
problem formulation that optimizes solution objectives via aggregation
functions of hidden (search) objectives. We show that our formulation supports
the application of standard MOS algorithms, necessitating only to properly
extend several core operations to reflect the specific aggregation functions
employed. We demonstrate our approach in several diverse robotics planning
problems, spanning motion-planning for navigation, manipulation and planning fr
medical systems under obstacle uncertainty as well as inspection planning, and
route planning with different road types. We solve the problems using
state-of-the-art MOS algorithms after properly extending their core operations,
and provide empirical evidence that they outperform by orders of magnitude the
vanilla versions of the algorithms applied to the same problems but without
objective aggregation.

</details>


### [140] [Ground-Truthing AI Energy Consumption: Validating CodeCarbon Against External Measurements](https://arxiv.org/abs/2509.22092)
*Raphael Fischer*

Main category: cs.AI

TL;DR: 本研究系统评估了AI模型能耗估算工具的准确性，通过与真实测量数据对比发现现有工具存在高达40%的误差，并提出了改进指南和验证框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI快速发展带来的环境影响日益显著，现有能耗估算工具虽然易于使用，但缺乏准确性验证，需要系统评估其可靠性。

Method: 通过比较静态和动态能耗估算方法与真实测量数据，在数百个AI实验中建立验证框架，分析估算不准确性。

Result: 研究发现现有估算工具虽然能大致反映AI能耗模式，但存在高达40%的持续误差，提供了关于能耗估算质量的实证证据。

Conclusion: 该研究为可持续AI发展建立了透明度，验证了广泛使用的工具，并提出了改进现有技术的指南，为资源感知的ML和AI可持续性研究做出重要贡献。

Abstract: Although machine learning (ML) and artificial intelligence (AI) present
fascinating opportunities for innovation, their rapid development is also
significantly impacting our environment. In response to growing
resource-awareness in the field, quantification tools such as the ML Emissions
Calculator and CodeCarbon were developed to estimate the energy consumption and
carbon emissions of running AI models. They are easy to incorporate into AI
projects, however also make pragmatic assumptions and neglect important
factors, raising the question of estimation accuracy. This study systematically
evaluates the reliability of static and dynamic energy estimation approaches
through comparisons with ground-truth measurements across hundreds of AI
experiments. Based on the proposed validation framework, investigative insights
into AI energy demand and estimation inaccuracies are provided. While generally
following the patterns of AI energy consumption, the established estimation
approaches are shown to consistently make errors of up to 40%. By providing
empirical evidence on energy estimation quality and errors, this study
establishes transparency and validates widely used tools for sustainable AI
development. It moreover formulates guidelines for improving the
state-of-the-art and offers code for extending the validation to other domains
and tools, thus making important contributions to resource-aware ML and AI
sustainability research.

</details>


### [141] [Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach](https://arxiv.org/abs/2509.22137)
*Seoyoung Lee,Seonbin Yoon,Seongbeen Lee,Hyesoo Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: Log2Plan是一个结合结构化两级规划框架和用户行为日志任务挖掘的GUI任务自动化系统，解决了现有LLM/VLM智能体的脆弱泛化、高延迟和有限长程连贯性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM或VLM的规划器-执行器智能体在GUI任务自动化中存在脆弱泛化、高延迟和有限长程连贯性问题，依赖单次推理或静态计划使其在UI变化或复杂任务下表现脆弱。

Method: Log2Plan采用结构化两级规划框架：高层规划将用户命令映射到结构化任务字典，实现一致可泛化的自动化；通过用户行为日志的任务挖掘识别用户特定模式；低层规划将高层计划基于实时GUI上下文转化为具体动作序列。

Result: 在200个真实世界任务上的评估显示，Log2Plan在任务成功率和执行时间上均有显著提升，在长程任务序列上保持超过60.0%的成功率，展现了复杂多步工作流中的鲁棒性。

Conclusion: Log2Plan通过结合结构化规划和任务挖掘方法，实现了鲁棒且适应性强的GUI自动化，显著提升了在复杂、多步工作流中的性能表现。

Abstract: GUI task automation streamlines repetitive tasks, but existing LLM or
VLM-based planner-executor agents suffer from brittle generalization, high
latency, and limited long-horizon coherence. Their reliance on single-shot
reasoning or static plans makes them fragile under UI changes or complex tasks.
Log2Plan addresses these limitations by combining a structured two-level
planning framework with a task mining approach over user behavior logs,
enabling robust and adaptable GUI automation. Log2Plan constructs high-level
plans by mapping user commands to a structured task dictionary, enabling
consistent and generalizable automation. To support personalization and reuse,
it employs a task mining approach from user behavior logs that identifies
user-specific patterns. These high-level plans are then grounded into low-level
action sequences by interpreting real-time GUI context, ensuring robust
execution across varying interfaces. We evaluated Log2Plan on 200 real-world
tasks, demonstrating significant improvements in task success rate and
execution time. Notably, it maintains over 60.0% success rate even on
long-horizon task sequences, highlighting its robustness in complex, multi-step
workflows.

</details>


### [142] [Clinical Uncertainty Impacts Machine Learning Evaluations](https://arxiv.org/abs/2509.22242)
*Simone Lionetti,Fabian Gröger,Philippe Gottfrois,Alvaro Gonzalez-Jimenez,Ludovic Amruthalingam,Alexander A. Navarini,Marc Pouly*

Main category: cs.AI

TL;DR: 论文主张在机器学习评估中考虑标注不确定性，使用概率度量来直接操作分布，而非简单的多数投票聚合标注。


<details>
  <summary>Details</summary>
Motivation: 临床数据集的标注通常存在不确定性，标注者之间存在分歧且置信度不一致，传统的聚合方法（如多数投票）掩盖了这种变异性。

Method: 提出使用概率度量来显式处理标注不确定性，这些度量可以独立于标注生成过程（简单计数、主观置信度评级或概率响应模型）应用，并具有线性时间复杂度的闭式表达式实现。

Result: 在医学影像基准测试的简单实验中，考虑二元标签的置信度显著影响了模型排名。

Conclusion: 呼吁社区发布数据集的原始标注并采用不确定性感知评估，以使性能估计更好地反映临床数据。

Abstract: Clinical dataset labels are rarely certain as annotators disagree and
confidence is not uniform across cases. Typical aggregation procedures, such as
majority voting, obscure this variability. In simple experiments on medical
imaging benchmarks, accounting for the confidence in binary labels
significantly impacts model rankings. We therefore argue that machine-learning
evaluations should explicitly account for annotation uncertainty using
probabilistic metrics that directly operate on distributions. These metrics can
be applied independently of the annotations' generating process, whether
modeled by simple counting, subjective confidence ratings, or probabilistic
response models. They are also computationally lightweight, as closed-form
expressions have linear-time implementations once examples are sorted by model
score. We thus urge the community to release raw annotations for datasets and
to adopt uncertainty-aware evaluation so that performance estimates may better
reflect clinical data.

</details>


### [143] [Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase Heuristics for 2D Bin-Packing](https://arxiv.org/abs/2509.22255)
*Syed Mahbubul Huq,Daniel Brito,Daniel Sikar,Rajesh Mojumder*

Main category: cs.AI

TL;DR: 评估大语言模型在组合优化问题（特别是二维装箱问题）中的能力，提出结合LLM与进化算法的系统方法，证明LLM能生成更高效的启发式解决方案。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在专业领域（组合优化）的能力，建立LLM在组合优化任务中的性能基准。

Method: 结合大语言模型与进化算法的系统方法，迭代生成和优化启发式解决方案，并与传统方法（有限首次适应和混合首次适应）进行比较。

Result: GPT-4o在两次迭代内达到最优解，平均使用箱数从16减少到15，空间利用率从0.76-0.78提升到0.83，同时计算资源需求更少。

Conclusion: LLM在组合优化中能产生更高效的解决方案，为理解LLM在专业领域的评估提供了贡献，并建立了组合优化任务的性能基准。

Abstract: This paper presents an evaluation framework for assessing Large Language
Models' (LLMs) capabilities in combinatorial optimization, specifically
addressing the 2D bin-packing problem. We introduce a systematic methodology
that combines LLMs with evolutionary algorithms to generate and refine
heuristic solutions iteratively. Through comprehensive experiments comparing
LLM generated heuristics against traditional approaches (Finite First-Fit and
Hybrid First-Fit), we demonstrate that LLMs can produce more efficient
solutions while requiring fewer computational resources. Our evaluation reveals
that GPT-4o achieves optimal solutions within two iterations, reducing average
bin usage from 16 to 15 bins while improving space utilization from 0.76-0.78
to 0.83. This work contributes to understanding LLM evaluation in specialized
domains and establishes benchmarks for assessing LLM performance in
combinatorial optimization tasks.

</details>


### [144] [InfiMed-Foundation: Pioneering Advanced Multimodal Medical Models with Compute-Efficient Pre-Training and Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.22261)
*Guanghao Zhu,Zhitian Hou,Zeyu Liu,Zhijie Sang,Congkai Xie,Hongxia Yang*

Main category: cs.AI

TL;DR: 提出了两种医疗专用多模态大语言模型InfiMed-Foundation-1.7B和4B，通过高质量数据筛选、高效训练策略和三阶段微调，在医疗任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 通用多模态大语言模型在医疗领域应用受限，缺乏专业知识导致回答不确定或产生幻觉，知识蒸馏难以捕捉放射学和药理学等专业领域知识，大规模医疗数据持续预训练计算成本高。

Method: 结合通用和医疗多模态数据，提出五维质量评估框架筛选高质量数据集；采用低到高图像分辨率和多模态序列打包提高训练效率；使用三阶段监督微调进行复杂医疗任务知识提取。

Result: 在MedEvalKit评估中，InfiMed-Foundation-1.7B优于Qwen2.5VL-3B，InfiMed-Foundation-4B超越HuatuoGPT-V-7B和MedGemma-27B-IT，在医疗视觉问答和诊断任务中表现优异。

Conclusion: 通过解决数据质量、训练效率和领域专业知识提取等关键挑战，为医疗领域提供更可靠有效的AI驱动解决方案。

Abstract: Multimodal large language models (MLLMs) have shown remarkable potential in
various domains, yet their application in the medical field is hindered by
several challenges. General-purpose MLLMs often lack the specialized knowledge
required for medical tasks, leading to uncertain or hallucinatory responses.
Knowledge distillation from advanced models struggles to capture
domain-specific expertise in radiology and pharmacology. Additionally, the
computational cost of continual pretraining with large-scale medical data poses
significant efficiency challenges. To address these issues, we propose
InfiMed-Foundation-1.7B and InfiMed-Foundation-4B, two medical-specific MLLMs
designed to deliver state-of-the-art performance in medical applications. We
combined high-quality general-purpose and medical multimodal data and proposed
a novel five-dimensional quality assessment framework to curate high-quality
multimodal medical datasets. We employ low-to-high image resolution and
multimodal sequence packing to enhance training efficiency, enabling the
integration of extensive medical data. Furthermore, a three-stage supervised
fine-tuning process ensures effective knowledge extraction for complex medical
tasks. Evaluated on the MedEvalKit framework, InfiMed-Foundation-1.7B
outperforms Qwen2.5VL-3B, while InfiMed-Foundation-4B surpasses HuatuoGPT-V-7B
and MedGemma-27B-IT, demonstrating superior performance in medical visual
question answering and diagnostic tasks. By addressing key challenges in data
quality, training efficiency, and domain-specific knowledge extraction, our
work paves the way for more reliable and effective AI-driven solutions in
healthcare. InfiMed-Foundation-4B model is available at
\href{https://huggingface.co/InfiX-ai/InfiMed-Foundation-4B}{InfiMed-Foundation-4B}.

</details>


### [145] [Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models](https://arxiv.org/abs/2509.22284)
*Aleksandar Terzić,Nicolas Menet,Michael Hersche,Thomas Hofmann,Abbas Rahimi*

Main category: cs.AI

TL;DR: 提出了PD-SSM方法，通过结构化稀疏参数化状态转移矩阵，在保持计算效率的同时显著提升了状态空间模型的表达能力，能够最优地模拟有限状态自动机。


<details>
  <summary>Details</summary>
Motivation: 现有状态空间模型使用结构化转移矩阵虽然计算高效，但表达能力受限；而无结构矩阵虽然表达能力最优，但计算和内存成本过高。需要一种既能保持计算效率又能实现最优表达能力的方法。

Method: 将转移矩阵参数化为列独热矩阵(P)和复值对角矩阵(D)的乘积，使得并行扫描的计算成本与状态大小呈线性关系，同时保证模型稳定性和最优状态跟踪能力。

Result: 在FSA状态跟踪任务上显著优于现有SSM变体，在时间序列分类上与神经控制微分方程性能相当，并能有效跟踪复杂FSA状态。

Conclusion: PD-SSM方法在保持计算效率的同时实现了最优的FSA模拟能力，为状态空间模型提供了表达能力和效率的良好平衡。

Abstract: Modern state-space models (SSMs) often utilize transition matrices which
enable efficient computation but pose restrictions on the model's expressivity,
as measured in terms of the ability to emulate finite-state automata (FSA).
While unstructured transition matrices are optimal in terms of expressivity,
they come at a prohibitively high compute and memory cost even for moderate
state sizes. We propose a structured sparse parametrization of transition
matrices in SSMs that enables FSA state tracking with optimal state size and
depth, while keeping the computational cost of the recurrence comparable to
that of diagonal SSMs. Our method, PD-SSM, parametrizes the transition matrix
as the product of a column one-hot matrix ($P$) and a complex-valued diagonal
matrix ($D$). Consequently, the computational cost of parallel scans scales
linearly with the state size. Theoretically, the model is BIBO-stable and can
emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout
of size $N \times N$, significantly improving on all current structured SSM
guarantees. Experimentally, the model significantly outperforms a wide
collection of modern SSM variants on various FSA state tracking tasks. On
multiclass time-series classification, the performance is comparable to that of
neural controlled differential equations, a paradigm explicitly built for
time-series analysis. Finally, we integrate PD-SSM into a hybrid
Transformer-SSM architecture and demonstrate that the model can effectively
track the states of a complex FSA in which transitions are encoded as a set of
variable-length English sentences. The code is available at
https://github.com/IBM/expressive-sparse-state-space-model

</details>


### [146] [Large Language Models as Nondeterministic Causal Models](https://arxiv.org/abs/2509.22297)
*Sander Beckers*

Main category: cs.AI

TL;DR: 本文提出了一种更简单的反事实生成方法，将LLM表示为非确定性因果模型，适用于任何黑盒LLM而不需要修改。


<details>
  <summary>Details</summary>
Motivation: 现有方法对LLM的解释存在歧义，要么没有字面解释LLM，要么没有按预期解释LLM。需要一种基于LLM预期语义的更简单方法。

Method: 将LLM表示为非确定性因果模型，而不是确定性因果模型，从而生成反事实。这种方法对任何黑盒LLM都直接适用。

Result: 新方法比现有方法更简单，且不依赖LLM的实现细节，可直接应用于任何黑盒LLM。

Conclusion: 两种方法各有优势：新方法更通用简单，现有方法能直接生成特定类型的反事实。本文为基于LLM预期语义的反事实推理提供了理论基础。

Abstract: Recent work by Chatzi et al. and Ravfogel et al. has developed, for the first
time, a method for generating counterfactuals of probabilistic Large Language
Models. Such counterfactuals tell us what would - or might - have been the
output of an LLM if some factual prompt ${\bf x}$ had been ${\bf x}^*$ instead.
The ability to generate such counterfactuals is an important necessary step
towards explaining, evaluating, and comparing, the behavior of LLMs. I argue,
however, that the existing method rests on an ambiguous interpretation of LLMs:
it does not interpret LLMs literally, for the method involves the assumption
that one can change the implementation of an LLM's sampling process without
changing the LLM itself, nor does it interpret LLMs as intended, for the method
involves explicitly representing a nondeterministic LLM as a deterministic
causal model. I here present a much simpler method for generating
counterfactuals that is based on an LLM's intended interpretation by
representing it as a nondeterministic causal model instead. The advantage of my
simpler method is that it is directly applicable to any black-box LLM without
modification, as it is agnostic to any implementation details. The advantage of
the existing method, on the other hand, is that it directly implements the
generation of a specific type of counterfactuals that is useful for certain
purposes, but not for others. I clarify how both methods relate by offering a
theoretical foundation for reasoning about counterfactuals in LLMs based on
their intended semantics, thereby laying the groundwork for novel
application-specific methods for generating counterfactuals.

</details>


### [147] [PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning](https://arxiv.org/abs/2509.22315)
*Hieu Tran,Zonghai Yao,Nguyen Luong Tran,Zhichao Yang,Feiyun Ouyang,Shuo Han,Razieh Rahimi,Hong Yu*

Main category: cs.AI

TL;DR: PRIME是一个多智能体推理框架，灵感来自人类认知的双过程理论，通过动态整合快速直觉思维（System 1）和慢速审慎思维（System 2）来提升LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 受《思考，快与慢》中人类认知双过程理论的启发，旨在让LLM更贴近人类的认知过程，在效率和准确性之间取得平衡。

Method: 采用多智能体设计：首先由快速思考智能体（System 1）生成快速答案，如果检测到不确定性，则触发包含规划、假设生成、检索、信息整合和决策等专门智能体的System 2推理流程。

Result: 实验结果显示，PRIME让开源LLaMA 3模型在多跳推理和知识基础推理基准测试中，能够与GPT-4、GPT-4o等闭源模型竞争。

Conclusion: PRIME为需要复杂知识密集型推理的领域提供了一个可扩展的解决方案，能够有效提升LLM的推理能力。

Abstract: Inspired by the dual-process theory of human cognition from \textit{Thinking,
Fast and Slow}, we introduce \textbf{PRIME} (Planning and Retrieval-Integrated
Memory for Enhanced Reasoning), a multi-agent reasoning framework that
dynamically integrates \textbf{System 1} (fast, intuitive thinking) and
\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick
Thinking Agent (System 1) to generate a rapid answer; if uncertainty is
detected, it then triggers a structured System 2 reasoning pipeline composed of
specialized agents for \textit{planning}, \textit{hypothesis generation},
\textit{retrieval}, \textit{information integration}, and
\textit{decision-making}. This multi-agent design faithfully mimics human
cognitive processes and enhances both efficiency and accuracy. Experimental
results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to
perform competitively with state-of-the-art closed-source models like GPT-4 and
GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This
research establishes PRIME as a scalable solution for improving LLMs in domains
requiring complex, knowledge-intensive reasoning.

</details>


### [148] [Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents](https://arxiv.org/abs/2509.22391)
*Jiaqi Shao,Yuxiang Lin,Munish Prasad Lohani,Yufeng Miao,Bing Luo*

Main category: cs.AI

TL;DR: SeekBench是首个用于评估LLM搜索代理认知能力的基准，通过步骤级分析响应轨迹来评估代理是否基于观察证据生成推理步骤、自适应重新制定搜索以及正确评估证据充分性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注最终答案准确性，忽视了LLM搜索代理如何基于外部证据进行推理和行动，需要更细粒度的评估方法。

Method: 构建包含190个专家标注轨迹和1800多个响应步骤的SeekBench基准，每个步骤都带有证据标注，用于分析代理的认知能力。

Result: 提出了首个专门评估LLM搜索代理认知能力的基准，能够进行细粒度的步骤级分析。

Conclusion: SeekBench为评估LLM搜索代理的认知能力提供了重要工具，有助于更全面地理解代理的推理和行为模式。

Abstract: Recent work has explored training Large Language Model (LLM) search agents
with reinforcement learning (RL) for open-domain question answering (QA).
However, most evaluations focus solely on final answer accuracy, overlooking
how these agents reason with and act on external evidence. We introduce
SeekBench, the first benchmark for evaluating the \textit{epistemic competence}
of LLM search agents through step-level analysis of their response traces.
SeekBench comprises 190 expert-annotated traces with over 1,800 response steps
generated by LLM search agents, each enriched with evidence annotations for
granular analysis of whether agents (1) generate reasoning steps grounded in
observed evidence, (2) adaptively reformulate searches to recover from
low-quality results, and (3) have proper calibration to correctly assess
whether the current evidence is sufficient for providing an answer.

</details>


### [149] [EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer](https://arxiv.org/abs/2509.22407)
*Zhehao Dong,Xiaofeng Wang,Zheng Zhu,Yirui Wang,Yang Wang,Yukun Zhou,Boyuan Wang,Chaojun Ni,Runqi Ouyang,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang*

Main category: cs.AI

TL;DR: 提出了EMMA框架，通过DreamTransfer生成多视角一致的机器人操作视频，结合AdaMix训练策略，显著提升视觉-语言-动作模型在未见物体类别和新视觉领域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 收集大规模真实机器人操作数据成本高昂且耗时，限制了VLA模型的泛化能力。需要一种方法能够利用有限真实数据生成多样化的训练数据。

Method: 1. DreamTransfer：基于扩散Transformer的框架，生成多视角一致、几何基础的机器人操作视频；2. AdaMix：硬样本感知训练策略，动态调整批次权重；3. 混合训练：结合真实和生成数据。

Result: 生成的视频在多视角一致性、几何保真度和文本条件准确性方面显著优于现有方法。在零样本视觉领域的真实机器人操作任务中，相比仅使用真实数据训练，性能提升超过200%，结合AdaMix后进一步提升13%。

Conclusion: EMMA框架通过生成高质量的训练数据和有效的训练策略，显著提升了VLA模型在机器人操作任务中的泛化能力，仅需单一外观的演示即可泛化到未见物体类别和新视觉领域。

Abstract: Vision-language-action (VLA) models increasingly rely on diverse training
data to achieve robust generalization. However, collecting large-scale
real-world robot manipulation data across varied object appearances and
environmental conditions remains prohibitively time-consuming and expensive. To
overcome this bottleneck, we propose Embodied Manipulation Media Adaptation
(EMMA), a VLA policy enhancement framework that integrates a generative data
engine with an effective training pipeline. We introduce DreamTransfer, a
diffusion Transformer-based framework for generating multi-view consistent,
geometrically grounded embodied manipulation videos. DreamTransfer enables
text-controlled visual editing of robot videos, transforming foreground,
background, and lighting conditions without compromising 3D structure or
geometrical plausibility. Furthermore, we explore hybrid training with real and
generated data, and introduce AdaMix, a hard-sample-aware training strategy
that dynamically reweights training batches to focus optimization on
perceptually or kinematically challenging samples. Extensive experiments show
that videos generated by DreamTransfer significantly outperform prior video
generation methods in multi-view consistency, geometric fidelity, and
text-conditioning accuracy. Crucially, VLAs trained with generated data enable
robots to generalize to unseen object categories and novel visual domains using
only demonstrations from a single appearance. In real-world robotic
manipulation tasks with zero-shot visual domains, our approach achieves over a
200% relative performance gain compared to training on real data alone, and
further improves by 13% with AdaMix, demonstrating its effectiveness in
boosting policy generalization.

</details>


### [150] [Guiding Evolution of Artificial Life Using Vision-Language Models](https://arxiv.org/abs/2509.22447)
*Nikhil Baid,Hannah Erlebach,Paul Hellegouarch,Frederico Wieser*

Main category: cs.AI

TL;DR: ASAL++是一种基于多模态基础模型的开放搜索方法，通过第二个基础模型根据模拟视觉历史提出新的进化目标，引导人工生命模拟产生越来越复杂的目标。


<details>
  <summary>Details</summary>
Motivation: 基础模型为人工生命领域提供了强大的自动化搜索工具，但之前的工作主要使用视觉语言模型将人工生命模拟与自然语言目标提示对齐。本文旨在开发更具开放性的搜索方法。

Method: 在ASAL基础上引入ASAL++，使用第二个基础模型根据模拟视觉历史提出新的进化目标。探索两种策略：EST（每次迭代匹配单个新提示）和ETT（匹配整个生成提示序列）。在Lenia基底中使用Gemma-3提出进化目标。

Result: 实验结果表明，EST策略促进了更大的视觉新颖性，而ETT策略培养了更连贯和可解释的进化序列。

Conclusion: ASAL++为基础模型驱动的人工生命发现指出了具有开放特性新方向。

Abstract: Foundation models (FMs) have recently opened up new frontiers in the field of
artificial life (ALife) by providing powerful tools to automate search through
ALife simulations. Previous work aligns ALife simulations with natural language
target prompts using vision-language models (VLMs). We build on Automated
Search for Artificial Life (ASAL) by introducing ASAL++, a method for
open-ended-like search guided by multimodal FMs. We use a second FM to propose
new evolutionary targets based on a simulation's visual history. This induces
an evolutionary trajectory with increasingly complex targets.
  We explore two strategies: (1) evolving a simulation to match a single new
prompt at each iteration (Evolved Supervised Targets: EST) and (2) evolving a
simulation to match the entire sequence of generated prompts (Evolved Temporal
Targets: ETT). We test our method empirically in the Lenia substrate using
Gemma-3 to propose evolutionary targets, and show that EST promotes greater
visual novelty, while ETT fosters more coherent and interpretable evolutionary
sequences.
  Our results suggest that ASAL++ points towards new directions for FM-driven
ALife discovery with open-ended characteristics.

</details>


### [151] [GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation](https://arxiv.org/abs/2509.22460)
*Shichao Weng,Zhiqiang Wang,Yuhua Zhou,Rui Lu,Ting Liu,Zhiyang Teng,Xiaozhang Liu,Hanmeng Liu*

Main category: cs.AI

TL;DR: GeoSketch是一个神经符号框架，将几何推理转化为交互式的感知-推理-行动循环，通过动态操作图表（如绘制辅助线）来提升几何问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型将图表视为静态图像，缺乏动态操作能力，而人类几何推理需要辅助线构造和仿射变换等动态操作。

Method: 集成三个模块：感知模块将图表抽象为结构化逻辑形式，符号推理模块应用几何定理决定下一步推理步骤，草图行动模块执行操作（如绘制辅助线）并更新图表。采用两阶段训练：监督微调和强化学习。

Result: 在GeoSketch基准测试中，相比静态感知方法，GeoSketch显著提高了逐步推理准确性和问题解决成功率。

Conclusion: 通过统一分层决策、可执行视觉动作和符号验证，GeoSketch将多模态推理从静态解释推进到动态可验证交互，为复杂视觉空间问题解决建立了新基础。

Abstract: Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large
Language Models (MLLMs), requiring not only the joint interpretation of text
and diagrams but also iterative visuospatial reasoning. While existing
approaches process diagrams as static images, they lack the capacity for
dynamic manipulation - a core aspect of human geometric reasoning involving
auxiliary line construction and affine transformations. We present GeoSketch, a
neural-symbolic framework that recasts geometric reasoning as an interactive
perception-reasoning-action loop. GeoSketch integrates: (1) a Perception module
that abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning
module that applies geometric theorems to decide the next deductive step, and
(3) a Sketch Action module that executes operations such as drawing auxiliary
lines or applying transformations, thereby updating the diagram in a closed
loop. To train this agent, we develop a two-stage pipeline: supervised
fine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement
learning with dense, symbolic rewards to enhance robustness and strategic
exploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a
high-quality set of 390 geometry problems requiring auxiliary construction or
affine transformations. Experiments on strong MLLM baselines demonstrate that
GeoSketch significantly improves stepwise reasoning accuracy and
problem-solving success over static perception methods. By unifying
hierarchical decision-making, executable visual actions, and symbolic
verification, GeoSketch advances multimodal reasoning from static
interpretation to dynamic, verifiable interaction, establishing a new
foundation for solving complex visuospatial problems.

</details>


### [152] [InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios](https://arxiv.org/abs/2509.22502)
*Chenglin Yu,Yang Yu,Songmiao Wang,Yucheng Wang,Yifan Yang,Jinjia Li,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: InfiAgent是一个基于有向无环图的金字塔式多智能体框架，通过自动分解复杂任务、双重审核机制、智能路由和自主进化等创新，解决了传统LLM智能体开发复杂、难以扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 传统LLM智能体开发需要精心设计工作流、提示工程和迭代调优，这些手工限制阻碍了智能体在不同行业中的可扩展性和成本效益。

Method: 提出金字塔式DAG多智能体框架，包含：智能体即工具机制自动分解复杂任务、双重审核确保质量、智能路由匹配任务与智能体、自主进化机制根据新任务或性能问题重构DAG。

Result: 在多个基准测试中比ADAS框架性能提升9.9%，案例研究显示InfiHelper生成的科研论文获得了IEEE顶级会议人类评审的认可。

Conclusion: InfiAgent框架能够演化成通用的金字塔式多智能体系统，有效解决广泛领域的问题，显著提升了LLM智能体的可扩展性和执行效率。

Abstract: Large Language Model (LLM) agents have demonstrated remarkable capabilities
in organizing and executing complex tasks, and many such agents are now widely
used in various application scenarios. However, developing these agents
requires carefully designed workflows, carefully crafted prompts, and iterative
tuning, which requires LLM techniques and domain-specific expertise. These
hand-crafted limitations hinder the scalability and cost-effectiveness of LLM
agents across a wide range of industries. To address these challenges, we
propose \textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that
can be applied to \textbf{infi}nite scenarios, which introduces several key
innovations: a generalized "agent-as-a-tool" mechanism that automatically
decomposes complex agents into hierarchical multi-agent systems; a dual-audit
mechanism that ensures the quality and stability of task completion; an agent
routing function that enables efficient task-agent matching; and an agent
self-evolution mechanism that autonomously restructures the agent DAG based on
new tasks, poor performance, or optimization opportunities. Furthermore,
InfiAgent's atomic task design supports agent parallelism, significantly
improving execution efficiency. This framework evolves into a versatile
pyramid-like multi-agent system capable of solving a wide range of problems.
Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\%
higher performance compared to ADAS (similar auto-generated agent framework),
while a case study of the AI research assistant InfiHelper shows that it
generates scientific papers that have received recognition from human reviewers
at top-tier IEEE conferences.

</details>


### [153] [Estimating the Empowerment of Language Model Agents](https://arxiv.org/abs/2509.22504)
*Jinyeop Song,Jeff Gore,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 提出基于信息论中赋能概念的语言模型智能体评估框架EELMA，通过计算智能体行为与未来状态间的互信息来评估开放环境中的智能体能力。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试成本高且需要人工设计任务，需要一种可扩展的开放评估框架来评估语言模型智能体的能力。

Method: 开发EELMA算法，通过多轮文本交互估算语言模型智能体的有效赋能，在语言游戏和真实网页浏览场景中进行验证。

Result: 赋能与平均任务性能强相关，能表征环境复杂性、思维链、模型规模和记忆长度等因素的影响，高赋能状态往往是关键能力节点。

Conclusion: 赋能是评估复杂开放环境中语言模型智能体的有吸引力的通用指标。

Abstract: As language model (LM) agents become more capable and gain broader access to
real-world tools, there is a growing need for scalable evaluation frameworks of
agentic capability. However, conventional benchmark-centric evaluations are
costly to design and require human designers to come up with valid tasks that
translate into insights about general model capabilities. In this work, we
propose information-theoretic evaluation based on empowerment, the mutual
information between an agent's actions and future states, as an open-ended
method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of
Language Model Agents), an algorithm for approximating effective empowerment
from multi-turn text interactions. We validate EELMA on both language games and
scaled-up realistic web-browsing scenarios. We find that empowerment strongly
correlates with average task performance, characterize the impact of
environmental complexity and agentic factors such as chain-of-thought, model
scale, and memory length on estimated empowerment, and that high empowerment
states and actions are often pivotal moments for general capabilities.
Together, these results demonstrate empowerment as an appealing general-purpose
metric for evaluating and monitoring LM agents in complex, open-ended settings.

</details>


### [154] [TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent and Explainable Digital Assessments](https://arxiv.org/abs/2509.22516)
*Rakesh Thakur,Shivaansh Kaushik,Gauri Chopra,Harsh Rohilla*

Main category: cs.AI

TL;DR: TrueGradeAI是一个AI驱动的数字考试框架，通过手写保存和可解释的自动化评分系统，解决传统纸质考试的纸张浪费、评分延迟和评估者偏见问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统纸质考试的局限性，包括纸张浪费、物流复杂、评分延迟和评估者偏见，同时保持手写的自然性。

Method: 使用安全平板捕获手写输入，应用基于transformer的光学字符识别进行转录，通过检索增强管道整合教师解决方案、缓存层和外部参考，让大语言模型进行评分并提供证据关联的推理。

Result: 系统实现了手写保存与可扩展透明评估的结合，减少了环境成本，加速了反馈周期，并逐步构建可重用的知识库。

Conclusion: TrueGradeAI通过结合手写保存与可扩展透明评估，推进了数字考试领域，同时积极减轻评分偏见并确保评估公平性。

Abstract: This paper introduces TrueGradeAI, an AI-driven digital examination framework
designed to overcome the shortcomings of traditional paper-based assessments,
including excessive paper usage, logistical complexity, grading delays, and
evaluator bias. The system preserves natural handwriting by capturing stylus
input on secure tablets and applying transformer-based optical character
recognition for transcription. Evaluation is conducted through a
retrieval-augmented pipeline that integrates faculty solutions, cache layers,
and external references, enabling a large language model to assign scores with
explicit, evidence-linked reasoning. Unlike prior tablet-based exam systems
that primarily digitize responses, TrueGradeAI advances the field by
incorporating explainable automation, bias mitigation, and auditable grading
trails. By uniting handwriting preservation with scalable and transparent
evaluation, the framework reduces environmental costs, accelerates feedback
cycles, and progressively builds a reusable knowledge base, while actively
working to mitigate grading bias and ensure fairness in assessment.

</details>


### [155] [REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language Model](https://arxiv.org/abs/2509.22518)
*Bo Li,Guanzhi Deng,Ronghao Chen,Junrong Yue,Shuo Zhang,Qinghua Zhao,Linqi Song,Lijie Wen*

Main category: cs.AI

TL;DR: 该论文提出了推理流形(Reasoning Manifold)概念和REMA框架，通过分析模型内部表示的低维几何结构来量化推理失败的原因。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型如何进行复杂推理及其失败机制是解释性研究的挑战，需要可测量的几何分析视角。

Method: 定义推理流形概念，构建REMA框架，通过计算错误表示与正确表示形成的流形之间的k近邻距离来量化几何偏差，并追踪各层的偏差变化定位分歧点。

Result: 实验证明推理流形具有低维特性，错误与正确推理表示高度可分，REMA框架能有效分析推理失败起源。

Conclusion: 该研究将抽象推理失败与表示中的可测量几何偏差联系起来，为深入理解黑盒模型的内部计算过程提供了新途径。

Abstract: Understanding how Large Language Models (LLMs) perform complex reasoning and
their failure mechanisms is a challenge in interpretability research. To
provide a measurable geometric analysis perspective, we define the concept of
the Reasoning Manifold, a latent low-dimensional geometric structure formed by
the internal representations corresponding to all correctly reasoned
generations. This structure can be conceptualized as the embodiment of the
effective thinking paths that the model has learned to successfully solve a
given task. Based on this concept, we build REMA, a framework that explains the
origins of failures by quantitatively comparing the spatial relationships of
internal model representations corresponding to both erroneous and correct
reasoning samples. Specifically, REMA first quantifies the geometric deviation
of each erroneous representation by calculating its k-nearest neighbors
distance to the approximated manifold formed by correct representations,
thereby providing a unified failure signal. It then localizes the divergence
points where these deviations first become significant by tracking this
deviation metric across the model's layers and comparing it against a baseline
of internal fluctuations from correct representations, thus identifying where
the reasoning chain begins to go off-track. Our extensive experiments on
diverse language and multimodal models and tasks demonstrate the
low-dimensional nature of the reasoning manifold and the high separability
between erroneous and correct reasoning representations. The results also
validate the effectiveness of the REMA framework in analyzing the origins of
reasoning failures. This research connects abstract reasoning failures to
measurable geometric deviations in representations, providing new avenues for
in-depth understanding and diagnosis of the internal computational processes of
black-box models.

</details>


### [156] [The Emergence of Altruism in Large-Language-Model Agents Society](https://arxiv.org/abs/2509.22537)
*Haoyang Li,Xiao Jia,Zhanzhan Zhao*

Main category: cs.AI

TL;DR: 研究发现大语言模型在社会模拟中存在两种不同的社会倾向："适应性利己主义者"和"利他主义优化者"，揭示了LLM内在的社会行为逻辑异质性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注小规模任务导向游戏中的合作行为，忽略了大规模智能体社会中利他主义的涌现机制。

Method: 引入Schelling变体城市迁移模型，创建社会困境，让200多个LLM代理在利己和利他目标间进行选择，并使用扎根理论方法分析代理推理。

Result: 识别出两种原型：适应性利己主义者默认优先考虑自身利益但在社会规范影响下利他行为显著增加；利他主义优化者则始终优先集体利益。

Conclusion: 社会模拟中的模型选择不仅是选择推理能力，更是选择内在的社会行为逻辑，不同原型适用于不同模拟场景。

Abstract: Leveraging Large Language Models (LLMs) for social simulation is a frontier
in computational social science. Understanding the social logics these agents
embody is critical to this attempt. However, existing research has primarily
focused on cooperation in small-scale, task-oriented games, overlooking how
altruism, which means sacrificing self-interest for collective benefit, emerges
in large-scale agent societies. To address this gap, we introduce a
Schelling-variant urban migration model that creates a social dilemma,
compelling over 200 LLM agents to navigate an explicit conflict between
egoistic (personal utility) and altruistic (system utility) goals. Our central
finding is a fundamental difference in the social tendencies of LLMs. We
identify two distinct archetypes: "Adaptive Egoists", which default to
prioritizing self-interest but whose altruistic behaviors significantly
increase under the influence of a social norm-setting message board; and
"Altruistic Optimizers", which exhibit an inherent altruistic logic,
consistently prioritizing collective benefit even at a direct cost to
themselves. Furthermore, to qualitatively analyze the cognitive underpinnings
of these decisions, we introduce a method inspired by Grounded Theory to
systematically code agent reasoning. In summary, this research provides the
first evidence of intrinsic heterogeneity in the egoistic and altruistic
tendencies of different LLMs. We propose that for social simulation, model
selection is not merely a matter of choosing reasoning capability, but of
choosing an intrinsic social action logic. While "Adaptive Egoists" may offer a
more suitable choice for simulating complex human societies, "Altruistic
Optimizers" are better suited for modeling idealized pro-social actors or
scenarios where collective welfare is the primary consideration.

</details>


### [157] [StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models](https://arxiv.org/abs/2509.22558)
*Chenyu Zhou,Tianyi Xu,Jianghao Lin,Dongdong Ge*

Main category: cs.AI

TL;DR: StepORLM是一个自演化的LLM框架，通过生成式过程监督解决运筹学问题，采用策略模型和生成过程奖励模型的协同进化循环，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个关键限制：结果奖励存在信用分配问题（正确答案可能强化错误推理），传统判别式过程监督目光短浅，无法整体评估运筹学建模的相互依赖步骤。

Method: StepORLM采用协同进化循环，策略模型和生成过程奖励模型相互迭代改进。通过双反馈机制：外部求解器的确定性结果验证和GenPRM的细致整体过程评估，使用加权直接偏好优化对齐策略并同时精炼GenPRM。

Result: 8B参数的StepORLM在六个基准测试中建立了新的最先进水平，显著优于更大的通用模型、代理方法和专门基线。协同进化的GenPRM可作为强大的通用过程验证器，大幅提升自身模型和其他现有LLM的推理扩展性能。

Conclusion: StepORLM通过生成式过程监督和协同进化框架有效解决了运筹学问题中的信用分配和过程评估问题，证明了该方法在提升LLM解决复杂问题能力方面的有效性。

Abstract: Large Language Models (LLMs) have shown promising capabilities for solving
Operations Research (OR) problems. While reinforcement learning serves as a
powerful paradigm for LLM training on OR problems, existing works generally
face two key limitations. First, outcome reward suffers from the credit
assignment problem, where correct final answers can reinforce flawed reasoning.
Second, conventional discriminative process supervision is myopic, failing to
evaluate the interdependent steps of OR modeling holistically. To this end, we
introduce StepORLM, a novel self-evolving framework with generative process
supervision. At its core, StepORLM features a co-evolutionary loop where a
policy model and a generative process reward model (GenPRM) iteratively improve
on each other. This loop is driven by a dual-feedback mechanism: definitive,
outcome-based verification from an external solver, and nuanced, holistic
process evaluation from the GenPRM. The combined signal is used to align the
policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously
refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new
state-of-the-art across six benchmarks, significantly outperforming vastly
larger generalist models, agentic methods, and specialized baselines. Moreover,
the co-evolved GenPRM is able to act as a powerful and universally applicable
process verifier, substantially boosting the inference scaling performance of
both our own model and other existing LLMs.

</details>


### [158] [UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration](https://arxiv.org/abs/2509.22570)
*Qi Mao,Tinghan Yang,Jiahao Li,Bin Li,Libiao Jin,Yan Lu*

Main category: cs.AI

TL;DR: UniMIC是一个统一的基于token的多模态交互编码框架，通过紧凑的token化表示作为通信媒介，在保持与大型多模态模型兼容的同时实现高效低比特率传输。


<details>
  <summary>Details</summary>
Motivation: 现有编解码器仍针对单模态单向通信优化，在传统的压缩-传输-重建流程中会导致重复的质量下降，无法满足多模态双向交互的需求。

Method: 采用轻量级Transformer熵模型，包含通用型、掩码型和文本条件型三种场景特定设计，有效最小化token间冗余。

Result: 在文本到图像生成、文本引导修复、扩展和视觉问答等任务中，UniMIC实现了显著的比特率节省，即使在超低比特率(<0.05bpp)下仍保持鲁棒性，且不影响下游任务性能。

Conclusion: UniMIC为下一代多模态交互通信提供了一个实用且前瞻性的范式。

Abstract: The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI
agents is transforming human-AI collaboration into bidirectional, multimodal
interaction. However, existing codecs remain optimized for unimodal, one-way
communication, resulting in repeated degradation under conventional
compress-transmit-reconstruct pipelines. To address this limitation, we propose
UniMIC, a Unified token-based Multimodal Interactive Coding framework that
bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or
plain text, UniMIC employs compact tokenized representations as the
communication medium, enabling efficient low-bitrate transmission while
maintaining compatibility with LMMs. To further enhance compression,
lightweight Transformer-based entropy models with scenario-specific
designs-generic, masked, and text-conditioned-effectively minimize inter-token
redundancy. Extensive experiments on text-to-image generation, text-guided
inpainting, outpainting, and visual question answering show that UniMIC
achieves substantial bitrate savings and remains robust even at ultra-low
bitrates (<0.05bpp), without compromising downstream task performance. These
results establish UniMIC as a practical and forward-looking paradigm for
next-generation multimodal interactive communication.

</details>


### [159] [Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time](https://arxiv.org/abs/2509.22572)
*Yixuan Han,Fan Ma,Ruijie Quan,Yi Yang*

Main category: cs.AI

TL;DR: 提出Dynamic Experts Search (DES)方法，通过动态控制MoE模型中激活的专家数量来增强推理能力，无需额外计算成本


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放方法主要依赖输出级采样，忽略了模型架构的作用。在MoE模型中，发现改变激活专家数量可以产生互补的解决方案集

Method: DES包含两个关键组件：Dynamic MoE（在推理时直接控制专家数量生成多样化推理轨迹）和Expert Configuration Inheritance（在推理路径内保持一致的专家数量，在不同运行间变化专家数量）

Result: 在多个MoE架构、验证器和推理基准测试中，DES可靠地优于TTS基线方法，提高了准确性和稳定性，且无需额外成本

Conclusion: DES是一种实用且可扩展的架构感知TTS方法，展示了现代LLM中结构灵活性如何推进推理能力

Abstract: Test-Time Scaling (TTS) enhances the reasoning ability of large language
models (LLMs) by allocating additional computation during inference. However,
existing approaches primarily rely on output-level sampling while overlooking
the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we
observe that varying the number of activated experts yields complementary
solution sets with stable accuracy, revealing a new and underexplored source of
diversity. Motivated by this observation, we propose Dynamic Experts Search
(DES), a TTS strategy that elevates expert activation into a controllable
dimension of the search space. DES integrates two key components: (1) Dynamic
MoE, which enables direct control of expert counts during inference to generate
diverse reasoning trajectories without additional cost; and (2) Expert
Configuration Inheritance, which preserves consistent expert counts within a
reasoning path while varying them across runs, thereby balancing stability and
diversity throughout the search. Extensive experiments across MoE
architectures, verifiers and reasoning benchmarks (i.e., math, code and
knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing
accuracy and stability without additional cost. These results highlight DES as
a practical and scalable form of architecture-aware TTS, illustrating how
structural flexibility in modern LLMs can advance reasoning.

</details>


### [160] [Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective](https://arxiv.org/abs/2509.22613)
*Siwei Wang,Yifei Shen,Haoran Sun,Shi Feng,Shang-Hua Teng,Li Dong,Yaru Hao,Wei Chen*

Main category: cs.AI

TL;DR: 本文通过图抽象模型分析强化学习在LLM规划中的作用，发现SFT会产生伪相关解，而RL通过探索实现正确规划。但策略梯度存在多样性崩溃问题，Q学习则能保持多样性并避免奖励黑客攻击。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习显著提升了LLM的规划能力，但其理论有效性基础尚不明确。本文旨在通过可处理的图抽象模型系统分析RL在LLM规划中的优势和局限。

Method: 采用基于图的抽象模型，重点分析策略梯度和Q学习方法，通过理论分析比较不同方法在规划任务中的表现。

Result: SFT会产生基于共现的伪相关解；RL通过探索实现正确规划；策略梯度存在多样性崩溃问题；Q学习能保持输出多样性并避免奖励黑客攻击。

Conclusion: 探索在RL规划中起关键作用，Q学习比策略梯度更具优势，但需要精心设计奖励函数。这些发现在真实规划基准测试中得到验证。

Abstract: Recent reinforcement learning (RL) methods have substantially enhanced the
planning capabilities of Large Language Models (LLMs), yet the theoretical
basis for their effectiveness remains elusive. In this work, we investigate
RL's benefits and limitations through a tractable graph-based abstraction,
focusing on policy gradient (PG) and Q-learning methods. Our theoretical
analyses reveal that supervised fine-tuning (SFT) may introduce
co-occurrence-based spurious solutions, whereas RL achieves correct planning
primarily through exploration, underscoring exploration's role in enabling
better generalization. However, we also show that PG suffers from diversity
collapse, where output diversity decreases during training and persists even
after perfect accuracy is attained. By contrast, Q-learning provides two key
advantages: off-policy learning and diversity preservation at convergence. We
further demonstrate that careful reward design is necessary to prevent reward
hacking in Q-learning. Finally, applying our framework to the real-world
planning benchmark Blocksworld, we confirm that these behaviors manifest in
practice.

</details>
